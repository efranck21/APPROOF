\documentclass[a4paper,french,english,10pt]{article}
\usepackage{a4wide}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{lmodern}


\usepackage{picture}
%\usepackage{ae}

%\usepackage{refcheck}

%\usepackage{geometry}
%\geometry{verbose,a4paper,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm,headheight=3cm,headsep=3cm}
\usepackage{babel}
\usepackage{color}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{amsmath,amsthm, amssymb, mathrsfs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{epsfig}
%\usepackage{esint}R
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{amscd}
 % \usepackage[all]{xy}
\usepackage{fancyhdr}
%\usepackage[affil-it]{authblk}
%\usepackage[nottoc, notlof, notlot,numbib]{tocbibind}
%\usepackage{refcheck}


 %Macros pour faciliter la frappe(?)
\newcommand\ljr{l_{jr}}
\newcommand\njr{\mathbf{n}_{jr}}
\newcommand\tnjr{\mathbf{\tilde{n}}_{j,r}}
\newcommand\uu{\mathbf{u}}
\newcommand\vv{\mathbf{v}}

\newcommand\Cjr{\mathbf{C}_{jr}}
\newcommand\al{\widehat{\alpha}_{jr}}
\newcommand\be{\widehat{\beta}_{jr}}
\newcommand\si{\sigma}
\newcommand\eps{\varepsilon}
\newcommand{\id}{\widehat{I}_d}
\newcommand{\cotan}{\cot} 
\newcommand{\U}{\uu}
\newcommand{\dx}{\partial_x}
\newcommand{\dy}{\partial_y}
\newcommand{\dz}{\partial_z}
\newcommand{\dt}{\partial_t}
\newcommand{\ds}{\displaystyle}
\newcommand{\f}{f(\x,\V,t)}
\newcommand{\I}{I(\x,\mathbf{\Omega},\nu,t)}
\newcommand{\alp}{\mathbf{\alpha}}
\newcommand\alj{\widehat{\alpha}_{jr}}
\newcommand\ali{\widehat{\alpha}_{ir}}
\newcommand\bei{\widehat{\beta}_{ir}}
\newcommand\bej{\widehat{\beta}_{jr}}
\newcommand\lir{l_{ir}}
\newcommand\nir{\mathbf{n}_{ir}}
\newcommand\tui{\tilde{\uu}_i}
\newcommand\tuj{\tilde{\uu}_j}
\newcommand\tur{\tilde{\uu}_r}
\newcommand\tuhr{\tilde{\uu}_{h,r}}
\newcommand\tuhj{\tilde{\uu}_{h,j}}
\newcommand\tpj{\tilde{E}_j}
\newcommand\tpi{\tilde{E}_i}
\newcommand\Fj{\uj}
\newcommand\ui{\uu_i}
\newcommand\tu{\tilde{\uu}}

\newcommand\x{\mathbf{x}}
\newcommand\xj{\mathbf{x}_j}
\newcommand\xr{\mathbf{x}_r}

\newcommand\pej{\overline{p}_j}
\newcommand\uej{\overline{\uu}_j}
\newcommand\uer{\overline{\uu}_r}

\newcommand\pj{\bar{p}_j}
\newcommand\uj{\uu_j}
\newcommand\ur{\uu_r}

\newcommand\fj{\mathbf{f}_j}
\newcommand\fr{\mathbf{f}_r}

\newcommand\cc{\mathbf{c}}
\newcommand\bb{\mathbf{b}}

\newcommand\cj{\cc_j}
\newcommand\ccr{\cc_r}
\newcommand\br{\bb_r}
\newcommand\bj{\bb_j}

\newcommand\diam{\mbox{diam}}

\newcommand\ff{\mathbf{f}}
\newcommand\gf{\mathbf{g}}
\newcommand\V{\mathbf{V}}
\newcommand\W{\mathbf{W}}
\newcommand\G{\mathbf{G}}
\newcommand\F{\mathbf{F}}
\newcommand\nvhp{\|\V^\eps(0)\|_{H^3(\Omega)}}
 % Fin des macros pour faciliter la frappe

\newtheorem{theorem}{Theorem}[section]
\newtheorem{theor}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{pro}[theorem]{Proposition}
\newtheorem{algo}[theorem]{Algorithm}
\newtheorem{hyp}[theorem]{Hypothesis}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{coro}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}[theorem]{Remark}






\begin{document}

\title{Proof of uniform convergence for a cell-centered  AP discretization  of the hyperbolic heat equation 
 on general meshes}


\author{Christophe Buet\thanks{Electronic address: 
\texttt{christophe.buet@cea.fr},
CEA, DAM, DIF,  F-91297 Arpajon Cedex, France}
,
Bruno Despr\'es\thanks{Electronic address: 
\texttt{despres@ann.jussieu.fr}, Laboratoire Jacques-Louis Lions,
  Universit\'e Pierre et Marie Curie,
  75252 Paris Cedex 05,
  France},
Emmanuel Franck\thanks{Electronic address: \texttt{emmanuel.franck@ipp.mpg.de}, Max-Planck-Institut f\"ur Plasmaphysik, Boltzmannstra\ss e 2
D-85748 Garching, Germany},
Thomas Leroy\thanks{Electronic address: \texttt{thomas.leroy@cea.fr},
CEA, DAM, DIF, F-91297 Arpajon Cedex,France,
and Laboratoire Jacques-Louis Lions,
  Universit\'e Pierre et Marie Curie,
  75252 Paris Cedex 05,
  France}
}





%\date{Version 2}
\maketitle

\begin{abstract} 
We prove the uniform  AP convergence on unstructured
meshes in 2D of a  generalization, see \cite{glaceap},      of the Gosse-Toscani 
 1D scheme  for the hyperbolic heat equation. 
This   scheme 
 is also a nodal  extension in 2D  of the Jin-Levermore scheme described in \cite{jinlev} for the  1D case.
In 2D, the proof  is performed using a new diffusion scheme.
\end{abstract}

%\tableofcontents
%\newpage

\section{Introduction}
%
%\setlength{\unitlength}{.3cm}
%\begin{picture}(1, 1) 
%\thicklines
%\put(0,0){\vector(0,1){1}}
%\put(0,1){\line(1,0){1}}
%\put(1,1){\line(0,-1){1}}
%\put(1,0){\line(-1,0){1}}
% \end{picture}
%
%
%$$
%{\setlength{\unitlength}{.3cm}
%\begin{picture}(1, 1) 
%%\thicklines
%\put(0,0){\vector(0,1){1}}
%\put(0,1){\line(1,0){1}}
%\put(1,1){\line(0,-1){1}}
%\put(1,0){\line(-1,0){1}}
% \end{picture}
%} 
%$$
%
%$$
%C^{\begin{picture}(1, 1) 
%%\thicklines
%\put(1,1){\vector(1,0){15}}
%\end{picture}
%}
%$$
%$$
%C^\Longrightarrow
%$$
%


We address the convergence analysis  on unstructured meshes
of diffusion
asymptotic preserving schemes for the discretization
of a problem with a stiff parameter denoted as $\eps\leq 1$.
The model problem considered in this work
is  the hyperbolic heat equation in the domain 
$t\geq 0$ and  $x\in \Omega\subset \mathbb R^n$
\begin{equation} \label{eq:mod1}
P^\varepsilon: \qquad 
\left\{\begin{array}{ll}
\ds\partial_t p^{\eps}+\frac{1}{\eps}\operatorname{div}(\uu^{\eps})=0,
& p^{\eps}\in \mathbb R, \\
\\
\ds\partial_t \uu^{\eps}+\frac{1}{\eps}\nabla
p^{\eps}=-\frac{\sigma}{\eps^2}\uu^{\eps},
& \uu^{\eps}  \in \mathbb R^n
 \end{array}\right.
\end{equation}
discretized with first order finite volume schemes.
This problem is representative
of many  transport problem such
as transfer and neutron transport,  for which the small parameter
$\eps$ is the ratio of two very different sound velocities and
$\sigma$ is the absorption or the opacity.
For simplicity both $\eps$ and  $\sigma>0$ are kept constant in space in this study.
The system (\ref{eq:mod1}) can also be introduced as a specific linearization
of a pressure-velocity system of partial differential equations
in the acoustic regime. In this work we will need the following well known energy estimates concerning 
the solution $\V^\eps$ of the Cauchy problem for the partial differential equation (\ref{eq:mod1}).


\begin{proposition}\label{bee}
If $\Omega= \mathbb R^n$ or $\Omega= \mathbb T^n$, then
\begin{equation}\label{bee1}
\vert\vert \V^\varepsilon \vert \vert_{H^p( \Omega)}\leq \vert\vert \V^\varepsilon(0) \vert \vert_{H^p( \Omega)}
\end{equation}
 and moreover 
\begin{equation}\label{bee2}
\frac{\sigma}{\eps^2}\vert\vert \uu^\varepsilon \vert \vert^2_{L^2([0,T];H^p( \Omega))}\leq \vert\vert \V^\varepsilon(0) \vert \vert^2_{H^p( \Omega)}.
\end{equation}
\end{proposition}

We will consider well prepared data in the sense that:
 $p^\eps(t=0)$
is independent of $\eps$ and is sufficiently smooth;
the initial velocity satisfies the equality in the second equation
of (\ref{eq:mod1}) at leading order. It writes 
\begin{equation} \label{eq:iniwp}
p^\eps(t=0)=p_0 \mbox{ and } 
\mathbf u^\eps(0)=-\frac\eps\sigma \nabla p_0.
\end{equation}
For such well prepared data, it can be easily shown that
the formal limit of $P^\varepsilon$ for small $\eps$ is
\begin{equation} \label{eq:mod2}
P^0: \qquad
\partial_t p - \frac1\sigma \Delta p=0. 
\end{equation}

\begin{remark}\label{sigmanotzero}
We do not consider the regime $\sigma \rightarrow 0$, since it introduces a singularity both
  in the initial data of the hyperbolic heat equation and in the limit parabolic equation.
\end{remark}

\subsection{Precision of AP discretizations}

 Before addressing the main difficulty of this work  which is the discretization on  
  unstructured meshes, we briefly recall 
the  now well known notion 
of an asymptotic preserving technique \cite{jinbase}-\cite{jinreview}
which  is illustrated in the figure \ref{fig:diag1}.
For the simplicity of the presentation,
we will consider mainly semi-discrete numerical methods, this is why
the time step does not show up in the graphic.
The parameter $h$ designs a numerical method
with characteristic length $h\leq 1$:
that is  we assume  a numerical method $P_h^\varepsilon$ for the discretization
of $P^\varepsilon$.  
\begin{definition}[Uniform AP] \label{defAPu}
If  $P_h^\varepsilon$ is consistent with $P^\varepsilon$
uniformly with respect to $\varepsilon$, 
then we  say that the  scheme $P_h^\varepsilon$
is  uniformly AP  (uniformly asymptotic preserving).  
\end{definition}

However the design of such methods and  the numerical proof
of this property is difficult. This is why it has been proposed
in \cite{jinbase} to rely on the simpler necessary condition,
where the limit as $\eps\rightarrow 0$  of $P_h^\varepsilon$
is called the limit diffusion scheme $P_h^0$.



 
 \begin{figure}[h]
 \begin{center}
 \begin{tabular}{cc}
 \scalebox{.4}{\input{diag1.pdf_t}} 
 \end{tabular}
 \end{center}
 \caption{The AP (asymptotic diagram)
 diagram}
 \label{fig:diag1}
 \end{figure}



\begin{definition}[AP] \label{defAP}
 If $P_h^0$ is 
consistent with the limit model $P^0$, then we say that 
the scheme $P_h^\varepsilon$
is  AP (asymptotic preserving).
\end{definition}

This property is simpler to analyze than the uniform AP. It explains
why it has been very fruitful in the past.
In 1D, many AP schemes have been designed for some PDE and physical problems: S.
Jin, C. D. Levermore \cite{jinlev} or L. Gosse, G. Toscani \cite{Gosse} for the
hyperbolic heat equation, M. Lemou, L. Mieussens, N. Crouseilles
\cite{lemou}-\cite{MMvlasov}-\cite{couplingRad} for some kinetic equations, L.
Gosse \cite{GosseSn}, C. Buet and co-workers \cite{buet} or S. Jin and C. D.
Levermore \cite{SnJin} for $S_N$ equations and C. Berthon, R. Turpault
\cite{ber1}-\cite{ber3}-\cite{ber4} for generic systems and a non linear
radiative transfer model. Recently some asymptotic preserving schemes for linear
systems and non linear radiative transfer model have been designed in 2D
\cite{glaceap}-\cite{cras}-\cite{FVCA6}.
Other application to non linear hyperbolic systems of conservation laws with stiff diffusive relaxation is to be found is
\cite{F1}. Relaxation systems are treated in \cite{F2}. More general situation 
for transport and discrete velocity systems are in \cite{F3,F4}.
 However for this type of schemes it is
 difficult to obtain convergence estimates due to the competition between
the two parameters $\eps$ and $h$. To our knowledge this type of
proof are only given for uniform grids \cite{glaceap} (consistence and
stability, Lax theorem), \cite{Gosse} ($L^1$ and BV estimates), \cite{mmcv}
($L^2$ estimates).
The goal of this work is to prove the uniform AP property on unstructured 
grids.

To this end we adapt a strategy developed in 
 \cite{cveps} in a slightly different context.
It relies on the derivation of a priori estimates attached to the AP diagram in figure
\ref{fig:diag1}. To have a more global perspective
on this strategy,  let us assume 
some natural abstract a priori  estimates for a given norm
which is in our work based on   $\|f\|=\|f\|_{L^2([0,T]\times \Omega ) }$
where $T>0$ is a given final time, $\Omega=\mathbb{R}$, in 1D or $\Omega=[0,1]^2$ with periodic boundary conditions in 2D.
We assume four  constants $a,b,c,d>0$ and four  additional  constants $~_\downarrow C, C ^\rightarrow  ,C_\leftarrow,  C_\downarrow>0>0$ 
 such that the error attached to the branches of the AP diagram can be bounded like  
 \begin{equation}\label{eq:mod7}
\| P_h^\varepsilon - P^\varepsilon   \|_{ {\rm naive}} \leq~_\downarrow C \varepsilon ^{-b}h^c, % \qquad ~_\downarrow C>0,
\end{equation}
\begin{equation}\label{eq:mod3-bis}
\| P^\varepsilon_h - P^0_h \| \leq
C ^\rightarrow  \varepsilon ^{e}. % \qquad C ^\rightarrow  >0.
\end{equation}
\begin{equation}\label{eq:mod4}
\| P_h^0 - P^0   \| \leq
C_\downarrow  h^d,  % \qquad C_\downarrow>0.
\end{equation}
\begin{equation}\label{eq:mod3}
\| P^\varepsilon - P^0 \| \leq
C_\leftarrow  \varepsilon ^{a},     %\qquad C_\leftarrow>0,
\end{equation}
 The first inequality is  the naive error bound which typically blows up for small $\varepsilon$.
The second inequality 
for
$\| P_h^\eps-P_h^0\|$ 
is 
assumed to have  a form similar to the last one which  expresses that $P^0$ is  the limit of
$P^\varepsilon$. 
The third inequality corresponds to  the usual AP property.


\begin{pro} \label{prop:1}
 Assume that all these inequalities are at 
hand and that $d\geq c$ and $e\geq a$.
 Then the uniform AP holds with a rate at least
$O\left(h{^\frac{ac}{a+b}  } \right)$.
\end{pro}

\begin{proof}
The triangular inequality writes
$$
\| P_h^\varepsilon - P^\varepsilon   \|
\leq
\min
\left(
\| P_h^\varepsilon - P^\varepsilon   \|_{ {\rm naive}},
\| P_h^\eps-P_h^0\|+
\| P_h^0 - P^0   \| +
\| P^\varepsilon - P^0 \|
\right)
$$
which yields, using $\min(x,y+z)\leq \min(x,y)+\min(x,z)$, $d\geq c$  and   $e\geq a$, 
\begin{equation} \label{eq:toutdebut}
\| P_h^\varepsilon - P^\varepsilon   \|
\leq
C\left(
\min\left(
 \varepsilon ^{-b}h^c,
\eps^e
\right)+h^d+\min
\left(
 \varepsilon ^{-b}h^c,
\eps^a
\right)
\right)
\leq C\left(
2\min  
\left(
 \varepsilon ^{-b}h^c,
\eps^a
\right) +h^d\right)
\end{equation}
with $C=\max\left( _\downarrow C,C^\rightarrow, C_\downarrow  ,  C_\leftarrow \right)$.
We define a  threshold value $\varepsilon_{\rm thresh} $
by  $\varepsilon_{\rm thresh}  ^{-b}h^c=
\eps_{\rm thresh}^a$. So either $\eps\leq \varepsilon_{\rm thresh} $
so that 
$$
\min
\left(
 \varepsilon ^{-b}h^c,
\eps^a
\right)\leq \varepsilon_{\rm thresh}^a =h^{\frac{ac}{a+b}  },
$$ 
or 
$\eps\geq \varepsilon_{\rm thresh} $ and the same bound is obtained by 
taking the other term as the minimum.
And since $d\geq c$, one gets the abstract bound
$
\| P_h^\varepsilon - P^\varepsilon   \|
\leq
3Ch^{\frac{ac}{a+b}  }
$ which ends the proof.
\end{proof}

\subsection{Organization of the proof}
The structure of these  inequalities explains our strategy: that is we
prove separately each of these inequalities 
(\ref{eq:mod3}-\ref{eq:mod3-bis}) with care, so that
the inequalities $d\geq c$ and $e\geq a$  are true.
This part of the proof relies on specific hyperbolic
and parabolic numerical methods. Even if it is technical, 
the first three inequalities 
(\ref{eq:mod3}-\ref{eq:mod4}) do not yield additional difficulties
with respect to the state of the art. The proof of  inequality  (\ref{eq:mod3-bis})
is provided in 1D, and can be probably  be generalized 
straightforwardly on cartesian meshes in 2D and 3D.
On the other hand  our researches on proving 
   (\ref{eq:mod3-bis})
for  $\| P_h^\eps-P_h^0\|$  show 
a fundamental  obstruction in dimension greater than one on unstructured meshes   which was not expected
initially. 
%Indeed the study of this  inequality  is based
%on  comparison principles and a priori estimates. 
Since the main difficulty is related
to $P_h^0$, 
it motivates the definition
of a new  diffusion scheme. 
To this end we remark that another diffusion scheme is naturally
 defined
from $P_h^\varepsilon$ by {killing}
the derivative $\partial_t v_h$ in the discrete version
of the second equation of (\ref{eq:mod1}).
 {Killing} at the continuous level
the $\partial_t v$ is absolutely
equivalent to taking the formal limit $\varepsilon \rightarrow 0^+$.
But at the discrete level, 
it appears that it generates a new family of diffusion 
schemes, where
both parameters $h$ and $\eps$ are present. We 
call them Diffusion Asymptotic schemes, $DA_h^\varepsilon$.
By construction $P_h^0=\lim_{\varepsilon\rightarrow 0} DA_h^\varepsilon$. This 
is summarized
in figure \ref{fig:diag5}.
Finally since the scheme $DA_h^\eps$  is
 still  an accurate discretization of $P^0$,
our proof of the uniform AP properties is based
on the new AP diagram  displayed in figure \ref{fig:diag4}. 

 
 \begin{figure}[h]
 \begin{center}
 \begin{tabular}{cc}
 \scalebox{.3}{\input{diag5.pdf_t}} 
  \end{tabular}
 \end{center}
 \caption{Definition of the diffusion asymptotic scheme $DA_h^\varepsilon$.}
 \label{fig:diag5}
 \end{figure}



Our main  theorem \ref{theor:main} in dimension 2
is based on this structure and it may be stated as follows:
{\bf The so-called JL-(b) scheme defined
in \cite{glaceap} for the discretization of the hyperbolic heat equation 
(\ref{eq:mod1}) (the scheme  is cell-centered with nodal based fluxes)
 is uniformly AP on unstructured meshes, with a rate
of convergence at least $O(h^\frac14)$ for sufficiently smooth initial data}.
This is an improvement with respect to \cite{glaceap} where only AP was proven.
To our knowledge this is the first time that such a result is obtained on general unstructured
multidimensional meshes. 


More precisely the convergence estimate can be written as 
$$
%\|\V_h^\eps-\V^\eps\|
\mbox{\bf error}
\leq
C \min \left(
\sqrt{\frac h \eps  } \; \; , \; \;
\eps \max\left(1, \sqrt{\frac \eps h } \right) +
h + 
%\left(h +\eps
%\right)+
\eps
\right)
$$
where  the first argument in the min function comes from the hyperbolic analysis 
and the second argument comes from the parabolic analysis. 
Some natural regularity 
assumptions 
 are  nevertheless 
imposed on the mesh in the hypothesis \ref{geometrie1d}, this  is  not very restrictive. For example
 meshes with angles greater
than $90$ degrees are allowed. If the mesh is made with triangles,
the hypothesis is fulfilled if all angles are greater
than $12$ degrees, see \cite{glaceap}.
It is interesting to notice that
the rate of uniform 
convergence is $O(h^\frac13)$ 
in dimension one. 
The difference essentially comes from the 
estimate of the reconstruction of the initial velocity which is needed
to rewrite a diffusion scheme as a non homogeneous hyperbolic scheme:
it is much simpler in dimension one (see equation \eqref{init_1D}) than in 
dimension two (see proposition \eqref{esti_initial_data}).
In this work we considered mainly  semi-discrete numerical schemes, since it simplifies
a lot the notations and allow to focus on the main difficulties,
but the final estimates of convergence can be generalized to fully discrete schemes, using the a priori
estimates developed in \cite{de04}. 
For explicit schemes,
these estimates add a term proportional to the square root of the maximal time step allowed by the CFL
condition. Since our problem is an hyperbolic+relaxation problem, with a limit which is parabolic,
this additional term can be computed and is of the order between $h$ (for purely hyperbolic)
to $h^2$ (for purely parabolic). We refer to  \cite{glaceap} for the detail of CFL condition in 1D and 2D.
Concerning the implicit fully discrete version of the semi-discrete scheme which is unconditionally
stable and well adapted to the test problem analyzed at the end of this work, the same kind
of error terms  can be analyzed. We will obtained the following result in dimension two.

\begin{theor} \label{theor:fullconv}
With some usual regularity assumptions on the mesh, the error between our cell-centered
finite volume
corner-based-flux
implicit discretization  $\mathbf P_{h,\Delta t}^\eps$ and the exact solution  is
$$
\left\| \mathbf V_h^\eps(t_n)  - \mathbf V^\eps \right\|_{L^2(\Omega)} \leq C \left( h^\frac14 +\Delta t ^\frac12\right)
 \| p_0\|_{H^4(\Omega)}, \qquad t_n=n\Delta t \leq T.
$$
The constant is independent of $h$, $\eps$ and $\Delta t$.
\end{theor}

The proof is an easy add-on on the space estimate $O(h^\frac14)$  of   theorem  \ref{theor:main}, 
by means of an abstract method  \cite{de04} which gives a general bound $O(\Delta t^\frac12)$  %compares
of the difference between  the semi-discrete scheme and the implicit Euler scheme. This will be explained
at the end of this work. %in section \ref{end:theor}.
The rate of convergence is  confirmed by the numerical results of section \ref{sec::num}, 
which show an even better rate of convergence.

 \begin{figure}[h]
 \begin{center}
 \begin{tabular}{cc}
 \scalebox{.4}{\input{diag4.pdf_t}} 
  \end{tabular}
 \end{center}
 \caption{The new AP diagram, where the previous branch is still
 displayed in dashed lines. 
 }
 \label{fig:diag4}
 \end{figure}


%More important for applications is the fact that 
%The fact that the finite volume scheme is nodal based and not edge based
%has been stressed in \cite{glaceap}. This fact is important if one has in mind to

We think that some of our results can have an interest for the 
development and  use of such methods in research or industrial codes with complex non linear 
physics   on unstructured meshes.
Indeed for such codes  cell-centered Finite Volume schemes %well adapted
are a natural solution in terms of  data structure.
The point is the following: the scheme studied in this work
is the only cell-centered  one  we know in 2D 
to compute
the solutions of problems which admit  diffusion limits in certain regimes and for which it is possible to prove the AP property.
Since the structure of this cell-centered scheme is nodal based, it strongly questions the ability
of standard Finite Volume methods with edge-based fluxes to recover asymptotic diffusion regimes.
As demonstrated in this work, nodal based Finite Volume techniques do not suffer from this drawback. For linear wave equation the nodal scheme can be understand as some 1D Riemann problem written in some direction around each node, so can be interpreted as
  an approximation of the 2D Riemann problem \cite{2Driemann}.


%It is another
%open problem to identify alternative finite volume schemes, valid  on unstructured meshes,
%which can be used to compute the solution of such problems with uniform accuracy.
%If ever edge based finite volume techniques are indeed
%strongly limited for such problems, as was suggested in \cite{glaceap}, it is a good reason
%to still continue to develop nodal based finite volume techniques.

\subsection{Organization of the work}

This work is organized as follows.
Section \ref{sec:2}
is dedicated to the discretization 
of the model problem in dimension one on irregular grids.
The convergence is proved in 
theorem \ref{theor:1d}
with order
$h^{\frac13}$ in the $L^2$ space-time norm.
In the next section, the nodal solvers for the hyperbolic equation are defined, and the various a priori
estimates proved. 
The main theorem of uniform AP for the JL-(b) scheme
with a rate $O(h^\frac14)$
is proved at the end of the section.
Section \ref{sec::num}
  provides numerical results that sustain the fact
that the convergence order depends on the relative value of $\eps$ and $h$.,
and so is mixed hyperbolic/parabolic. 
Our final remarks will be gathered in a conclusion.

All our results and numerical methods in 2D can be generalized in 3D 
provided a convenient definition of the nodal corner vector is used as in \cite{de10}.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis in 1D} \label{sec:2}

%\subsection{The JL-(b) scheme}


The model problem in dimension one writes
\begin{equation}\label{chaleur22}
P^\varepsilon: \qquad
 \left \{
\begin{array}{lll}
\partial_t p^{\eps} + \frac{1}{\eps}\partial_x u^{\eps} =0, \\
\partial_t u^{\eps} + \frac{1}{\eps}\partial_x p^{\eps} =
-\frac{\sigma}{\eps^2}u^{\eps}.
\end{array}
\right.
\end{equation}
As stressed already in (\ref{eq:iniwp}), we consider well-prepared data
$
p^\varepsilon(t=0)=p_0 $  and  $u_0^\varepsilon=-\frac\varepsilon \sigma \partial_{x}p_0$.
The equations (\ref{chaleur22}) 
admit the formal diffusion limit when $\eps$ tends to $0$:
\begin{equation}\label{difffff}
P^0: \qquad 
\dt p -\frac{1}{\sigma}\partial_{xx}p=0.
\end{equation}
A useful variable will be  the scaled gradient  
\begin{equation} \label{eq:diff-v}
v=-\frac{1}{\sigma}\dx p.
\end{equation}
%%%%

\subsection{Notations}
We denote $x_{j+1/2}$ the nodes, the cells $j$ are the intervals $[x_{j-1/2},x_{j+1/2}]$, thus $\Delta x_j=x_{j+1/2}-x_{j-1/2}$, $x_j$ is the center of the cell $j$ that is $x_j =\frac12(x_{j+1/2}+x_{j-1/2})$, and $\Delta x_{j+1/2}=x_{j+1}-x_j=\frac12(\Delta x_{j+1}+\Delta x_{j})$. Natural assumptions on the mesh are summarized below:

\begin{hyp}[Regularity of the mesh in 1D and constant $C_\mathcal M$] \label{geometrie1d}
 We consider that there exists 
a universal constant $0<C_{\mathcal{M}}\leq 1$ independent of the mesh size
$h=\sup_{j\in \mathbb Z} \Delta x_j $
which  controls the mesh from below
\begin{equation}\label{constante_mesh_1D}
C_{\mathcal{M}} h \leq \Delta x_j \leq h \qquad \forall j \in \mathbb{Z}.
\end{equation}
\end{hyp}


The semi-discrete
JL(b) scheme,   derived in  \cite{glaceap} in 2D,
can also be written in 1D on irregular meshes as 
\begin{equation}\label{schema1}
P^\varepsilon_h: \qquad
\left\{
\begin{array}{lll}
\displaystyle\frac{d}{dt}p^{\eps}_j +\frac{u_{j+\frac12 }^{\eps} -
u_{j-\frac12 }^{\eps} }{\eps\Delta x_j} = 0,\\
\displaystyle\frac{d}{dt}u^{\eps}_j+\frac{p_{j+\frac12 }^{\eps}-
p_{j-\frac12 }^{\eps}}{\eps\Delta x_j} = -\frac{\sigma}{\eps^2} \frac{
u_{j+\frac12 }^{\eps}+u_{j-\frac12 }^{\eps}}{2 } ,
\end{array}
\right.
\end{equation}
with  the fluxes  $p_{j+\frac12 }^{\eps}$ and $u_{j+\frac12 }^{\eps}$
are the solutions of the well-posed linear system
\begin{equation} \label{eq:fflux}
j\in \mathbb Z: \qquad
\left\{
\begin{array}{cl}
\displaystyle
p_{j+\frac12 }^{\eps}+u_{j+\frac12 }^{\eps}
+\frac{\sigma \Delta x_j}{2\eps}
 u_{j+\frac12 }^{\eps}
&=
p^{\eps}_j+u^{\eps}_j ,\vspace{2mm}
\\
\displaystyle
-p_{j+\frac12 }^{\eps} +u_{j+\frac12 }^{\eps}
+\frac{\sigma \Delta x_{j+1}}{2\eps} 
u_{j+\frac12 }^{\eps} 
&=
-p^{\eps}_{j+1}+u^{\eps}_{j+1}.
\end{array}
\right.
\end{equation}
This scheme is the same
as the Gosse-Toscani scheme\footnote{A 
long and tedious computation  shows that the  scheme is strictly
equivalent to 
  the   Gosse-Toscani's scheme, described in    \cite{Gosse} but only for
uniform meshes, which writes in terms of $w^{\eps} ,v^{\eps} =p^{\eps} \pm u^{\eps} $
\begin{equation*} \label{hhe:diagnum}
\left\{
\begin{array}{ll}
\displaystyle\frac{d w_j}{dt}
+\frac{ M_{j- \frac12} }{\varepsilon}
\frac{  w_j^{\eps}  -  w_{j-1}^{\eps} }{\Delta  x_j}=
\frac{1  }{\varepsilon\Delta  x_j}(1-M_{j-
\frac12})(v_j^{\eps}  -w_j^{\eps}  )=
 M_{j- \frac12}  
\frac{\Delta  x_{j- \frac12}}{\Delta  x_j}\frac{ \sigma }{2\varepsilon^2}
(v_j^{\eps} -w_j^{\eps} ),   \\
\displaystyle\frac{ dv_j^{\eps}  }{dt}
-\frac{  M_{j+ \frac12} }{\varepsilon}
\frac{  v_{j+1} ^{\eps} - v_j^{\eps} }{\Delta  x_j}=\frac{1  }{\varepsilon\Delta  x_j}(1-M_{j+
\frac12})(w_j ^{\eps}  -v_j^{\eps}  )
=
 M_{j+ \frac12} \frac{\Delta  x_{j+ \frac12}}{\Delta  x_j}
\frac{ \sigma}{2\varepsilon^2} (w_j^{\eps} -v_j^{\eps} )
\end{array}
\right.
\end{equation*}
with 
%\begin{equation} \label{eq:64num}
$M_{j+ \frac12} =\frac {2  \varepsilon }{\sigma\Delta  x_{j+ \frac12} + 2 
\varepsilon }
$%\end{equation}
and $ \Delta  x_{j+ \frac12}=\frac{\Delta  x_{j}+\Delta  x_{j+1}}{2}$. 
By writing
\begin{equation*} \label{hhe:toto}
\left\{
\begin{array}{ll}
M_{j- \frac12}
( w_{j-1}^{\eps}  - w_j^{\eps}    ) = M_{j- \frac12}w_{j-1} -M_{j+ \frac12}w_j + (M_{j+
\frac12}-M_{j- \frac12}) w_j^{\eps} \\
 M_{j+ \frac12} (  v_{j+1}^{\eps}  - v_j^{\eps} )=M_{j+ \frac12}
v_{j+1}^{\eps}  -M_{j- \frac12}v_j^{\eps}  -
(M_{j+ \frac12}-M_{j- \frac12})v_j^{\eps} 
\end{array}
\right.
\end{equation*}
then  in terms of $p^{\eps} $ an $u^{\eps} $ we have evidently
\begin{equation*} \label{eq:63num}
\left\{
\begin{array}{l}
\displaystyle\frac{dp_j^{\eps} }{dt }
+\frac{1}{\varepsilon}\frac{ M_{j+ \frac12}  u_{j+\frac12}^{\eps} 
-
M_{j- \frac12}  u_{j-\frac12}^{\eps}  }{\Delta  x_j}
=0,   \\
\\
\displaystyle\frac{du_j^{\eps} }{dt }
+\frac{1}{\varepsilon}\frac{ M_{j+ \frac12} p_{j+\frac12}^{\eps} 
-
M_{j- \frac12}  p_{j-\frac12} ^{\eps}  }{\Delta  x_j}
=-\frac12\left(
M_{j+ \frac12} 
\frac{\Delta  x_{j+ \frac12}}{\Delta  x_j}\frac{\sigma}{\varepsilon^2}
+
M_{j- \frac12} 
\frac{\Delta  x_{j- \frac12}}{\Delta  x_j}\frac{\sigma}{\varepsilon^2}
\right)
u_j^{\eps} 
 +\frac{M_{j+ \frac12} - M_{j- \frac12}}{\varepsilon \Delta  x_j} p_j^{\eps} 
\end{array}
\right.
\end{equation*} 
with  the fluxes given by
$p_{j+\frac12}^{\eps} =\frac{  p_j^{\eps}  +p_{j+1}^{\eps} }{2 }
+
\frac{  u_j^{\eps}  - u_{j+1}^{\eps} }{2 }
$ and $
\displaystyle u _{j+\frac12}= 
\frac{ u_j^{\eps}  + u_{j+1}^{\eps} }{2 }
+
\frac{ p_j^{\eps}  - p_{j+1}^{\eps} }{2 }$.
}.
Other  equivalent forms of $P_h^\eps$
  can be  obtained by various manipulations, as in (\ref{schema}).
  We use another formulation of the Gosse-Toscani obtained using the Jin-Levemore scheme \cite{jinlev} and a discretization of the source term which use the fluxes. Contrary to the Gosse-Toscani scheme which uses Riemann problem, this formulation based an 
  elementary  algebraic computation is easier to write in 2D on unstructured meshes (the design is detailed in \cite{glaceap}). 
The natural pointwise initialization is chosen
\begin{equation} \label{eq:initp1}
p_j^\eps(0)=p_0(x_j) \mbox{ and }
u_j^\eps(0)=-\frac\eps\sigma \partial_x p_0(x_j) \mbox{ for all }j\in \mathbb Z.
\end{equation}
%Our goal is this section is to show that this scheme is AP.
 When $\eps$ tends to $0$, 
 the scheme $P_h^\eps$ admits the 
diffusion limit scheme $P_h^0$
\begin{equation}\label{schemadifff}
P_h^0: \qquad
\displaystyle\Delta x_j\frac{d}{dt} p_j -\frac{1}{\sigma } \bigg(
\frac{p_{j+1}-p_j}{\Delta x_{j+\frac12 }}-\frac{p_j -p_{j-1}}{\Delta
x_{j-\frac12 }}\bigg)=0
\end{equation}
with the pointwise  initialization %is
\begin{equation} \label{eq:intip0}
p_j(0)=p_0(x_j) \mbox{ for all }j\in \mathbb Z.
\end{equation}
Other quantities are the reconstructed gradient 
\begin{equation} \label{eq:vv}
\left \{
\begin{array}{l}
\displaystyle v_{j+\frac12 }=-\frac{1}{\sigma}\frac{p_{j+1}-p_{j}}{\Delta
x_{j+\frac12 }},\\
\displaystyle v_j=\frac{v_{j+\frac12 }+v_{j-\frac12 }}{2}.
\end{array}
\right.
\end{equation}
We denote by $\V^{\eps}(t,x)=\left(p^{\eps}(x,t),
u^{\eps}(x,t)\right)$ the solution of the hyperbolic heat equations $P^\eps$. % with pointwise initialization (\ref{eq:initp1}).
We  reconstruct
similar quantities from the diffusion scheme: it yields 
$\W^\eps(t,x)=\left(p(x,t),\varepsilon v(x,t)\right)$ 
which is the 
solution of the diffusion limit
%$P^0$ 
(\ref{difffff})-(\ref{eq:diff-v}).
The indicatrix function of the interval  $(x_{j-1/2},x_{j+1/2})$  is denoted as
 $
 1_j(x)= 1$ if  $x\in (x_{j-1/2},x_{j+1/2})$ and  $1_j(x)=0$ in the other case.
With this notation  we note
$\V^{\eps}_h(t,x)=
\left( \sum_{j\in \mathbb Z} p_j^{\eps}(t) {1}_{j}(x) ,  \sum_{j\in \mathbb Z} u_j^{\eps}(t) {1}_{j}(x) \right)$ the solution of the JL-(b) scheme
 $P_h^\eps$.
Finally we note  $\W_h^\eps(t,x)= \left( \sum_{j\in \mathbb Z} p_j(t) {1}_{j}(x) ,
\varepsilon \sum_{j\in \mathbb Z} v_j(t)  {1}_{j}(x) \right)$ %which is
the
solution of the diffusion scheme $P_h^0$ (\ref{schemadifff})-(\ref{eq:vv}). 

For simplicity we choose a final time $T>0$.
All  error estimates will be given for 
$t\leq T$, either
in the norm
$\|f(t)\|_{L^\infty([0,T];L^2(\mathbb{R}))}$, or mostly in the norm
$\|f\|_{L^2([0,T]\times \mathbb{R})}$.


\begin{hyp}[Regularity of the initial data   and constant $C_\mathcal A$] \label{initdata1d}
 We consider that there exists 
a  universal constant $C_{\mathcal A}>0$
which controls all kind of  approximations/interpolations/projections   on the mesh  of 
exact functions. 
We will write for example
%It  yields 
the error  estimates at the initial time under the form 
\begin{equation} \label{hyphen:1}
\left\| \mathbf V_h^\varepsilon(0) -  \mathbf V^\varepsilon(0) \right\|_{L^2(\mathbb R)} \leq C_\mathcal A h 
 \|p_0\|_{H^2(\mathbb{R})}
%\left\| \mathbf V^\varepsilon(0) \right\|_{H^1(\mathbb R)},
\end{equation}
and 
\begin{equation} \label{hyphen:2}
\left\| \mathbf W_h^\varepsilon(0) -  \mathbf W^\varepsilon(0) \right\|_{L^2(\mathbb R)} \leq C_\mathcal A h 
 \|p_0\|_{H^2(\mathbb{R})}.
%\left\| \mathbf V^\varepsilon(0) \right\|_{H^2(\mathbb R)}.
\end{equation}
%Notice that by definition of $V^\varepsilon(0)$, one has for example 
%that 
%$$
%\left\| \mathbf V^\varepsilon(0) \right\|_{H^1(\mathbb R)}\leq \left\| \mathbf V^1(0) \right\|_{H^1(\mathbb R)}\leq
%(1+\sigma^{-1})  \|p_0\|_{H^2(\mathbb{R})}
%$$ 
%which is bounded independently of $\varepsilon$. The same holds
%for any norm of $V^\varepsilon(0)$.
\end{hyp}
The second inequality in the hypothesis  can be  related to % as a consequence of 
the sharper inequality %(\ref{init_1D})
%The same constant $C_\mathcal A$ is  used for sharper  inequalities such as
\begin{equation}\label{init_1D}
\bigg\| \sum_j \left(  u_j^{\eps}(0) - \eps  v_j(0)  \right) 1_j  \bigg\|_{L^2(\mathbb{R})} \leq C_\mathcal A h\eps \|p_0\|_{H^2(\mathbb{R})}.
\end{equation}
%by taking $\varepsilon\leq 1$.
%Of course one can further simplify the notations by taking $C=\max( C_\mathcal M, C_\mathcal A  )$, which can be justified
%for example because the projection error at initial time is influenced by the structure of the mesh.
%However  we consider it is instructing to keep the two constant with  distinct notations. 
The other technical  constants used to bound the errors of the left,  top, right and bottom   branches of the AP diagram \ref{fig:diag1}
will be denoted as  $~_\downarrow C$, $C^\rightarrow $, $C_\downarrow $ and  $C_\leftarrow$.   
We will also use  $e^{T/2\sigma}$ in our constants to show the usual exponential dependance of the Gronwall lemma. 
%We begin with the  bottom branch which is straightforward
%as shown just below.


\subsection{Study of $\|P^\varepsilon-P^0  \|$}

In this section we prove a natural error estimate \cite{cveps}
between the solution of the
hyperbolic heat equations \eqref{chaleur22} and the solution of the diffusion
limit equation \eqref{difffff}. 


\begin{lemma} \label{heqtode}
One has the estimate
\begin{equation}\label{esti_p_eps_p_0}
\|\V^{\eps}-\W^\eps\|_{L^2([0,T]\times \mathbb{R}) } \leq
 C_\leftarrow \  \eps 
  \|p_0\|_{H^3(\mathbb{R})}, \qquad C_\leftarrow=\frac{T^\frac32}{\sigma^2}.
\end{equation}
\end{lemma}
\begin{proof}
We redefine $v=-\frac{\varepsilon}\sigma \partial_x p$ with $p$ the diffusion solution of (\ref{difffff}) and
introduce $R^{\eps}$ such that the solution of the diffusion equation satisfies
\begin{equation}\label{modifdiff}
\left \{
\begin{array}{lll}
\dt p +\frac{1}{\eps}\partial_{x}v=0,\\
\partial_t v+ \frac{1}{\eps}\partial_x p +\frac{\sigma}{\eps^2}v = R^{\eps}
\end{array}
\right.
\end{equation}
where
 $R^{\eps}=\dt v=-\frac{\varepsilon}\sigma \partial_{tx} p=
-\frac{\varepsilon}{\sigma^2} \partial_{xxx}p$. 
Note that 
$\|R^{\eps}(t)\|_{L^2(\mathbb{R})}\leq \|R^{\eps}(0)\|_{L^2(\mathbb{R})}\leq \frac\varepsilon{\sigma^2}   \|p_0\|_{H^3(\mathbb{R})}$. 
Denoting $e^{\eps}=p-p^{\eps}$, $f^{\eps}=v-u^{\eps}$, we make the difference between the
systems \eqref{chaleur22} et \eqref{modifdiff}
\begin{equation}\label{diffmodif}
\left \{
\begin{array}{lll}
\dt e^{\eps} +\frac{1}{\eps}\partial_{x}f^{\eps}=0,\\
\partial_t f^{\eps} +\frac{1}{\eps}\partial_x e^{\eps} +\frac{\sigma}{\eps^2}f^{\eps} = R^{\eps}.
\end{array}
\right.
\end{equation}
Since data are well-prepared, one has $e^{\eps}(0)=f^{\eps}(0)=0$.
Consider
$\|\V^{\eps}-\W^\eps\|_{L^2(\mathbb{R})}^2=\|e^{\eps}\|_{L^2(\mathbb{R})}
^2+\|f^{\eps}\|_{L^2(\mathbb{R})}^2$.
Adding the first equation of
\eqref{diffmodif} multiplied by $e^{\eps}$ and the second multiplied by $f^{\eps}$ and integrating
on $\mathbb{R}$,
 we find out that:
$\frac12 \frac{d}{dt}\|\V^{\eps}-\W^\eps\|_{L^2(\mathbb{R})}^2 \leq
\int_{\mathbb{R}}R^{\eps}f^{\eps} dx \leq  \|R^{\eps}\|_{L^2(\mathbb{R})}
\|\V^{\eps}-\W^\eps\|_{L^2(\mathbb{R})}
$.
One gets a bound of $\|\V^{\eps}-\W^\eps\|_{ L^\infty([0,T];L^2(\mathbb{R}) )  }$
by integration between 0 and T.
Finally $\|\V^{\eps}-\W^\eps\|_{ L^2([0,T]\times \mathbb{R} )  }\leq \sqrt T \|\V^{\eps}-\W^\eps\|_{ L^\infty([0,T];L^2(\mathbb{R}) )  }$ which ends
the proof.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 \subsection{Stability  estimates for $P_h^\varepsilon$ and $P_h^0$}

The  estimates (\ref{estiuepsflux}-\ref{estiuepsflux2}) and (\ref{l2}) characterize the dissipation rate 
of both schemes.

\begin{proposition}\label{prodentrgosse}
The  scheme $P_h^\eps$
is stable in $L^2$ norm. Moreover, 
\begin{equation}\label{estiuepsflux}
\sqrt{
\int_0^T \left( \sum \Delta x_{j+\frac12}  ( u_{j+\frac12 }^\eps)^2 \right)dt
}
\leq
\frac{\eps}{\sqrt{\sigma}}
  \|\V^{\eps}_h(0)\|_{L^2(\mathbb{R})}
\end{equation}
and
\begin{equation}\label{estiuepsflux2}
 \sqrt{
\int_0^T
\left(
\sum_{j\in \mathbb{Z}} 
{
(u_{j+\frac12 }^{\eps}-u^{\eps}_j)^2 }
 +\sum_{j\in \mathbb{Z}} 
{
(u_{j-\frac12 }^{\eps}-u^{\eps}_j)^2 }
\right)dt }
\leq \sqrt \eps  \|\V^{\eps}_h(0)\|_{L^2(\mathbb{R})}.
\end{equation}
\end{proposition}
\begin{remark}
The strategy of the proof of many estimates in this work
consists in analyzing the balance
between the dissipation of the fluxes and the physical dissipation (all 
 source terms like 
$-\frac\sigma{\eps^2}u$) on the one hand, and some truncation errors
on the other hand.
This is why it is  convenient
to reformulate $P_h^\eps$
so that   the pressure fluxes 
$p_{j+\frac12 }^{\eps}$ and $p_{j-\frac12 }^{\eps}$ are eliminated in the second
equation of (\ref{schema1}). This elimination is technically convenient since
all dissipation terms are expressed using the same variable, namely $u$.
It will simplify a lot the comparisons between
all kinds of dissipation terms and other errors terms.
\end{remark}

\begin{proof}
According to the above remark we obtain  
the formulation 
(\ref{schema}) 
which is 
equivalent to $P_h^\eps$
\begin{equation}\label{schema}
\left\{
\begin{aligned}
&\Delta x_j\frac{d}{dt}p^{\eps}_j +\frac{u_{j+\frac12 }^{\eps} -
u_{j-\frac12 }^{\eps} }{\eps} = 0,\\
&\Delta x_j\frac{d}{dt}u^{\eps}_j-\frac{u_{j+\frac12 }^{\eps}+
u_{j-\frac12 }^{\eps}}{\eps} +\frac{2}{\eps}u_j^{\eps}=0, \\
&
\left( 2
+\frac{\sigma \Delta x_{j+\frac12}
 }{\eps}
\right) u_{j+\frac12 }^{\eps} 
=p^{\eps}_j-p^{\eps}_{j+1}+u^{\eps}_j+u^{\eps}_{j+1}
.
\end{aligned}
\right.
\end{equation}
Consider now the discrete quadratic energy
$E(t)=\frac12 \sum_j \Delta x_j ((p^{\eps}_j)^2+(u^{\eps}_j)^2)$.
 Multiplying the first equation of \eqref{schema} by $p_j^{\eps}$ and the second
equation by $u_j^{\eps}$ and adding on all the cells, one finds
\begin{equation*}
 E'(t)=-\sum_{j\in \mathbb{Z}} \frac{
u_{j+\frac12 }^{\eps}-u_{j-\frac12 }^{\eps}}{\eps}p^{\eps}_j + \sum_{j\in
\mathbb{Z}} \frac{
u_{j+\frac12 }^{\eps}+u_{j-\frac12 }^{\eps}}{\eps}u^{\eps}_j
-\frac{2}{\eps}\sum_j (u^{\eps}_j)^2.
\end{equation*}
Since $
\sum_{j}(
u_{j+\frac12 }^{\eps}-u_{j-\frac12 }^{\eps})p^{\eps}_j =
\sum_{j} u_{j+\frac12 }^{\eps}(p^{\eps}_j-p^{\eps}_{j+1})$, one
has  by using the third equation of \eqref{schema} and rearranging the
terms
\begin{equation}\label{prodentrgosseeq}
E'(t)+\sum_{j\in \mathbb{Z}} \frac{
(u_{j+\frac12 }^{\eps}-u^{\eps}_j)^2 }{\eps}+\sum_{j\in \mathbb{Z}} \frac{
(u_{j-\frac12 }^{\eps}-u^{\eps}_j)^2 }{\eps} +
\frac{\sigma}{\eps^2}\sum_{j\in \mathbb{Z}} \Delta x_{j}
\frac{(u_{j+\frac12 }^{\eps})^2+(u_{j-\frac12 }^{\eps})^2}{2}= 0.
\end{equation}
Integrating  \eqref{prodentrgosseeq} between 0 and t, one finds  $E(t)\leq E(0)$, that is the $L^2$ stability of $P_h^\eps$.
The estimate  (\ref{estiuepsflux}) 
comes from 
$\Delta x_{j+\frac12}=\frac12( \Delta x_{j}+\Delta x_{j+1})$. The estimate 
 (\ref{estiuepsflux2}) is directly deduced from
(\ref{prodentrgosseeq}).
\end{proof}


Some similar bounds hold 
for  the quantities related to the diffusion scheme (\ref{schemadifff}).
First, multiplying the diffusion scheme by $p_j$ and adding on all the cells,
one has the $L^2$ stability in the sense
\begin{equation*}
\frac12 \frac{d}{dt}\sum_j \Delta x_jp_j^2=-\frac{1}{\sigma}\sum_j
\frac{(p_{j+1}-p_j)^2}{\Delta x_{j+\frac12 }}.
\end{equation*}
Thus 
the following estimate holds for
 the function 
$\bar{v}_h=\left( v_{j+\frac12}  \right)_j$ defined by
(\ref{eq:vv})  
\begin{equation}\label{l2}
 \|\bar{v}_{h}\|_{L^2([0,T]\times\mathbb{R})}
=
\sqrt{
\int_0^T \sum_j \Delta x_{j+\frac12} (v_{j+\frac12} )^2
}
\leq
\sqrt{\frac{\sigma}{2}}\|p_h(0)\|_{L^2(\mathbb R)}, \qquad C>0.
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Study of $\| P_h^\varepsilon - P^\varepsilon   \|_{ {\rm naive}}$}


In this section we  
prove the 
convergence of $P_h^\varepsilon $ to  $P^\varepsilon $. We still denote $V^\eps(t)=(p^\eps,u^\eps)$.
\begin{lemma}\label{cv1d}
%Under assumption \ref{geometrie1d} on the mesh and assuming that the initial conditions satisfy \eqref{eq:initp1}, t
There exists a constant
%\  ${\setlength{\unitlength}{.3cm}
%\begin{picture}(1, 1) 
%%\thicklines
%\put(0,1){\vector(0,-1){1}}
%\put(0,1){\line(1,0){1}}
%\put(1,1){\line(0,-1){1}}
%\put(1,0){\line(-1,0){1}}
% \end{picture}
%} 
$~_\downarrow C
>0
$ independent of  $h,\eps,C_\mathcal M$ such that the following estimate holds
\begin{equation}\label{esti_naive_1d}
\|\V_h^{\eps}-\V^{\eps} \|_{L^2([0,T]\times \mathbb{R})}
\leq 
\frac{~_\downarrow C
}{\sqrt{C_\mathcal M}}
\sqrt \frac{h}\eps 
\left\|  p_0   %\V^{\eps}(0)
\right\|_{H^2(\mathbb{R})}.
\end{equation}

\end{lemma}

\begin{proof}
We  use the method introduced by C.
Mazeran \cite{Mazeran} in his PhD thesis.
It  starts with an estimate for the time derivative of
$\mathscr{E}=\frac12 \|\mathbf V_h^\eps - \mathbf V^\eps  \|^2_{L^2(\mathbb{R})}$.
For the sake of simplicity,   $q'$ stands indifferently for  $\frac{d}{dt}q $ or $\partial_t q $ for any quantity $q$. One has 
\begin{equation*}\label{esti}
\begin{array}{l}
\mathscr{E}'(t) = \underbrace{\frac12 
\int_{\mathbb{R}}((p_{h}^{\eps})^2+(u_{h}^{\eps})^2)'dx}_{D_1}+
\underbrace{\frac12 
\int_{\mathbb{R}}((p^{\eps})^2+(u^{\eps})^2)'dx}_{D_2} \\
\hspace{0.8cm}
+ \underbrace{\int_{\mathbb{R}}
(-(p_{h}^{\eps})'p^\eps-(u_{h}^{\eps})'u^\eps)dx}_{D_3} + \underbrace{\int_{\mathbb{R}}
(-p_{h}^{\eps}(p^{\eps})'-u_{h}^{\eps}(u^{\eps})'dx}_{D_4}
\end{array}
\end{equation*}
We will successively estimate each of those terms,
the fundamental idea being that
$D_1\leq 0$ and $D_2\leq 0$ are used to control 
spurious contributions in $D_3$ and $D_4$.
First $D_1$ corresponds to the
entropy production of the scheme. Thanks to (\ref{prodentrgosseeq}),
one has
\begin{equation*}
 D_1 = -\frac{1}{\eps}\sum_{j\in \mathbb{Z}}
(u_{j+\frac12 }^{\eps}-u^{\eps}_j)^2 -\frac{1}{\eps} \sum_{j\in
\mathbb{Z}}(u_{j-\frac12 }^{\eps}-u^{\eps}_j)^2-
\frac{\sigma}{\eps^2}\sum_{j\in \mathbb{Z}} \Delta x_{j}
\frac{(u_{j+\frac12 }^{\eps})^2+(u_{j-\frac12 }^{\eps})^2}{2}\leq 0.
\end{equation*}
One also directly obtains
\begin{equation*}
 D_2 = -\sum_{j\in \mathbb{Z}} \Delta x_j \frac{ \sigma }{\eps^2} \: \bigg(
\frac{1}{\Delta x_j}{\int_{x_{j-\frac12}}^{x_{j+\frac12}}}(u^{\eps})^2 dx \bigg)
\leq 0.
\end{equation*}
For $D_4$, one gets  directly
\begin{eqnarray*}
 D_4 &=& \sum_{j\in \mathbb{Z}} p^{\eps}_j
\frac{u^{\eps}(x_{j+\frac12 })-u^{\eps}(x_{j-\frac12 })}{\eps} +
\sum_{j\in \mathbb{Z}} u^{\eps}_j 
\frac{p^{\eps}(x_{j+\frac12 })-p^{\eps}(x_{j-\frac12 })}{\eps} +
\sum_{j\in \mathbb{Z}} \frac{\sigma}{\eps^2} u^{\eps}_j 
{\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx 
\end{eqnarray*}
In this method the third term
 $D_3$ is more complicated to study
\begin{equation*}
\begin{aligned}
 D_3 &= \sum_{j\in \mathbb{Z}}
\frac{u_{j+\frac12 }^{\eps}-u_{j-\frac12 }^{\eps}}{\eps} \: \bigg(
\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}}
 p^{\eps}(x) dx \bigg)
+\sum_{j\in \mathbb{Z}}
\frac{p_{j+\frac12 }^{\eps}-p_{j-\frac12 }^{\eps}}{\eps} \: \bigg(
\frac{1}{\Delta x_j} 
{\int_{x_{j-\frac12}}^{x_{j+\frac12}}}
 u^{\eps}(x) dx \bigg) \\
&+\sum_{j\in \mathbb{Z}} \Delta x_j  \frac{\sigma}{\eps^2}
\frac{u_{j+\frac12 }^{\eps}+u_{j-\frac12 }^{\eps}}{2}\: \bigg(
\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}}
 u^{\eps}(x) dx \bigg).
\end{aligned}
\end{equation*}
 It is  decomposed in several pieces.
We add and subtract in each fluxes  the value of the unknowns in the
cell. We also add and subtract to the two first  integrals the value of the
unknowns on the edge. Denoting by $\delta_{j }^\pm
\left( g \right) =\frac{1}{\Delta
x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} g(x) dx -g(x_{j\pm\frac12 })$,
 one gets after rearrangements
\begin{equation*}
\begin{aligned}
 D_3 &= \underset{j\in \mathbb{Z}}{\sum}
\frac{u_{j+\frac12 }^{\eps}-u_j^{\eps}}{\eps} \delta_{j}^+\left(p^{\eps}\right)+
\underset{j\in \mathbb{Z}}{\sum}
\frac{u^{\eps}_j-u_{j-\frac12 }^{\eps}}{\eps}\delta_{j}^-\left(p^{\eps}\right)
\\
& + \underset{j\in \mathbb{Z}}{\sum}
\frac{p_{j+\frac12 }^{\eps}-p_{j}^{\eps}}{\eps} 
\delta_{j}^+\left(u^{\eps}\right)
+\underset{j\in \mathbb{Z}}{\sum}
\frac{p^{\eps}_j-p_{j-\frac12 }^{\eps}}{\eps} 
\delta_{j}^-\left(u^{\eps}\right) \\
&-\underset{j\in \mathbb{Z}}{\sum} 
\frac{u^{\eps}(x_{j+\frac12 })-u^{\eps}(x_{j-\frac12 })}{\eps} 
p^{\eps}_j -
\underset{j\in \mathbb{Z}}{\sum} 
\frac{p^{\eps}(x_{j+\frac12 })-p^{\eps}(x_{j-\frac12 })}{\eps}
u^{\eps}_j  \\
&+ \underset{j\in \mathbb{Z}}{\sum} \Delta x_j  \frac{\sigma}{\eps^2}
\frac{u_{j+\frac12 }^{\eps}+u_{j-\frac12 }^{\eps}}{2}\: \bigg(
\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}}u^{\eps}(x) dx \bigg)
\end{aligned}
\end{equation*}
Using the fluxes' definition (\ref{eq:fflux}), one can 
eliminate the pressure fluxes. With
 a Young's inequality 
$ab\leq \alpha a^2+\frac1{4\alpha }b^2$ where $\alpha>0$, one gets 
$$
\underset{j\in \mathbb{Z}}{\sum}
\frac{p_{j+\frac12 }^{\eps}-p_{j}^{\eps}}{\eps}
\delta_{j }^+\left( u^{\eps}\right) 
= \sum_{j\in \mathbb{Z}} \frac{1}{\eps} (
u^{\eps}_j-u_{j+\frac12 }^{\eps})\delta_{j }^+\left( u^{\eps}\right) 
 -\frac{\sigma}{2\eps^2}
\sum_{j\in \mathbb{Z}}\Delta x_j 
u_{j+\frac12 }^{\eps}
\delta_{j }^+\left( u^{\eps}\right) 
$$
$$
\leq \alpha \sum_{j\in \mathbb{Z}}
\frac{(u_{j+\frac12 }^{\eps}-u_j^{\eps})^2}{\eps}
+
\left( \frac{1}{4\alpha\eps}+
\frac{\sigma}{2\eps^2}
\right)
\sum_{j\in \mathbb{Z}}
\delta_{j}^+\left(u^{\eps}\right)^2+
\frac\sigma{8\eps^2}
\sum_{j\in \mathbb{Z}}
\Delta x_j^2 \left( u_{j+\frac12 }^{\eps}\right)^2.
$$
Using this expression in $D_3$ and  using again Young's inequality, one gets for 
arbitrary  $\alpha>0$
\begin{equation*}
\begin{aligned}
 D_3 &\leq \alpha \sum_{j\in \mathbb{Z}}
\frac{(u_{j+\frac12 }^{\eps}-u_j^{\eps})^2}{\eps} + \alpha \sum_{j\in
\mathbb{Z}} \frac{(u_{j-\frac12 }^{\eps}-u_j^{\eps})^2}{\eps} \\
&+\sum_{j\in \mathbb{Z}} \bigg(
\left( \frac{1}{2\alpha\eps}+\frac{\sigma}{2\eps^2}\right)
\left( 
\delta_{j}^+\left(u^{\eps}\right)^2+
\delta_{j}^-\left(u^{\eps}\right)^2
\right)
+\frac{\delta_{j}^+\left(p^{\eps}\right)^2
+\delta_{j}^-\left(p^{\eps}\right)^2
}{2\eps\alpha} \bigg) \\
&+ \sum_{j\in \mathbb{Z}} \frac{1}{8\eps} \sigma \Delta x_j^2
\frac{(u_{j-\frac12 }^{\eps})^2 + (u_{j+\frac12 }^{\eps})^2}{\eps}+
\sum_{j\in \mathbb{Z}} \Delta x_j  \frac{\sigma}{\eps^2}
\frac{u_{j+\frac12 }^{\eps}+u_{j-\frac12 }^{\eps}}{2}\: \bigg(
\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}}u^{\eps}(x) dx \bigg) \\
&- \sum_{j\in \mathbb{Z}} u^{\eps}_j 
\frac{p^{\eps}(x_{j+\frac12 })-p^{\eps}(x_{j-\frac12 })}{\eps} -
\sum_{j\in \mathbb{Z}} p^{\eps}_j
\frac{u^{\eps}(x_{j+\frac12 })-u^{\eps}(x_{j-\frac12 })}{\eps}.
\end{aligned}
\end{equation*}
We now sum  all  bounds contributing to $\mathscr{E}'(t)$ and we get:
\begin{equation*}
\begin{aligned}
\mathscr{E}'(t)  & \leq (-1+\alpha) \sum_{j\in \mathbb{Z}}
\frac{(u_{j+\frac12 }^{\eps}-u_j^{\eps})^2+(u_{j-\frac12 }^{\eps}-u_j^{
\eps})^2}{\eps} \\
&+\sum_{j\in \mathbb{Z}} \bigg(
\left( \frac{1}{2\alpha\eps}+\frac{\sigma}{2\eps^2}\right)
\left ( \delta_{j}^+(u^{\eps})^2+\delta_{j}^-(u^{\eps})^2\right )
+\frac{\delta_{j}^+(p^{\eps})^2+\delta_{j}^-(p^{\eps}
)^2}{2\eps\alpha} \bigg) \\
&+ \sum_{j\in \mathbb{Z}} \frac{1}{8\eps} \sigma \Delta x_j^2
\frac{(u_{j-\frac12 }^{\eps})^2+ (u_{j+\frac12 }^{\eps})^2}{\eps}\\
&+ \sum_{j\in \mathbb{Z}} \Delta x_j  \frac{\sigma}{\eps^2}
\frac{u_{j+\frac12 }^{\eps}+u_{j-\frac12 }^{\eps}}{2}\: \bigg(
\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}}
 u^{\eps}(x) dx \bigg) 
-\sum_{j\in \mathbb{Z}} \Delta x_j \frac{\sigma}{2\eps}
\frac{(u_{j-\frac12 }^{\eps})^2+(u_{j+\frac12 }^{\eps})^2}{\eps} \\
&+ \sum_{j\in \mathbb{Z}} \Delta x_j  \frac{\sigma}{\eps^2} u^{\eps}_j \:
\bigg( \frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg) -
\sum_{j\in \mathbb{Z}} \Delta x_j \frac{ \sigma  }{\eps^2} \: \bigg(
\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} (u^{\eps})^2(x) dx \bigg).
\end{aligned}
\end{equation*} 
We now examine
 the sum of all terms in the two last lines of the RHS of the above inequality , which we denote $S$. 
One finds
$$
S
=-
\sum_{j\in \mathbb{Z}} \Delta x_j \frac{\sigma}{2\eps^2}
\left[
{
\left(u_{j-\frac12 }^{\eps}-\frac{1}{\Delta x_j}
{\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx\right)^2+
\left(u_{j+\frac12 }^{\eps}
-\frac{1}{\Delta x_j} {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx
\right)^2}\right]
$$
$$
+\frac{\sigma}{2\eps^2} \sum_{j\in \mathbb{Z}}\bigg( 
{\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg)\bigg( u_j^{\eps}-u_{j+\frac12 }^{\eps} +
u_j^{\eps}-u_{j-\frac12 }^{\eps}\bigg)
$$
$$ \leq \frac{\sigma}{2\eps^2} \sum_{j\in \mathbb{Z}}\bigg( 
{\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg)\bigg( u_j^{\eps}-u_{j+\frac12 }^{\eps} +
u_j^{\eps}-u_{j-\frac12 }^{\eps}\bigg).
$$
Using another Young's inequality, one has for all $\widehat{\alpha}>0$
\begin{eqnarray*}
S \leq \frac{\sigma^2}{8\widehat{\alpha}\eps^3} \sum_{j\in \mathbb{Z}}\bigg(
{\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg)^2+ \widehat{\alpha} \sum_{j\in
\mathbb{Z}} \frac{(u^{\eps}_j-u_{j+\frac12 }^{\eps})^2 +
(u^{\eps}_j-u_{j-\frac12 }^{\eps})^2}{\eps} .
\end{eqnarray*}
For example by choosing $\alpha=\frac12$ and $\widehat{\alpha}=\frac12$, and coming back to $\mathscr{E}'(t) $ we get
\begin{eqnarray} 
\mathscr{E}'(t)  & \leq & \sum_{j\in \mathbb{Z}} \bigg(
\left( \frac{1}{\eps}+\frac{\sigma}{2\eps^2}\right)\left ( 
\delta_{j}^+(u^{\eps})^2+\delta_{j}^-(u^{\eps})^2\right )
+\frac{1}{\eps}\left (
\delta_{j}^+(p^{\eps})^2+\delta_{j}^-(p^{\eps}
)^2\right ) \bigg)\label{eq:eqn1} \\
&+& \underset{j\in \mathbb{Z}}{\sum} \frac{1}{8\eps} \sigma \Delta x_j^2
\frac{(u_{j-\frac12 }^{\eps})^2+(
u_{j+\frac12 }^{\eps})^2}{\eps}+\frac{\sigma^2}{4\eps^3} \sum_{j\in
\mathbb{Z}}\bigg( {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg)^2.\label{eq:eqn2}
\end{eqnarray} 
To estimate the contributions on the first line we use the following fact:
for any quantity $q$, one can use  $q(x_{j-\frac12})=q(x)+\int_x^{x_{j-\frac12}}\frac{d}{ds}q(s)ds$
 and integrate this
expression in the cell $\Delta x_j$; we
get $ \sum_{j\in \mathbb{Z}} \delta_{j}^\pm(q)^2\leq
h \| q \|_{H^1(\mathbb{R})}^2$. 
Therefore
the first terms on the right hand side of (\ref{eq:eqn1}) can be estimated as
$$
\left( \frac{1}{\eps}+\frac{\sigma}{2\eps^2}\right)
\int_0^t 
\sum_{j\in \mathbb{Z}} 
\left ( 
\delta_{j}^+(u^{\eps})^2+\delta_{j}^-(u^{\eps})^2\right )dt
\leq 2 h \left( \frac{1}{\eps}+\frac{\sigma}{2\eps^2}\right)
\|  u^\eps  \|^2_{ L^2( [0,t] : H^1(\mathbb R)) }.
$$
Since $\| \mathbf u^\eps  \|^2_{ L^2( [0,t] : H^1(\mathbb R) )}\leq t \| \V^{\eps}(0) \|_{H^1(\mathbb{R})}^2$ and also 
$\frac{\sigma}
{\eps^2}\| \mathbf u^\eps  \|^2_{ L^2( [0,t] : H^1(\mathbb R) )}\leq  \| \V^{\eps}(0) \|_{H^1(\mathbb{R})}^2$
by (\ref{bee1}) and (\ref{bee2}), one gets that
\begin{equation} \label{eq:sm1}
\left( \frac{1}{\eps}+\frac{\sigma}{2\eps^2}\right)
\int_0^t 
\sum_{j\in \mathbb{Z}} 
\left ( 
\delta_{j}^+(u^{\eps})^2+\delta_{j}^-(u^{\eps})^2\right )dt
\leq  2 h  \left( \frac{t}\eps+\frac{1}{2}\right)
\| \V^{\eps}(0) \|_{H^1(\mathbb{R})}^2.
\end{equation}
A similar and simpler formula for the next terms is
\begin{equation} \label{eq:sm2}
 \frac{1}{\eps}\int_0^t 
 \sum_{j\in \mathbb{Z}} 
 \left (
\delta_{j}^+(p^{\eps})^2+\delta_{j}^-(p^{\eps}
)^2\right ) \leq  2  \frac{ h t}\eps 
\|  p^\eps  \|^2_{  H^1(\mathbb R) }
\leq   2  \frac{ h t}\eps 
\| \V^{\eps}(0) \|_{H^1(\mathbb{R})}^2.
\end{equation}
Next, using the assumption \eqref{geometrie1d} on the mesh and the estimate \eqref{estiuepsflux}, one controls the next term by 
\begin{equation} \label{eq:sm3}
 \int_0^T \underset{j\in \mathbb{Z}}{\sum} \frac{1}{8\eps} \sigma \Delta x_j^2
\frac{(u_{j-\frac12 }^{\eps})^2+(
u_{j+\frac12 }^{\eps})^2}{\eps} \leq  \frac{h}{4C_{\mathcal{M}}} \|\mathbf{V}^{\eps}(0)\|_{L^2(\mathbb{R})}^2.
\end{equation}
Finally the last term in (\ref{eq:eqn2}) can be bounded as %using the Jensen inequality 
$$
\frac{\sigma^2}{4\eps^3} \sum_{j\in
\mathbb{Z}}\bigg( {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg)^2
\leq 
\frac{\sigma^2}{4\eps^3}
h \|  u^\eps  \|^2_{  L^2(\mathbb R) }
$$
so that
\begin{equation} \label{eq:sm4}
\frac{\sigma^2}{4\eps^3} 
\int_0^t 
 \sum_{j\in
\mathbb{Z}}\bigg( {\int_{x_{j-\frac12}}^{x_{j+\frac12}}} u^{\eps}(x) dx \bigg)^2
\leq 
\frac{\sigma^2}{4\eps^3}
h \| u^\eps  \|^2_{  L^2([0,t]\times \mathbb R) }
\leq 
\frac{\sigma}{4\eps}
h  \|\mathbf{V}^{\eps}(0)\|_{L^2(\mathbb{R})}^2
\end{equation}
by means of the energy identity.
We note that 
\begin{equation} \label{eq:v0p0} 
\| \V^{\eps}( 0)
\|_{H^p(\mathbb{R})} \leq (1+\eps /\sigma) \| p_0
\|_{H^{p+1}(\mathbb{R})}  \leq (1+1/\sigma) \| p_0
\|_{H^{p+1}(\mathbb{R})} \quad \forall p\in \mathbb N.
\end{equation}
 So using (\ref{eq:sm1}-\ref{eq:sm4}) we obtain for all time $t\leq T$
\begin{equation*}
  \begin{aligned}
\mathscr{E}(t) &\leq
\mathscr{E}(0)+
\left( \frac{t}{\eps}+\frac{1}2 + \frac{t}{\eps}+\frac1{4C_\mathcal M}+\frac{\sigma
}{4\eps}\right)
h
\|
 \V^{\eps}(0) \|_{H^1(\mathbb{R})}^2 \\
& \leq (1+1 /\sigma)
\left( C_\mathcal A ^2 h +  \frac{2 t}{\eps}+\frac{1}2 +\frac1{4C_\mathcal M}+\frac{\sigma
}{4\eps}\right)
h
\| p_0 \|_{H^2(\mathbb{R})}^2
\end{aligned}
\end{equation*}
where 
the initialization stage is estimated  using (\ref{hyphen:1}).
%consistent (equation \ref{eq:initp1}) so 
% $\mathscr{E}(0)\leq h^2\| \V^{\eps}(0) \|_{H^1(\mathbb{R})}^2$.
 One   obtains 
\begin{equation*}\label{esti1}
\|\V_h^{\eps}-\V^{\eps} \|_{L^2([0,T]\times \mathbb{R})}
\leq 
\left( 
\sqrt{1+1 /\sigma }
\times \sqrt{C_\mathcal A^2 h\eps + 2T + \frac{\eps}{2} + \frac{\eps}{4C_{\mathcal{M}}} + \frac{\sigma}{4} } \right) \sqrt{ \frac{h}{\eps}}
\| \V^{\eps}( 0)
\|_{H^1(\mathbb{R})}.
\end{equation*}
The constant in parentheses is
$
%\sqrt{C_\mathcal A^2 h\eps + 2T + \frac{\eps}{2} + \frac{\eps}{4C_{\mathcal{M}}} + \frac{\sigma}{4} }=
\sqrt{1+1 /\sigma } \frac{   
\sqrt{C_\mathcal A^2 h \eps C_\mathcal M + 2TC_\mathcal M + \frac{\eps}{2} C_\mathcal M
+ \frac{\eps}{4} + \frac{\sigma}{4} C_\mathcal M }
 }{\sqrt {C_\mathcal M}} 
%$$
%$$
\leq 
\frac{ ~_\downarrow C }{\sqrt {C_\mathcal M}}  $
 with 
 $$ ~_\downarrow C = \sqrt{1+1/\sigma} \times
\sqrt{C_\mathcal A^2 + 2T  + \frac{1}{2} 
+ \frac{1}{4} + \frac{\sigma}{4} }.
$$ 
The proof is ended. % up to the redefinition of the constant $~_\downarrow C$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Study of $\| P_h^0 - P^0  \|$}

We  first 
 recall a fundamental   error estimate \cite{FV}
for the diffusion limit scheme (\ref{schemadifff}-\ref{eq:intip0}). 

\begin{lemma}\label{ds1d}
There exists a constant $C_\downarrow >0$ independent
of  %$\frac{\eps^2 T}{\sigma}$ and
 $h,\eps,C_\mathcal M$, such that the following estimate holds
\begin{equation}\label{esti_p_h_0_p_0}
\|\W_h^\eps-\W^\eps\|_{L^2([0;T]\times \mathbb{R})} \leq %C_{P_h^0 - P^0} 
C_\downarrow  \frac{e^{T/2\sigma} }{\sqrt{C_{\mathcal{M}}}}
h \|p_0\|_{H^2(\mathbb{R})}.
\end{equation}
\end{lemma}

\begin{proof}
We  use a  method that one can find in  Eymard-Gallouet-Herbin   \cite{FV}. It is % which
%is 
 based on  a notion of consistency for finite volumes schemes.
We set
$$
s_j=\partial_{xx}p(x_j)-\frac{\partial_xp(x_{j+\frac12}  )
-
\partial_xp(x_{j-\frac12}  )
}{\Delta x_j}
\mbox{ and }
r_{j+\frac12}=\partial_{x}p(x_{j+\frac12})-\frac{p(x_{j+1}  )
-
p(x_{j}  )
}{\Delta x_{j+\frac12}},
$$
so that one has the identity
$$
 \frac{d}{dt} p(x_j)
-\frac{1}{\sigma\Delta x_j}\bigg(\frac{p(x_{j+1})-p(x_{j})}{\Delta
x_{j+\frac12 }}-\frac{p(x_{j})-p(x_{j-1})}{\Delta
x_{j-\frac12 }}\bigg)=
\frac{s_j}\sigma+
\frac{r_{j+\frac12}-r_{j-\frac12}}{\sigma \Delta x_j}.
$$
We next  introduce the difference
$e_j=p(x_j)-p_j$ which satisfies
$$
 \frac{d}{dt} e_j
-\frac{1}{\sigma\Delta x_j}\bigg(\frac{e_{j+1}-e_{j}}{\Delta
x_{j+\frac12 }}-\frac{e_{j}-e_{j-1}}{\Delta
x_{j-\frac12 }}\bigg)=
\frac{s_j}\sigma+
\frac{r_{j+\frac12}-r_{j-\frac12}}{\sigma \Delta x_j}
$$
with $e_j(0)=0$ for all $j$.
By multiplying this equation by $e_j$
and  denoting by $\|e_h\|_{L^2(\mathbb{R})}^2= \sum_j \Delta x_je_j^2$,
one finds that 
\begin{equation*}\label{fgfg}
\frac12 \frac{d}{dt}\|e_h\|_{L^2(\mathbb{R})}^2 +\frac{1}{\sigma} \sum_j
\frac{(e_{j+1}-e_{j})^2}{\Delta
x_{j+\frac12 }}=
\frac{1}{\sigma}\sum_j \Delta x_j s_j e_j+\frac{1}{\sigma}\sum_jr_{j+\frac12 }(e_j-e_{j+1}).
\end{equation*}
The Cauchy-Schwarz inequality yields 
\begin{equation*}
\sum_jr_{j+\frac12 }(e_j-e_{j+1})\leq \frac{1}{2} \sum_j
\frac{(e_{j+1}-e_{j})^2}{\Delta x_{j+\frac12 }}+\frac{1}{2}\sum_j\Delta
x_{j+\frac12 }r_{j+\frac12 }^2.
\end{equation*}
One finds out with natural notations
\begin{equation} \label{eq:eee}
\frac12 \frac{d}{dt} \|e_h\|_{L^2(\mathbb{R})}^2+
\frac{1}{2 \sigma} \sum_j
\frac{(e_{j+1}-e_{j})^2}{\Delta x_{j+\frac12 }}\leq
\frac{1}{\sigma}\| s_h\|_{L^2(\mathbb{R})} \|e_h\|_{L^2(\mathbb{R})}+
\frac{1}{2\sigma} \|r_h\|^2_{L^2(\mathbb{R})}.
\end{equation}
Using the  definitions of  the truncation error $s_h$, one easily obtains by using classical arguments $\|s_h\|_{L^2([0,T]\times\mathbb{R})}\leq \sqrt{2}h\|\partial_{xxx}p\|_{L^2([0,T]\times\mathbb{R})}$: since $p$ satisfies the diffusion equation \eqref{difffff}, one gets $\|\partial_{xxx}p\|_{L^2([0,T]\times\mathbb{R})} \leq \sqrt{\sigma/2} \|\partial_{xx}p_0\|_{L^2(\mathbb{R})}$;
one gets  $\|s_h\|_{L^2([0,T]\times\mathbb{R})}\leq  \sqrt{ \sigma} h\|\partial_{xx}p_0\|_{L^2(\mathbb{R})} $.
 The same manipulations on the second truncation error $r_h$ yields
\begin{equation} \label{eq:bsr}
\|s_h\|_{L^2([0,T]\times\mathbb{R})}+
\|r_h\|_{L^2([0,T]\times\mathbb{R})} \leq %\sqrt{\sigma} h
\sqrt  \sigma  h
 \| p_0 \| _{H^2(\mathbb{R})} .
\end{equation}
%Rescaling  for convenience $\widehat s_h=\frac1hs_h$ for any quantity
%$s$, o
One gets the bound from (\ref{eq:eee})
$$
\frac12 \frac{d}{dt} \| %\widehat
 e_h\|_{L^2(\mathbb{R})}^2
\leq
\frac{1}{\sigma} \| %\widehat
 s_h\|_{L^2(\mathbb{R})} \| %\widehat 
 e_h\|_{L^2(\mathbb{R})}+
\frac{1}{2\sigma} \|  %\widehat 
r_h\|^2_{L^2(\mathbb{R})}
\leq
\frac{1}{2\sigma}
\left(\| %\widehat 
s_h\|_{L^2(\mathbb{R})}+  \|%\widehat 
r_h\|_{L^2(\mathbb{R})}  \right)^2+ \frac{1}{2\sigma}
 \|%\widehat
  e_h\|^2_{L^2(\mathbb{R})},
$$
where the term in parenthesis is bounded due to (\ref{eq:bsr}).
The Gronwall's inequality yields  boundedness of
$\|  %\widehat
 e_h\|_{L^2(\mathbb{R})}$ at any time.
The initial value is bounded using (\ref{hyphen:2}).
Therefore we obtain after some convenient simplifications with (\ref{eq:v0p0})
\begin{equation} \label{eq:e2:0}
\| e_h\|_{L^\infty([0,T]:L^2(\mathbb{R}))}\leq 
(1+1/\sigma) \left(
 1
+ C_\mathcal A 
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}
\right)
  e^{T/2\sigma} h \| p_0 \| _{H^2(\mathbb{R})}.
%+ C_\mathcal A h  
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}e^{T/2\sigma} .
 %+ \| e_h(0)\|_{L^2(\mathbb{R}))}.
\end{equation}
and 
\begin{equation} \label{eq:e2}
\| e_h\|_{L^2([0,T]\times\mathbb{R})}\leq   \sqrt{T}
(1+1/\sigma) \left(
 1
+ C_\mathcal A 
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}
\right)
  e^{T/2\sigma} h  \| p_0 \| _{H^2(\mathbb{R})}.
%+ C_\mathcal A h  
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}e^{T/2\sigma} .
 %+ \| e_h(0)\|_{L^2(\mathbb{R}))}.
\end{equation}
We also deduce from  (\ref{eq:eee}) %and (\ref{eq:e2}) 
\begin{equation*}
\begin{aligned}
\int_0^T  \sum_j
\frac{(e_{j+1}-e_{j})^2}{\Delta x_{j+\frac12 }} & \leq \sigma \|  %\widehat
 e_h(0)\|_{L^2(\mathbb{R})}^2+
\int_0^T  \left(\| %\widehat 
s_h\|_{L^2(\mathbb{R})}+  \|%\widehat 
r_h\|_{L^2(\mathbb{R})}  \right)^2
+
\int_0^T  \|  %\widehat
 e_h\|_{L^2(\mathbb{R})}^2\\
&\leq \big( \sigma+T \big) \| e_h(t)\|_{L^\infty([0,T];L^2(\mathbb{R}))}^2+
2\bigg(  \| %\widehat 
s_h\|_{L^2([0,T]\times\mathbb{R})}^2 +   \|%\widehat 
r_h\|_{L^2([0,T]\times\mathbb{R})}^2 \bigg)  \\
& \leq \left[  (\sigma+T)
(1+1/\sigma) ^2\left(
 1
+ C_\mathcal A 
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}
\right)^2
  e^{T/\sigma} +2\sigma \right]h^2  \| p_0 \| _{H^2(\mathbb{R})}^2
\end{aligned}
\end{equation*}
%This crude estimate is sufficient for our purposes. It yields, using the estimates \eqref{eq:bsr} and \eqref{eq:e2}
%\begin{equation*}
%\begin{aligned}
%\int_0^T \sum_j \Delta x_{j+\frac12 } \bigg( \frac{e_{j+1}-e_{j}}{\Delta x_{j+\frac12 }}\bigg)^2 & \leq h^2 \big(\sigma+T \big) e^{T/\sigma} 
%\bigg[ 
%\| p_0 \| _{H^2(\mathbb{R})}
%+ C_\mathcal A 
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}
%\bigg]^2 % \\
%%&
%+ 2\sigma h^2  \| p_0 \| _{H^2(\mathbb{R})}^2.
%%
% %h^2
%%\left(  \|\partial_{xx}p_0   \| _{L^2(\mathbb{R})}^2 +
%% \|\partial_{x}p_0   \| _{L^2(\mathbb{R})}^2 
%%\right)\bigg( \sigma + Te^{T/\sigma} \bigg) .
%\end{aligned}
%\end{equation*}
%
The other term 
that we must bound in (\ref{esti_p_h_0_p_0})  is
$
f_h=\eps\left(v(x_j)-v_j    \right)1_j(x) = - \eps
\left(  \frac{\partial_x p(x_j)}{\sigma} + v_j  \right)1_j(x)$ with  $v_j$ defined in \eqref{eq:vv}. It  yields
\begin{equation}\label{norme_fh_1D}
\| f_h \|_{L^2([0;T]\times \mathbb{R})} = \frac{\eps}{2\sigma}
\left(  \int_0^T \sum_j \Delta x_j \bigg| \frac{p_{j+1}-p_j}{\Delta x_{j+\frac12}} - \partial_x p(x_j) + \frac{p_{j}-p_{j-1}}{\Delta x_{j-\frac12}} - \partial_x p(x_j) \bigg|^2  \right)^\frac12
\end{equation}
where the definition of $e_j$ yields $\frac{p_{j+1}-p_j}{\Delta x_{j+\frac12}}-\partial_x p(x_{j})=
\left( \frac{e_{j+1}-e_j}{\Delta x_{j+\frac12 }} \right)  + \left(  \partial_x p(x_{j+\frac12}) - \partial_x p(x_{j})  \right) - r_{j+\frac12}$.
 One gets from the triangular inequality 
$$
\left( \int_0^T \sum_j \Delta x_{j+\frac12 } \bigg( \frac{p_{j+1}-p_{j}}{\Delta x_{j+\frac12 }} -\partial_x p(x_{j})  \bigg)^2 \right)^\frac12
$$
$$
  \leq  
   \left[  (\sigma+T)
(1+1/\sigma) ^2\left(
 1
+ C_\mathcal A 
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}
\right)^2
  e^{T/\sigma} +2\sigma \right] ^\frac12  h  \| p_0 \| _{H^2(\mathbb{R})}
  $$
  $$
  +  \int_0^T \sum_j \Delta x_{j+\frac12 } \bigg( \partial_x p(x_{j+\frac12}) - \partial_x p(x_{j})  \bigg)^2
  +  \| r_h \|_{L^2([0,T]\times\mathbb{R})}  .
  $$
Since $\left( \int_0^T \sum_j \Delta x_{j+\frac12 } \big( \partial_x p(x_{j+\frac12}) - \partial_x p(x_{j})  \big)^2
\right)  \leq h\sqrt{ \frac{\sigma}{2}}\| p_0\|_{H^1(\mathbb{R})}$ and  the estimate \eqref{eq:bsr} holds, 
one gets
\begin{equation} \label{eq:mlpo}
\left( \int_0^T \sum_j \Delta x_{j+\frac12 } \bigg( \frac{p_{j+1}-p_{j}}{\Delta x_{j+\frac12 }} -\partial_x p(x_{j})  \bigg)^2 \right)^\frac12
\end{equation}
$$
  \leq  
   \left(
   \left[  (\sigma+T)
(1+1/\sigma) ^2\left(
 1
+ C_\mathcal A 
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})}
\right)^2
  e^{T/\sigma} +2\sigma \right] ^\frac12 +\sqrt{ \frac{\sigma}{2}}
  +\sqrt \sigma  \right)h  \| p_0 \| _{H^2(\mathbb{R})}.
  $$
%\begin{equation*}
%\begin{aligned}
%\int_0^T \sum_j \Delta x_{j+\frac12 } \bigg( \frac{p_{j+1}-p_{j}}{\Delta x_{j+\frac12 }} -\partial_x p(x_{j})  \bigg)^2  \leq  &20\sigma h^2  \| p_0 \| _{H^2(\mathbb{R})}^2 + \frac{38}{3} h^2 \frac{\sigma}{2} \|\partial_x p_0\|_{L^2(\mathbb{R})}^2\\
%&\hspace{-3cm} + 2h^2 \big(\sigma+T \big) e^{T/\sigma} 
%\bigg[  \left(  \|\partial_{xx}p_0   \| _{L^2(\mathbb{R})}+
% \|\partial_{x}p_0   \| _{L^2(\mathbb{R})}
%\right)+ C_\mathcal A   
%\|\V^{\eps}(0) \|_{L^2(\mathbb{R})} \bigg]^2.
%\end{aligned}
%\end{equation*}
%Arranging the RHS and using the assumption \eqref{constante_mesh_1D} on the mesh, one writes this inequality as
%\begin{equation}\label{norme_intermediaire}
%\begin{aligned}
%\int_0^T \sum_j \Delta x_{j} \bigg( \frac{p_{j+1}-p_{j}}{\Delta x_{j+\frac12 }} -\partial_x p(x_{j})  \bigg)^2  \leq  & \frac{\sigma h^2}{C_{\mathcal{M}}} \bigg( \frac{91}{3} +4\frac{T}{\sigma}e^{T/\sigma} \bigg) \|p_0   \| _{H^2(\mathbb{R})}^2  \\
%&+ 4 \frac{\sigma h^2 C_{\mathcal{A}}^2}{C_{\mathcal{M}}}\bigg(1+\frac{T}{\sigma} \bigg) e^{T/\sigma}  \| \V^{\eps}(0) \|_{L^2(\mathbb{R})}^2 .
%\end{aligned}
%\end{equation}
%From this estimate one easily finds
%\begin{equation*}
%\begin{aligned}
%\int_0^T \sum_j \Delta x_{j} \bigg( \frac{p_{j}-p_{j-1}}{\Delta x_{j-\frac12 }} -\partial_x p(x_{j})  \bigg)^2  \leq  \frac{2}{C_{\mathcal{M}}}\int_0^T \sum_j \Delta x_{j} \bigg( \frac{p_{j+1}-p_{j}}{\Delta x_{j+\frac12 }} -\partial_x p(x_{j})  \bigg)^2 + h^2 \sigma \|\partial_x p_0\|_{L^2(\mathbb{R})}^2.
%\end{aligned}
%\end{equation*}
Taking into account that the weight $\Delta x_j$  \eqref{norme_fh_1D} is different from the weight
$\Delta x_{j+\frac12}$ in (\ref{eq:mlpo}), one gets 
% \eqref{norme_intermediaire}
\begin{equation}\label{eq:f2}
\| f_h \|_{L^2([0;T]\times \mathbb{R})}\leq \frac\varepsilon {\sqrt { C_\mathcal M} } C e^{T/2\sigma }
h  \| p_0 \| _{H^2(\mathbb{R})}\leq \frac1 { \sqrt { C_\mathcal M} } C e^{T/2\sigma }
h  \| p_0 \| _{H^2(\mathbb{R})}
\end{equation}
where the constant $C$ is bounded in function of $C_\mathcal A$.
Finally, the difference 
$\|\W_h^\eps-\W^\eps\|_{L^2([0;T]\times \mathbb{R})}^2=
\|e_h\|_{L^2([0;T]\times \mathbb{R})}^2+\|f_h\|_{L^2([0;T]\times \mathbb{R})}^2
$ 
is bounded using (\ref{eq:e2}) and (\ref{eq:f2}).
It ends the proof.
\end{proof}


\subsection{Study of $\| P_h^\varepsilon - P^0_h  \|$}

In this section we 
 prove an error estimate between the solution of the
scheme \eqref{schema} and the solution of the diffusion scheme
\eqref{schemadifff}.
It is necessary  to use some comparison
estimates between the initial data of $P_h^\eps$ and $P_h^0$.

\begin{lemma} \label{gttods}
%Under assumption \ref{geometrie1d}, t
There exists a constant $C^\rightarrow >0$  independent  of $h,\eps,C_\mathcal M$ such that the following estimate holds
\begin{equation}\label{p_h_eps_p_h_0_1d}
\|\V_h^{\eps}-\W^\eps_h\|_{L^2([0,T]\times\mathbb R)} \leq % C_{P_h^\varepsilon - P^0_h}
  \frac{C^\rightarrow e^{T/2\sigma}}{ { C_\mathcal M} } \eps    \|p_0\|_{H^2(\mathbb{R})}.
\end{equation}
\end{lemma}

\begin{proof}
For practical reasons we use the formulation 
(\ref{schema}) of the hyperbolic scheme  which is equivalent to 
(\ref{schema1}-\ref{eq:fflux})
%(\ref{schemadifff}-\ref{eq:vv}) 
and we reformulate the diffusion scheme (\ref{schemadifff}-\ref{eq:vv})
as 
 \begin{equation}\label{schemadiff3}
\left\{
\begin{array}{lll}
\displaystyle 
\Delta x_j\frac{d}{dt} p_j
+\frac{u_{j+\frac12 }-u_{j-\frac12 }}{\eps }=0,\\
\\
\displaystyle 
\Delta x_j\frac{d}{dt} u_j
-\frac{u_{j+\frac12 }+u_{j-\frac12 }}{\eps}+\frac{2}{\eps }u_j=\Delta x_j R_j,\\
\\
\displaystyle 
p_j-p_{j+1}+u_j+u_{j+1} =2u_{j+\frac12 } +\sigma \Delta x_{j+\frac12 }
\frac{u_{j+\frac12 } }{\eps}+\Delta x_{j+\frac12}S_{j+\frac12 } , \\
\displaystyle 
u_{j+\frac12}= -\frac\eps\sigma \frac{p_{j+1}-p_j}{\Delta x_{j+\frac12}}, \\
\displaystyle 
u_j=\frac{u_{j+\frac12}+u_{j-\frac12}}{2},
\end{array}
\right.
\end{equation}
where the error terms are $R_j$ and $S_{j+\frac12 }$.
A simple computation using the  last two identities in (\ref{schemadiff3})
yields
\begin{equation*}
R_j= \frac{d}{dt} u_j
\mbox{ and }
S_{j+\frac12 }= \frac1{\Delta x_{j+\frac12} }\left(
u_j+u_{j+1} -2u_{j+\frac12 }\right).
\end{equation*}
{One has  from the triangular inequality applied to $u_j=\frac{u_{j+\frac12}+u_{j-\frac12}}{2}$
%Thanks to the estimate \eqref{l2} and since the scheme is linear, $\frac{d}{dt} u_h = (\frac{d}{dt} u_j)_{j\in\mathbb{Z}}$
%satisfies the estimate 
$$
\bigg\|\frac{d}{dt} u_h\bigg\|_{L^2([0,T]\times \mathbb{R})} = \sqrt{\int_0^T \sum_{j\in\mathbb{Z}} \Delta x_j \left| \frac d{dt}u_j\right|^2 }
$$
$$
\leq
\frac1{\sqrt{C_\mathcal M}}
\sqrt{\int_0^T \sum_{j\in\mathbb{Z}} \Delta x_{j+\frac12} \left| \frac d{dt}u_{j+\frac12}\right|^2 }
=
\sqrt{ \frac\eps{{C_\mathcal M}} }
\sqrt{\int_0^T \sum_{j\in\mathbb{Z}} \Delta x_{j+\frac12} \left| \frac d{dt}v_{j+\frac12}\right|^2 }.
%\eps \sqrt{ \frac{\sigma}{2C_{\mathcal{M}} }} \bigg\|\frac{d}{dt} p_h(0)\bigg\|_{L^2(\mathbb{R})}.
$$
Since the scheme is invariant with respect
to the time variable,  one can apply (\ref{l2}) to the derivative with respect to time.
It yields
$$
\sqrt{\int_0^T \sum_{j\in\mathbb{Z}} \Delta x_{j+\frac12} \left| \frac d{dt}v_{j+\frac12}\right|^2 }
\leq
\sqrt{\frac\sigma 2  }
 \bigg\|\frac{d}{dt} p_h(0)\bigg\|_{L^2(\mathbb{R})}\leq \frac{1}{\sqrt{2 \sigma}}
\| p_0\|_{H^2(\mathbb{R})}
$$
where the last inequality is from the well preparedness of the initial data, as detailed in  
proposition \ref{pro:H1}.
%The definition of $\dt p_h(0)$ is from (\ref{schemadifff}-\ref{eq:intip0}),
%which
%implies that $\|\frac{d}{dt} p_h(0)\|_{L^2(\mathbb{R})}\leq \frac{1}{\sigma}
%\| p_0\|_{H^2(\mathbb{R})}$  (proposition \ref{pro:H1}). 
So one has the bound
\begin{equation} \label{eq:p5}
\|R\|_{L^2([0,T]\times \mathbb{R})}\leq   \frac{\eps }{ \sqrt{2 \sigma C_{\mathcal{M}} } } 
\| p_0\|_{H^2(\mathbb{R})}.
\end{equation}
}
Using the definitions of $u_j$ \eqref{schemadiff3}, $S_{j+\frac12}$ can be written in terms of $\frac{d}{dt}p_j$ and $\frac{d}{dt}p_{j+1}$
\begin{equation*}
S_{j+\frac12} = \frac{\eps}{2} \bigg( \frac{\Delta x_{j} }{\Delta x_{j+\frac12} }\frac{d}{dt}p_j - \frac{\Delta x_{j+1} }{\Delta x_{j+\frac12} }\frac{d}{dt}p_{j+1} \bigg).
\end{equation*}
Using  the technical proposition \ref{pro:H1}, one finds out that $S = (S_{j+\frac12})_{j\in\mathbb{Z}}$ satisfies
\begin{equation}\label{eq:xx}
\| S \|_{L^\infty([0,T]:L^2(\mathbb{R}))} \leq \frac{ \eps}{C_\mathcal M}
\left\| \frac d{dt}p\right\|_{L^\infty([0,T]: L^2(\mathbb{R}))}
%\leq \frac{ \eps}{C_\mathcal M}
%\left\| \frac d{dt}p\right\|_{L^\infty([0,T]: L^2(\mathbb{R}))}
%m
\leq    \frac{ \eps}{\sigma C_\mathcal M}    %  \frac{\sqrt{2}}{\sigma \sqrt{C_{\mathcal{M}}}}
\| p_0\|_{H^2(\mathbb{R})}.
\end{equation}
We now introduce the differences
\begin{equation} \label{eq:eff}
e_j= p_j-p_{j}^{\eps} ,\quad
f_j = u_j-u_j^{\eps} \mbox{ and }
f_{j+\frac12 }= u_{j+\frac12 }-u_{j+\frac12 }^{\eps}.
\end{equation}
Let us  look at the difference between the scheme \eqref{schema} and
\eqref{schemadiff3}. We get
\begin{equation*}\label{diff222}
\left\{
\begin{array}{lll}
\Delta x_j\frac{d}{dt} e_j
+\frac{f_{j+\frac12 }-f_{j-\frac12 }}{\eps}=0,\\
\\
\Delta x_j\frac{d}{dt} f_j
-\frac{f_{j+\frac12 }+f_{j-\frac12 }}{\eps}+\frac{2}{\eps}f_j=
\Delta x_j
R_j,\\
\\
e_j-e_{j+1}+f_j+f_{j+1} -2f_{j+\frac12 } -\sigma \Delta x_{j+\frac12 }
\frac{f_{j+\frac12 } }{\eps}=\Delta x_{j+\frac12}S_{j+\frac12 } .
\end{array}
\right.
\end{equation*}
We use the notation
$\|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}^2=\sum_j \Delta x_j
(e_j^2+f_j^2)$.
Using the same kind of proof than for the $L^2$ stability of 
proposition \ref{prodentrgosse}, one gets that
\begin{eqnarray*}
\frac12 \frac{d}{dt}\|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}
^2 \leq \sum_j \Delta x_j R_jf_j -\sum_j\Delta x_{j+\frac12}
\frac{f_{j+\frac12 }}{\eps}S_{j+\frac12 }
-\frac{\sigma}{\eps^2}\sum_{j\in \mathbb{Z}} \Delta
x_{j+\frac12}f_{j+\frac12 }^2.
\end{eqnarray*}
Using a Young's inequality on the second 
term of the right side of this inequality,
one finds out that 
\begin{eqnarray} \label{eq:plugin}
\frac12 \frac{d}{dt}\|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}
^2 \leq \sum_j \Delta x_j R_jf_j 
 +
\frac1{4\sigma  }
\sum_j \Delta x_{j+\frac12} 
{S_{j+\frac12 }^2}
%-\frac{\sigma}{2\eps^2}\sum_{j\in \mathbb{Z}} \Delta
%x_{j+\frac12}f_{j+\frac12 }^2
.
\end{eqnarray}
%To simplify the structure of the term $ \sum_j \Delta x_j R_jf_j $ we define $U_j=
%f_j-\frac12\left(  f_{j+\frac12} + f_{j-\frac12}\right)$.
%By definition (\ref{eq:eff}), one gets
%$U_j=\frac12 \left( u_{j+\frac12}^\eps -u_j^\eps \right)+
%\frac12 \left( u_{j-\frac12}^\eps -u_j^\eps \right)$.
%This quantity is easily estimated with (\ref{estiuepsflux2}). It yields
%$$
%\left\|  U \right\|_{L^\infty( [0,T]:L^2(\mathbb R)    )}\leq h\sqrt \eps \left\|  \mathbf V_h^\eps(0) \right\|_{L^2(\mathbb R)}
%\leq (1+1/\sigma+  C_\mathcal A h)h\sqrt \eps   \left\|   p_0 \right\|_{H^2(\mathbb R)}
%$$
%where the last  bound on $ \left\|  \mathbf V_h^\eps(0) \right\|_{L^2(\mathbb R)}$ is from (\ref{hyphen:1}) and 
%(\ref{eq:v0p0}).
%Therefore one can write
One has also
$
\sum_j \Delta x_j R_jf_j = \frac\sigma {2 }\sum_j \Delta x_j R_j^2+ \frac1{2\sigma }\sum_j \Delta x_jf_j^2
%\sum_j \Delta x_j R_jU_j + \frac12 \sum_j \Delta x_j R_j  f_{j+\frac12} +  
%\frac12 \sum_j \Delta x_j R_j  f_{j-\frac12}
$ where the constant $\sigma$ in this decomposition is somewhat arbitrary.
It has been chosen in order to obtain at the end of the Gronwall lemma a similar exponential dependance  
$e^{T/2\sigma}$.
%$$
%\leq  \frac12 \left\| R  \right\|_{L^2(\mathbb R)} ^2 +\frac12  \left\| T \right\|_{L^2(\mathbb R)} ^2
%+\frac{\eps^2}{4\sigma C_\mathcal M} \left\| R  \right\|_{L^2(\mathbb R)}^2+
%\frac{\sigma}{4\eps^2}\sum_{j\in \mathbb{Z}} \Delta
%C_\mathcal Mx_{j}f_{j+\frac12 }^2
%$$
%$$
%+\frac{\eps^2}{4\sigma  C_\mathcal M} \left\| R  \right\|_{L^2(\mathbb R)}^2+
%\frac{\sigma}{4\eps^2}\sum_{j\in \mathbb{Z}} \Delta
%C_\mathcal Mx_{j}f_{j-\frac12 }^2
%$$
%$$
%\leq \frac12 \left\| R  \right\|_{L^2(\mathbb R)}^2 +\frac12   \left\| U \right\|_{L^2(\mathbb R)}^2
%+\frac{\eps^2}{2\sigma C_\mathcal M} \left\| R  \right\|_{L^2(\mathbb R)}^2
%+\frac{\sigma}{4\eps^2}\sum_{j\in \mathbb{Z}} \Delta
%x_{j+\frac12}f_{j+\frac12 }^2.
%$$
Plugging in (\ref{eq:plugin}) 
%one gets
%$$
%\frac12 \frac{d}{dt}\|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}
%^2 \leq \sum_j \Delta x_j R_jf_j  +
%\frac1{2\sigma  }
%\sum_j \Delta x_{j+\frac12} 
%{S_{j+\frac12 }^2}
%.
%$$
%
%Using the Cauchy-Schwarz inequality, 
we thus have
\begin{eqnarray*}
 \frac{d}{dt}\|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}
^2 \leq  %\frac1\sigma \|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}^2 
% \left(
%1+\frac{\eps^2}{\sigma C_\mathcal M}
% \right)
\sigma  \left\| R  \right\|_{L^2(\mathbb R)} ^2 
+ \frac1{2\sigma}\| S \|_{L^2(\mathbb R)}^2 + \frac1\sigma \|\V_h^{\eps}-\W^\eps_h\|_{L^2(\mathbb R)}
^2 .
\end{eqnarray*}
A crude use of the  Gronwall lemma shows that 
\begin{equation*}
\|\V_h^{\eps}(t)-\W^\eps_h(t)\|_{L^2(\mathbb R)}^2 \leq 
  e^{T/\sigma }  \left(  \|\V_h^{\eps}(0)-\W^\eps_h(0)\|_{L^2(\mathbb R)}^2 
+\sigma   \left\| R  \right\|_{L^2([0,T]\times \mathbb R)} ^2 
+ \frac1{2\sigma}\| S \|_{L^2([0,T]\times\mathbb R)}^2 
  \right).
% $$
% $$
%+ 
% \left(
%1+\frac{\eps^2}{\sigma C_\mathcal M}
% \right)
% \left\| R  \right\|_{L^2([0,T]\times \mathbb R)} ^2 
%+ \frac1{\sigma}\| S \|_{L^2([0,T]\times\mathbb R)}^2 + \left\| U \right\|_{L^2([0,T]\times\mathbb R)}^2
\end{equation*}
Finally, using the previous estimates on $R$, $S$ and $T$
and the well-preparedness of the data (\ref{init_1D}), one gets
$$
\|\V_h^{\eps}-\W^\eps_h\|_{L^2([0,T]\times\mathbb{R})}^2 \leq 
e^{T/\sigma }  \left(  (C_\mathcal A h \eps  )^2\|p_0\|_{H^2(\mathbb{R})} ^2+
%\left(
%1+\frac{\eps^2}{\sigma C_\mathcal M}
 %\right)
   %\left( 
   \frac{\eps^2 }{ {2 C_{\mathcal{M}} } }  
\| p_0\|_{H^2(\mathbb{R})}^2
\right.
%\eps \big(  C_\mathcal Ah + \sqrt{ \frac{1}{\sigma C_{\mathcal{M}} } ( \frac12 + \frac{1}{\sigma^2} )}  \big) \|p_0\|_{H^2(\mathbb{R})} .
%$$
%$$
\left.
+%\frac1{4\sigma } 
%\left(
 \frac{ \eps ^2}{4\sigma ^3C_\mathcal M^2}  %  \right)^2
\| p_0\|_{H^2(\mathbb{R})}^2 
\right)
%+ 
 %(1+1/\sigma+  C_\mathcal A h)^2h^2  \eps   \left\|   p_0 \right\|_{H^2(\mathbb R)}^2.
$$
%Discarding the $\eps$ in the last term, 
The proof is ended.
\end{proof} 



\begin{proposition}[Technical result]\label{pro:H1} 
The bound $
\sqrt{ \sum_j \Delta x_j (\frac{d}{dt}p_j)^2(t) }
\leq \sigma^{-1}\|p_0\|_{H^2(\mathbb{R})}$
 holds at any time.
 \end{proposition}

\begin{proof}

By linearity of the diffusion scheme, $z_h=\frac{d}{dt} p_h$ is solution of
$P_h^0$:
$$
\Delta x_j\frac{d}{dt} z_j -\frac{1}{\sigma } \bigg(
\frac{z_{j+1}-z_j}{\Delta x_{j+\frac12 }}-\frac{z_j -z_{j-1}}{\Delta
x_{j-\frac12 }}\bigg)=0,
$$
with initial condition
\begin{equation}\label{eq:zz}
z_j(0)=\frac{d}{dt} p_0(x_j)=
\frac1 {\Delta x_j \sigma } \bigg(
\frac{p_0(x_{j+1})-p_0(x_j)}{\Delta x_{j+\frac12 }}-\frac{p_0(x_j)
 -p_0(x_{j-1}  )}{\Delta
x_{j-\frac12 }}\bigg).
\end{equation}
One gets from a Taylor expansion with integral residue
that
$$
\left|
\frac{p_0( x_{j+1}) -p_0(x_j)}{\Delta x_{j+\frac12 }}- 
\partial_x p _0(x_j)
\right|\leq 
 \int_{x_j}^{x_{j+1}  } \left|\partial_{xx}p_0(y)   \right| dy .$$ 
Similarly one has the bound
$
\left|
 \frac{p_0( x_{j}) -p_0(x_{j-1})}{\Delta x_{j+\frac12 }}-
\partial_x p _0(x_j)\right|
\leq
 \int_{x_{j-1}}^{x_{j}  } \left|\partial_{xx}p_0(y)   \right| dy $.
Therefore $|z_j(0)|\leq
\frac1 {\Delta x_j \sigma }  \int_{x_{j-1}}^{x_{j+1}  } \left|\partial_{xx}p_0(y)  
 \right| dy $ from which the bound
$
\sqrt{ \sum_j \Delta x_j z_j^2(0) }
\leq \sigma^{-1}\|p_0\|_{H^2(\mathbb{R})}$
is deduced.
Since the scheme $P_h^0$ is stable in $L^2$, this bound is true at any time.
Considering (\ref{eq:zz}) the discrete second derivative
attached to $P_h^0$ is bounded at any time, which ends the proof
of the claim.
\end{proof} 

\subsection{End of the proof of uniform AP property}


\begin{theorem} \label{theor:1d}
Assuming a sufficiently smooth well prepared initial data,
the scheme $P_h^\eps$ converges to $P^\eps$ at order at least 
$\frac13$ in $L^2([0,T]\times \mathbb{R}   )$, uniformly with respect to $\eps$
\end{theorem}
\begin{proof}
%All the previous estimates
%show that (\ref{eq:mod3}-\ref{eq:mod4})  are true
%with
%$a=1$, $b=c=\frac12$ and $d=1$.
%Moreover $\|P_h^\varepsilon-P_h^0  \|\leq C(T) \eps $
%by proposition (\ref{gttods}) which means this term has the same
%scaling as $\|P^\varepsilon-P^0  \|\approx C(T) \eps$.
%So it can be  incorporated 
%in  estimate (\ref{eq:mod3}) with $a=1$.
%Using the general
%method described at the beginning of this work in proposition \ref{prop:1}, 
%one  obtains the convergence
%estimate of convergence
%$
%\|\mathbf V_h^\eps -\mathbf V^\eps   \|_{L^2([0,T]\times \mathbb{R}   )  }
%\leq C(T) h^{ q }
%$
%with the order of convergence
%$
%q=\frac{ac}{a+b}=\frac13$.
%\end{proof} 
%~~\\
%\begin{proof}[ALTERNATIVE PROOF]~~\\
%\\
All the previous estimates
show that (\ref{eq:mod3}-\ref{eq:mod4})  are true
with
$a=1$, $b=c=\frac12$ and $d=1$. More specifically, estimates \eqref{esti_naive_1d}, \eqref{p_h_eps_p_h_0_1d}, \eqref{esti_p_h_0_p_0} and \eqref{esti_p_eps_p_0} shows that
\begin{equation*}
\|\V^{\eps}-\V^{\eps}_h \|_{L^2([0,T]\times \mathbb{R})}
\leq C \min\bigg( \sqrt \frac{h}\eps  \: , \:  h + 2 \eps   \bigg) \| p_0 \|_{H^3(\mathbb{R})}
\end{equation*}
where
\begin{equation*}
C =  \hbox{max } \bigg[ \frac{~_\downarrow C }{\sqrt{C_\mathcal M}} ,
\frac{ C^\rightarrow e^{T/2\sigma}}{C_\mathcal M} ,
\frac{    C_\downarrow  e^{T/2\sigma} } {\sqrt{C_\mathcal M}} ,
   C_\leftarrow   \bigg] 
\end{equation*}
Using the general
method described at the beginning of this work in proposition \ref{prop:1}, 
one  obtains the convergence
estimate 
$
\|\mathbf V_h^\eps -\mathbf V^\eps   \|_{L^2([0,T]\times \mathbb{R}   )  }
\leq C(T) h^{ q }
$
with the order of convergence
$
q=\frac{ac}{a+b}=\frac13$.


\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The 2D case}

In this section we prove the uniform 
convergence of the solution of the diffusion
AP 
 scheme introduced in \cite{glaceap} to the solution
of the hyperbolic heat equation.
The structure of our proof  
is globally the same
as in the previous section. However two major  difficulties
will be treated: a) the first one 
consists in  the adaptation to our problem
 of a  combination of specific finite volumes 
techniques for hyperbolic and parabolic equations;
b) the second one is to derive new bounds for the scheme $\mathbf{DA}_h^\eps$.

The model problem is the hyperbolic heat equation in the domain
$\Omega= ]0,1[^2$
with periodic boundary conditions and  well-prepared data
\begin{equation*}\label{HHE!}
\mathbf P^\eps: \qquad 
\left\{\begin{array}{l}
\ds\partial_t p^{\eps}+\frac{1}{\eps}\operatorname{div}(\uu^{\eps})=0,\\
\\
\ds\partial_t \uu^{\eps}+\frac{1}{\eps}\nabla
p^{\eps}=-\frac{\sigma}{\eps^2}\uu^{\eps},\\
\\
p^{\eps}(t=0)=p_0\mbox{,  }\uu^{\eps}(t=0)=\uu^{\eps}_0=
-\frac{\eps}\sigma
\nabla p_0.
 \end{array}\right.
\end{equation*}
When $\eps$ tends to zero,
this problem  admits the following diffusion  limit
\begin{equation*}\label{HHE!lim}
\mathbf P^0: \qquad
\ds\partial_t p-\frac{1}{\sigma}\operatorname{div}(\nabla{p})=0
, \qquad 
p(t=0)=p_0.  
\end{equation*}
The rescaled gradient is 
$
\mathbf v=-\frac{1}{\sigma} \nabla{p}$.
We will admit the following proposition, the proof of which can be easily obtained 
by a method similar to the one of proposition
\ref{heqtode}.

\begin{proposition}\label{heqtode2d}
The error between the two solutions
can be  upper bounded by
\begin{equation}\label{ine3}
\| p^\epsilon-p\|_{   L^\infty([0,T];H^n(\Omega))  }+
\| \mathbf{v} 
 \|_{   L^\infty([0,T]; H^n(\Omega))  }
 \leq
\frac{T}{\sigma^2} \eps
\|p_0 \|_{H^{3+n}(\Omega)},
\qquad n \in \mathbb N.
\end{equation}
\end{proposition}
\begin{proof}
The structure of the 
proof in the $L^\infty( [0,T];L^2(\Omega))$ norm is the same as the one
of proposition \ref{heqtode}.
Since the coefficients of the problem are constant, similar bounds are obtained
at any order of derivation which proves the estimate for any $n>0$.
\end{proof}


\subsection{Definition of $\mathbf P_h^\eps$}


Let us consider an unstructured mesh in dimension 2. The mesh is defined
by a finite number of vertices $\xr$ and cells $\Omega_{j}$. We
denote $\xj$ a point  chosen arbitrarily inside $\Omega_j$.
For simplicity we will call this point the center of the cell.
 By convention the vertices
are listed counter-clockwise $\x_{r-1},\xr,\x_{r+1}$
with
coordinates $\xr=(x_{r},y_{r})$. 
We note  $l_{jr}\njr$ the vector as follows
\begin{equation}\label{eq:not}
l_{jr}=\frac12 \mbox{dist}\left( \x_{r-1},\x_{r+1}\right) 
\mbox{ and }
\njr=\frac{1}{2l_{jr}}\left(
\x_{r+1}-\x_{r-1}
\right)^\perp.
\end{equation}
This notion of a corner vector can be rigorously  introduced  also in any dimension
using the  definition \cite{de10}.
The scalar product of two vectors is denoted as
$(\x,\mathbf{y})$.

\begin{figure}[ht!]
\begin{center}
\scalebox{0.7}{\includegraphics{atoc.pdf}}
\end{center}

\caption{Notation for node formulation. The corner length $l_{jr}$ 
and the corner normal $\njr$ are defined in equation (\ref{eq:not}).
The point $\x_j$ is an arbitrary point inside the cell, typically
the centroid of the cell or an averaged of the corners.
}
\end{figure}







The numerical approximation  
 of the problem  $\mathbf P^\eps$ that we study 
 is the JL-(b) scheme defined
in   \cite{glaceap}
\begin{equation}\label{eq:4}
\mathbf P_h^\eps:
\qquad \left\{ 
\begin{array}{l}
\displaystyle \mid\Omega_{j}\mid\frac{d}{dt} p_{j}^{\eps}+\frac{1}{\varepsilon}
\sum_{r}l_{jr}(\ur^{\eps},\njr)=0\\
\displaystyle \mid
\Omega_{j}\mid\frac{d}{dt}\uj^{\eps}+\frac{1}{\varepsilon}
\sum_{r} l_{jr} \njr    p_{jr}^{\eps}=-\frac{\sigma}{\varepsilon^{2}}\sum_{r}\widehat{
\beta}_{jr}\ur^{\eps},
\end{array}
\right.
\end{equation}
with for simplicity point wise  initial data $p_j^{\eps}(0)=p_0(\mathbf{x}_j)$ and $\mathbf{u}_j^{\eps}(0) = -\eps\sigma^{-1}\nabla p_0(\mathbf{x}_j)$.
The fluxes are defined by the so-called 
corner problem 
\begin{equation}\label{eq:5}
\left\{ \begin{array}{l}
\displaystyle 
p_{jr}^{\eps}-p_j^\eps=
(\njr,    \uj^{\eps}-\ur^{\eps}
)-\frac{\sigma}{\varepsilon}( \xr -\xj, \ur^{\eps}), 
\\
\sum_{j}\ljr
p_{jr}^{\eps}\njr=0.
 \end{array}\right.
\end{equation}
This corner problem has been introduced in 
\cite{glaceap} as a multidimensional version
of the 1D Jin-Levermore technique   \cite{jinlev}.
Its solution is provided by the solution
of the  linear system
\begin{equation} \label{eq:5bis}
\displaystyle \left( \sum_{j}\alj+ 
\sum_{j}\frac{\sigma}{\varepsilon}\bej\right)\ur^{
\eps}= \sum_{j}\ljr
p_j^{\eps}\njr+\sum_j\alj\uj^{\eps},
\end{equation}
 where the geometry of the mesh is used to define the  
  matrices  
$\alj$ and $\bej$ 
\begin{equation}\label{eq:6}
\alj=\ljr\njr\otimes\njr,\mbox{ and
}\bej=\ljr\njr\otimes(\xr-\xj) .
\end{equation}
We will use the notations
$A_j=\sum_r\alj$, $A_r=\sum_j\alj$ and
$B_r=\sum_j\bej$. 
By comparison with the scheme $P_h^\eps$ in dimension one, one sees %at once
that the multi-dimensional scheme (\ref{eq:4}-\ref{eq:6})  is  more 
tricky than the 1D scheme (\ref{schema1}-\ref{eq:fflux}).



Starting from (\ref{eq:4}) and taking into account of the definitions of
the fluxes (\ref{eq:5}) and also the identity $\sum_r l_{jr} \njr=0$,
the scheme $\mathbf P_h^\eps$ can also be rewritten as
\begin{equation}\label{eq:4bis}
\mathbf P_h^\eps:
\qquad
\left\{ 
\begin{array}{l}
\displaystyle \mid\Omega_{j}\mid\frac{d}{dt} p_{j}^{\eps}+\frac{1}{\varepsilon}
\sum_{r} l_{jr}(\ur^{\eps},\njr)=0\\
\displaystyle \mid
\Omega_{j}\mid\frac{d}{dt}\uj^{\eps}+\frac{1}{\varepsilon}
\sum_{r}l_{jr} (\njr,\uu_r^{\eps} -\uu_j^{\eps})\njr=0
\end{array}
\right.
\end{equation}
When $\eps \rightarrow 0$
the scheme $\mathbf P_h^\eps$, see (\ref{eq:4}) or (\ref{eq:4bis}), admits the limit diffusion scheme 
$\mathbf P_h^0$ 
\begin{equation}\label{diffglace}
\mathbf P_h^0:
\qquad\left\{ \begin{array}{l}
\displaystyle \left| \Omega_j 
\right| \frac{d}{dt} p_j+ 
\sum_{r} l_{jr}\bigg( 
\vv_r,\njr\bigg)=0,\\
\displaystyle \vv_r=\frac{1}{\sigma} B_r^{-1}
\sum_{j}l_{jr}p_j\njr,
\end{array}\right.
\end{equation}
with $B_r=\sum_{j}l_{jr}\njr\otimes(\xr-\xj)$.
We  define additionally $\vv_j$ by a kind of mean
$$
\left( \sum_r \alj \right) \vv_j=\sum_r \alj
\vv_r.
$$
This is well defined since the matrix
$\sum_r \alj$ is  symetric positive by definition of the $\alj$.




\subsection{Definition of $\mathbf{DA}_h^\eps$}


We define now that is  call thereafter the "diffusion approximation" 
scheme.
We just neglect the time derivative
in the second equation, that we make
 $\partial_t\uj^{\eps}=0$ for (\ref{eq:4bis}). It
leads to the scheme
\begin{equation}\label{das}
\mathbf{DA}_h^\eps:
\quad
\left\{ 
\begin{array}{l}
\displaystyle \mid\Omega_{j}\mid\frac{d}{dt} p_{j}^{\eps}+\frac{1}{\varepsilon}
\sum_{r}(l_{jr}\ur^{\eps},\njr)=0\\
\frac{1}{\varepsilon} \sum_{r}l_{jr} (\njr,\uu_r^{\eps} -\uu_j^{\eps})\njr=0\\
\displaystyle \left( \sum_{j}\alj+ 
\sum_{j}\frac{\sigma}{\varepsilon}\bej\right)\ur^{
\eps}= \sum_{j}\ljr
p_j^{\eps}\njr+\sum_j\alj\uj^{\eps}
\end{array}
\right.
\end{equation}
This  scheme depends of
two parameters, the size of the mesh $h$ and the small parameter 
$\eps$.
We notice that
$\mathbf{DA}_h^\eps\neq \mathbf P_h^0$ for $\eps> 0$, and that 
$\lim_{\eps\rightarrow 0^+} \mathbf{DA}_h^\eps = \mathbf P_h^0$.
The point wise  initial data for \eqref{das} is $p_j^{\eps}(0)=p_0(\mathbf{x}_j)$. There is no 
need of initial data for $(u_j^{\eps}(0))$, which will be obtained 
 as a function of $(p_j^{\eps}(0))$ by solving a linear system.

\subsection{Mesh assumptions}


 \begin{figure}[h!]
 \begin{center}
 \scalebox{0.8}{\includegraphics{vr.pdf}}
 \end{center}
 \caption{Definition of the control volume $V_r$ around
 vertex $\mathbf{x}_r$.
 The control volume  around the vertex $\mathbf{x}_r$
 is defined by the closed loop
 that joins the center of the cells ($\mathbf{x}_j$'s) and the middle
 of the edges ($\mathbf{x}_{j+\frac12}$'s).}
 \label{figcv}
 \end{figure}


The characteristic length of the mesh is 
%\begin{equation*} \label{eq:d6}
$h=\max_j \left( \mbox{diam}(\Omega_j)  \right)$,
 %\end{equation*}
so that 
%We characterize the regularity of the mesh with a  constant $C_{\mathcal{M}}>0$ such that 
\begin{equation} \label{eq:d7}
\left\{
\begin{aligned}
&l_{jr}\leq h , \qquad \forall j,r , \\
&|\Omega_j| \leq  h^2 , \qquad \forall j .
\end{aligned}
\right. 
\end{equation}
The control volume $V_r$ 
around the vertex $\mathbf x_r$ is 
defined by the closed loop
$
\dots, \mathbf{x}_{j-\frac12}, \mathbf{x}_j,\mathbf{x}_{j+\frac12},\dots$.
Here  the $\mathbf{x}_j$'s are the center of the cells, and the 
$\mathbf{x}_{j+\frac12}$'s are the middle of the edges around the vertices
$\mathbf{x}_r$.
A typical example is depicted in  figure \ref{figcv}. 

Additional geometrical assumptions are always
necessary in dimension greater than one to guarantee
some minimal regularity of the mesh.
We make  the usual assumptions
 listed below from 1 to 3.
The last items are more specific.

%We characterize the regularity of the mesh with a  constant $C_{\mathcal{M}}>0$ such that 

\begin{hyp}\label{geometrie}
Our geometrical assumptions  will be the following
\begin{enumerate}

\item The numbers of cells which share  a node $r$ is
bounded independently of $h$, which means there exists 
$P\in \mathbb N$ independent of $h$ such that
\begin{equation}\label{nbmr} 
\sum_j \delta_{jr}\leq P.
\end{equation}
For example, for a structured mesh of quadrangular cells $P=4$.
 
 \item For  each cell of the mesh, the  number of edges is
bounded independently of $h$, or equivalently the numbers of vertices for a cell
 is bounded independently of $h$.



\item The mesh is regular in the sense that
there exists a universal constant $C_\mathcal M >0$ such that the inverse inequalities hold:
\begin{equation} \label{eq:d5.0}
C_\mathcal M h \leq l_{jr} , \quad \forall j,r \quad \qquad
\mbox{ uniformly with respect to  }h
\end{equation}
where $\mathbf x_r$ is a vertex of the cell $\Omega_j$, 
and 
\begin{equation} \label{eq:d5}
C_\mathcal M h^2 \leq \left| \Omega_j \right| , \quad \forall j\qquad
\mbox{ uniformly with respect to  }h.
\end{equation}
and  
\begin{equation} \label{eq:d5.2}
C_\mathcal M h^2 \leq \vert V_r \vert \leq P h^2, \quad \forall r\qquad
\mbox{ uniformly with respect to  }h.
\end{equation}
We recall that $V_r$ is the volume control (centered on $\x_r$) and $\Omega_j$
is the cell $j$. The inequality $\vert V_r \vert \leq P h^2$ is immediate to check on the figure \ref{figcv}.

\item 
A consequence of the items 1-3 
is that there exists 
 a constant
$\alpha >0 $ such that 
\begin{equation}\label{geom4}
(A_j\uu, \uu)\geq \alpha h (\uu,\uu), \quad A_j=\sum_r\alj.
\end{equation}
It can be proved with a geometrical  identity that we borrow
 from \cite{de10}  (proposition 8).
%
%
% We will use the notations
%$A_j=\sum_r\alj$, $A_r=\sum_j\alj$ and
%$B_r=\sum_j\bej$. 


\item  The matrix $B_r=\sum_j\bej$ is positive in the sense that 
%and there exists a
%constant $\alpha >0 $ independent of r such that 
\begin{equation}\label{geom3}
(B_r \uu, \uu)=(B^s_r \uu, \uu)\geq \alpha \vert
V_r\vert (\uu,\uu),
\end{equation}
where $B^s_r=\frac12 (B_r+B_r^t)$ is the symmetric part of $B_r$, and $\alpha$ is the same constant as in (\ref{geom4}).
Square meshes satisfy (\ref{geom3}).
This assumption is however not trivial to check in the general case.
We point out  \cite{glaceap} where
 sufficient conditions such that (\ref{geom3}) is satisfied
can be found; in particular it is shown 
that  triangular meshes with all angles greater than $12$ degrees
satisfy it. 
\end{enumerate}
\end{hyp}

\subsection{Norms and error measurements}

The quadratic norms below are usual integral norms.
It yields for   any cell centered quantity $f=(f_j)_{j\in {\rm Cells}}$: 
$\|f\|_{L^2(\Omega)}=\sqrt{\sum_j |\Omega_j| |f_j|^2   }$. For vertex based % the quadratic
%norm of any vertex based
 quantity $g=(g_r)_{r\in {\rm Vertices}}$, we use 
$\|g\|_{L^2(\Omega)}=\sqrt{\sum_r |V_r| |g_r|^2   }$: it is more a convention.
Useful  quantities  are
\begin{itemize}
\item $\mathbf V_h^\eps(t,\x)=\left(  \sum_{j\in {\rm Cells}} p^\eps_j(t) 1_{\Omega_j}(\x) , \sum_{j\in {\rm Cells}} \uu^\eps_j(t)  1_{\Omega_j}(\x) \right) $
which  is the solution of $\mathbf P_h^\eps$.
\item 
$\mathbf V^\eps(t,\x) =\left(p^\eps, \uu^\eps  \right)(t,\x)$ which is the solution of $\mathbf P^\eps$,
\item $\mathbf W_h^\eps(t,\x) =\left(  \sum_{j\in {\rm Cells}}  p^\eps_j(t) 1_{\Omega_j}(\x) ,  \sum_{j\in {\rm Cells}}  \uu^\eps_j(t)  1_{\Omega_j} (\x) \right) $ 
which is the solution of $\mathbf{DA}_h^\eps$.
Notice that an abuse of notations is  made with the solution
of $\mathbf P_h^\eps$.
\item $\mathbf W^\eps (t,\x) = \left( p ,-\frac\eps\sigma \nabla p  \right) (t,\x)$ which is the solution of $\mathbf P^0$.
\end{itemize}
As in  dimension  one, the  differences between these quantities are characterized at the initial time with a universal
constant $C_\mathcal A>0$ which indicates it can be  related to the approximation/interpolation/projection of a smooth function
on the mesh. We will use for example 
some  bounds that can be obtained as by-product  or corollary of the first technical inequality  below.
%The initialization stage yields 
%\textcolor{red}{$ \mathbf{V}_0$?: a continuer- je mets $ \mathbf{V}_0^\eps$}
\begin{equation}\label{constant_init}
\left\{
\begin{aligned}
&\| \mathbf{V}^\eps(0) - \mathbf{V}^{\eps}_h(0)\|_{L^2(\Omega)} \leq C_\mathcal A h \|  \mathbf V ^\eps(0) \|_  { H^2(\Omega)  } 
\leq  C_\mathcal A h \|  \mathbf V ^1(0) \|_  { H^2(\Omega)  } ,\\
&\| \mathbf{W}^\eps(0)- \mathbf{W}^{\eps}_h(0)\|_{L^2(\Omega)} \leq C_\mathcal A h \|  \mathbf W ^\eps(0) \|_  { H^2(\Omega)  } .
\end{aligned}
\right.
\end{equation}

We will need additional technical estimates for the corner based Finite Volume scheme $\mathbf P_h^\eps$. 
These technical estimates can be formulated as follows.
Let  $f$ be a regular function. We define
$\delta_{j,r}(f)= \frac{1}{|\Omega_j|}\int_{\Omega_j} f d\x
-f(\x_{r})$ which is the  interpolation error term that compares mean value in a cell $\Omega_j$  and point 
values at a vertex $\mathbf x_r$ of the same cell.
Let $\Gamma_{j,r}=[\x_{r},\x_{r+1}]$ be the edge
oriented toward the outside of the cell $j$, with
length $|\Gamma_{j,r} | $. We define also $\tilde{\delta}_{j,r}(h)=
\frac1{|\Gamma_{j,r}| } \int_{\Gamma_{j,r}} h d s
-\frac{h(\textbf{x}_{r})+h(\textbf{x}_{r+1})}{2}$ which is another
interpolation error contribution that compares the  mean value and the mid sum,
on the edge.
%We recall the usresults taken from \cite{Mazeran}, chapter 4.





\begin{pro} \label{pro:interp}
%For any function $f$ in $ H^3(\Omega)$,   %using  Sobolev embeddings,
One has the technical  inequalities
\begin{equation}\label{const_delta1}
 \vert \delta_{jr}(f) \vert
 \leq 
  C_{\mathcal A}  \| f \| _{ H^2(\Omega_j) }
\end{equation}
and
\begin{equation}\label{const_delta2}
\vert \widetilde \delta_{jr}(f)\vert
\leq  
  C_{\mathcal A} h  \| f \| _{ H^3(\Omega_j) }
\end{equation}
\end{pro}
\begin{proof}
These non optimal inequalities are consequences  of classical approximations results.
We will not prove them. However one can notice that the scaling is correct. %In the sense
That if a  function $f$ has its third derivatives bounded in $L^\infty(\Omega_j)$,
then $ \| f \| _{ H^2(\Omega_j) } =O(h)$ because the problem is 2D: this is compatible with
the fact that $\delta_{jr}$ is a first order difference.
Similarly $h \| f \| _{ H^3(\Omega_j) } =O(h^2)$ is compatible with the fact that 
$ \widetilde \delta_{jr}$ is a second order difference.
An alternative proof is by assuming that $f$ is in $H^p(\Omega)$ for a sufficiently large $p$.
Then by the Sobolev embeddings, all derivatives up to fourth order are in $L^\infty$ which is enough
to prove that (\ref{const_delta1}) is a first order interpolation error term, and that
(\ref{const_delta2}) is a second order interpolation error term.
In this case it also explains very simply
why the constant $C_\mathcal A$ is independent of the mesh size. 
%Use Sobolev embeddings to get $L^\infty$ bounds on the  
\end{proof}

The first technical inequality is actually true  for any points in the cell.
So it allows to compare the mean value and the point value in the cell. This is why it yields
(\ref{constant_init}) after summation over all cells.



As in dimension one, we will use constants $~_\downarrow C$, $C^\rightarrow$, $C_\downarrow$ and
$C_\leftarrow$ in the errors bounds for the four branches of the new AP diagram.
The important point is that these constants are independent of $h$ and $\eps$.
They have of course some dependence with respect to other parameters such as 
the constant of the mesh $C_\mathcal M$ or example,
but we will not keep track of these dependence in order to simplify the notations.
Nevertheless the interest reader can compare with the same estimates in dimension where
the dependance with respect to the mesh constant and to some typical Gronwall exponential coefficients
is indicated.
A first result is the  inequality   (\ref{ine3}) which  yields the basic estimate for the lower branch of the AP diagram.
It can be formalized as follows.

%can be  rewritten as
\begin{lemma}
One has the estimate
\begin{equation} \label{eq:enc22}
\|\W^\eps-\mathbf V^\eps\|
_{ L^2([0,T]\times\Omega   ) } %\leq C \eps \|  \mathbf V ^\eps(0) \|_  { H^3(\Omega)  } 
\leq C_\leftarrow  \eps \|  p_0 \|_  { H^4(\Omega)  } 
\end{equation}
where the constant $C_\leftarrow $ is independent of $h$ and $\eps$.
\end{lemma}
%It remains to study the other branches of the generalized AP diagram of the figure \ref{fig:diag4}.





%It is implicit in the rest of this work that $C_\mathcal A$ and $C_0$ may  be redefined if needed by taking the maximum of the current value with a 
%new 
%one coming from another constraint of the same type.
%\textcolor{red}{ca m'embte qu'il n'y ait pas de $h$ ici : a quoi ca sert si on compare  (22) ?}.


\subsection{Study of $\| \mathbf 
P_h^\varepsilon -\mathbf P^\varepsilon \|_{\rm naive}$}


In this part, we exploit the hyperbolic nature
of both 
$\mathbf P^\varepsilon$ and $\mathbf P_h^\varepsilon $ to obtain the main bound.
As one will see below, the convergence estimate
(\ref{eq:veps2}) is not trivial. It indicates that, 
for a problem with $O(\varepsilon^{-2})$ terms, a scheme 
converges, with $h$, with  at rate $O(\varepsilon^{-\frac12})$ with respect to
$\eps$.

\begin{lemma}[Naive estimate]\label{ein_prosit}
There exists a constant $~_\downarrow C$ independent of
$h$ and $\eps$
such that the following estimate holds
\begin{equation} \label{eq:veps2}
\|\V^{\eps}_h-\V^{\eps}\|_{L^\infty( [0,T]: L^2  (\Omega) )}\leq
{~_\downarrow C}%{\sqrt{ C_\mathcal M } }
\sqrt{\frac h \eps}\| p_0\|_{H^4(\Omega)}.
\end{equation}
\end{lemma}


\subsubsection{Stability}

We first prove the $L^{2}$ stability of  the scheme $P_h^\varepsilon $ defined 
in (\ref{eq:4},\ref{eq:5}). % and then obtain the main bound.
\begin{pro}[Stability] \label{propesti} %Under the geometrical assumption (\ref{geom3}), t
The
semi-discrete general JL-(b) scheme defined by (\ref{eq:4},\ref{eq:5})  is
stable in the $L^2$ norm in the sense that $\frac{d}{dt}
 \vert\vert \V^{\eps}_{h}(t)
\vert\vert \leq 0$.
 Moreover we have the bounds
\begin{equation}\label{gggg}
\frac{\sigma}{\eps^2}\vert\vert \uu_r^{\eps} \vert\vert_{L^2([0,T]\times \Omega)}\leq
\frac{1}{\alpha}
\vert\vert
\V^{\eps}_h(0)\vert\vert_{L^2( \Omega)},
\end{equation}
\begin{equation}\label{gs}
\int_0^T\sum_j \sum_r
l_{jr}(\njr,(\uj^\eps-\ur^\eps))^{2} dt\leq \eps \vert\vert
\mathbf V^{\eps}_h(0)\vert\vert^2_{L^2( \Omega)} .
\end{equation}
\end{pro}

\begin{proof}
We define the functions $p^{\eps}_h$ and $\uu^{\eps}_h$  by
$p^{\eps}_h=p_j$ and $\uu^{\eps}_h=\uj$  on $\Omega_j$.
We set for convenience $E(t)= \vert\vert \V^{\eps}_{h}(t)
\vert\vert^2$.
 One has
$$
E^{'}(t)=\displaystyle \frac12  \int_{\Omega} \frac{d}{dt}( \mid
p^{\eps}_{h} \mid^{2}+
(\uu^{\eps}_{h},\uu^{\eps}_{h}))=\int_{\Omega}
p^{\eps}_{h}\frac{d}{dt}p^{\eps}_{h}+(\uu^{\eps}_{h},\frac{d}{dt
}\uu^{\eps}_{h})
=\displaystyle \sum_{j} \left| \Omega_j \right|
p_{j}^{\eps}\frac{d}{dt} p_{j}^{\eps}+(\uj^{\eps},\frac{d}{dt}\uj^{\eps}).
$$
Using the definition of scheme 
\begin{equation}\label{eq:ds1}
\displaystyle E^{'}(t)=-\frac{1}{\varepsilon}
\sum_{j}\sum_{r}l_{jr}p_{j}^{\eps}(\ur^{\eps},\njr)
-\frac{1}{\varepsilon
}\sum_{j}\sum_{r}(l_{jr}p_{j,r}^{\eps} \njr,\uj^{\eps})
-\frac{\sigma}{\varepsilon^{2}}
\sum_{j}\sum_r (\widehat{\beta}_{jr}\ur^{\eps},\uj^{\eps}).
\end{equation}
Using (\ref{eq:5}) we expand the second term of the previous equation 
\begin{equation}\label{eq:ds2}
\displaystyle
\sum_{j}\sum_{r}(
l_{jr}p_{j,r}^{\eps}\njr
,\uj^{\eps})=\sum_{j}\sum_{r}l_{jr}p_{j}^{\eps}
(\uj^{\eps},\njr)+\sum_{j}\sum_{r}(\widehat {\alpha}_{jr}(\uj^{\eps}
-\ur^{\eps}),\uj^{\eps})-\frac{\sigma}{\varepsilon}\sum_{j}\sum_{r}({
\widehat\beta}_{jr}\ur^{\eps},\uj^{\eps}).
\end{equation}
Since $\sum_{r}l_{jr}\njr=0$ the first term of (\ref{eq:ds2}) is
zero.
Summing on r the second equation of (\ref{eq:5}) and permuting the sums, we show
that $0=
 \displaystyle \sum_{j}\sum_{r}l_{jr}p_{jr}(\ur,\njr)$ which yields that
\begin{equation}\label{eq:ds3}
0=
 \displaystyle \sum_{j}\sum_{r}l_{jr}p_{j}^{\eps}(\ur^{\eps},\njr)-
\sum_{j}\sum_{r}((\widehat
{\alpha}_{jr}+\frac{\sigma}{\varepsilon}\widehat
{\beta}_{jr})\ur^{\eps},\ur^{\eps})
+\sum_{j}\sum_{r}(\widehat{\alpha}_{jr}\uj^{\eps},\uu_{r
}^{\eps}).
\end{equation}
Plugging (\ref{eq:ds2}) and (\ref{eq:ds3}) in (\ref{eq:ds1}) and permuting the
sums in $E'(t)$ gives
\begin{equation*}\label{rresti}
E^{'}(t)=\displaystyle -\frac{1}{\varepsilon}
\sum_{j}\sum_{r}( \widehat{\alpha}_{jr}(\uj^{\eps}-\ur^{\eps}),\uj^{\eps}
-\ur^{\eps})- \frac{\sigma}{\varepsilon^{2}}
\sum_{r}\sum_{j}(\widehat {\beta}_{jr}\ur^{\eps},\ur^{\eps})
\end{equation*}
which gives
\begin{equation}\label{eq:ds4}
\displaystyle
E^{'}(t)+\frac{1}{\varepsilon}\sum_{r}\sum_{j}l_{jr}(\njr,(\uu^{\eps}
_{j}-\ur^{\eps}))^{2}+ \frac{\sigma}{\varepsilon^{2}}
\sum_{r}(B_r\ur^{\eps},\ur^{\eps})= 0.
\end{equation}
By geometrical assumption  (\ref{geom3}) we have $E^{'}(t)\leq 0$, that is the
$L^2$ stability, and
by integrating this equality on $[0,T]$ we obtain
\begin{equation*}\label{eq:ds5}
\displaystyle
E(T)+\int_0^T\frac{1}{\varepsilon}\sum_{r}\sum_{j}l_{jr}(\njr,
(\uj^{\eps}-\ur^{\eps}))^{2}+ \int_0^T\frac{\sigma}{\varepsilon^{2}}
\sum_{r}(B_r\ur^{\eps},\ur^{\eps})=E(0)
\end{equation*}
Using again the geometrical assumption  (\ref{geom3}) for the terms
$(B_r\ur^{\eps},\ur^{\eps})$ we have
\begin{equation*}\label{eq:ds6}
\displaystyle
E(T)+\int_{0}^T\frac{1}{\varepsilon}\sum_{r}\sum_{j}l_{jr}(\njr,
(\uj^{\eps}-\ur^{\eps}))^{2}+ \alpha
\int_{0}^T\frac{\sigma}{\varepsilon^{2}} \sum_{r}\vert V_r\vert
\ \vert\ur^{\eps}\vert^2\leq E(0)
\end{equation*}
which gives (\ref{gggg}) and (\ref{gs}).
\begin{comment}
The last inequality can be obtained as follows.
One has
$$
\begin{array}{ll}
\vert\vert \uu_j^{\eps} \vert\vert_{L^2( \Omega)}&
\leq 
C \sum_j h^2 \vert\mathbf u_j ^\eps \vert^2
\\
& \leq 
C h  \sum_j \sum_r l_{jr} 
( \mathbf n_{jr}, \mathbf{u}_j^\eps    )^2
\\
& \leq 
C h  \sum_j \sum_r l_{jr} 
2 ( \mathbf n_{jr}, \mathbf u_r ^\eps    )^2
+
C h  \sum_j \sum_r l_{jr} 
2 ( \mathbf n_{jr}, \mathbf u_j ^\eps  - \mathbf u_r ^\eps    )^2 \\
&
\leq C
\vert\vert \uu_r^{\eps} \vert\vert_{L^2(\Omega)} 
+
C h  \sum_j \sum_r l_{jr} 
2 ( \mathbf n_{jr}, \mathbf u_j ^\eps  - \mathbf u_r ^\eps    )^2 .
\end{array}
$$
Using inequalities  (\ref{gggg}) and (\ref{gs}), one gets
$$
\vert\vert \uu_j^{\eps} \vert\vert_{L^2( [0,T]\times \Omega)}^2
\leq C 
\vert\vert \uu_r^{\eps} \vert\vert_{L^2( [0,T]\times \Omega)}^2
+C\int_0^T 
h  \sum_j \sum_r l_{jr} 
 ( \mathbf n_{jr}, \mathbf u_j ^\eps  - \mathbf u_r ^\eps    )^2 
\leq 
C ( \eps^2  + h \eps) \vert\vert
\mathbf V^{\eps}_h(0)\vert\vert^2_{L^2( \Omega)}.
$$
\end{comment}
The proof is ended.
\end{proof}


\subsubsection{Main estimate}

Our goal now is to prove the lemma \ref{ein_prosit}  %result 
as  the consequence
of propositions \ref{pro:conv1} to \ref{pro:conv2}.
This part is the more technical one of the paper, but is essential
to be able to use the general strategy of proposition
\ref{prop:1} with convenient exponents. 
Like in 1D, we use the method introduced by Mazeran \cite{Mazeran} and decompose
the proof in several steps.
We introduce
$\mathscr{E}(t)=\frac12 \|\V^{\eps}-\V^{\eps}_h\|_{L^2(\Omega)}^2$.
As for the 1D proof and for the sake of simplicity, for any quantity $q$,   $q'$ stands indifferently for  $\frac{d}{dt}q $ or $\partial_t q $.

\begin{pro} \label{pro:conv1}
One has the formula
\begin{equation} \label{eq:ee1}
%\begin{aligned}
\mathscr{E}'(t) =
-\frac{1}{\eps}\sum_{j,r}l_{j,r}(\mathbf{n}_{j,r},\mathbf{u}_j^{\eps}-\mathbf{u}
_r^{\eps})^2 +E_1+E_2+E_3
\end{equation}
where
$$
\begin{aligned}
E_1=& \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}(\mathbf{u}_r^{\eps}-\mathbf{u}_j^{\eps}),\mathbf{n}_{j,r} \bigg)
\delta_{j,r}(p^{\eps}) +\frac{1}{\eps} \sum_j \sum_r \bigg( 
 l_{j,r}\mathbf{n}_{j,r} ( p_{jr}^{\eps}  - p_j^{\eps}) \: , \:
\delta_{j,r}(\uu^{\eps}) \bigg), \\
E_2=&\frac{1}{\eps}\sum_j \sum_r  |\Gamma_{j,r} | p_j^{\eps} (\mathbf{n}_{j,r} ,
\tilde{\delta}_{j,r}(\uu^{\eps}) ) + \frac{1}{\eps}\sum_j \sum_r |\Gamma_{j,r} | \bigg(
\mathbf{u}_j^{\eps} , \mathbf{n}_{j,r} \tilde{\delta}_{j,r}(p^{\eps}) \bigg) \\
E_3=&\frac{\sigma}{\eps^2} \sum_r \sum_j 
\bigg(\widehat{\beta}_{j,r}\mathbf{u}_r^{\eps}  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg)
+\frac{\sigma}{\eps^2}\sum_j\bigg(  \mathbf{u}_j^{\eps} ,
\int_{\Omega_j}\uu^{\eps}d\x \bigg) \\
&
-\frac{\sigma}{\eps^2}\sum_j\int_{\Omega_j}(\uu^{\eps},\uu^{\eps}
)d\x -\frac{\sigma}{\eps^2}\sum_{r}
(B_{r}\mathbf{u}_r^{\eps},\textbf{u}_r^{\eps}).
\end{aligned}
$$
\end{pro}



\begin{proof}
We first consider  the time  derivative
\begin{equation*} \label{eq:eetoile}
\begin{array}{l}
\mathscr{E}'(t)
=\underbrace{\int_{\Omega}(p_h^{\eps}(p_h^{\eps})'+(\uu_h^{\eps},(\textbf{u}
_h^{\eps})'))d\x}_{D_1}+ \underbrace{
\int_{\Omega}(p^{\eps}(p^{\eps})'+(\textbf{u}^{\eps},(\textbf{u}^{\eps})'))d\x}_{
D_2} \\
\hspace{0.8cm}+ \underbrace{\int_{\Omega}
(-(p_h^{\eps})'p^{\eps}-((\textbf{u}_h^{\eps})',\textbf{u}^{\eps}))d\x}_{D_3} 
+ \underbrace{\int_{\Omega}
(-p_h^{\eps}(p^{\eps})'-(\textbf{u}_h^{\eps},(\textbf{u}^{\eps})'))d\x}_{D_4}.
\end{array}
\end{equation*}
One has thanks to  (\ref{eq:ds4})
$$
D_1
=-\frac{1}{\eps}\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2-\frac{\sigma}{\eps^2}\sum_{r}
( B_r\textbf{u}_r^{\eps},\textbf{u}_r^{\eps}).
$$
One also directly has
\begin{equation*}
D_2 =
-\frac{\sigma}{\eps^2}\int_{\Omega}(\uu^{\eps},\uu^{\eps})d\x =
-\frac{\sigma}{\eps^2}\sum_j\int_{\Omega_j}(\uu^{\eps},\uu^{\eps}
)d\x.
\end{equation*}
Then,  using the  definition (\ref{eq:4},\ref{eq:5}) of the scheme we have
\begin{equation*}
\begin{array}{l}
D_3 = \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}\textbf{u}_r^{\eps},\textbf{n}_{j,r} \bigg) 
\frac{1}{|\Omega_j|}\int_{\Omega_j} p^{\eps} dx \\
\hspace{1cm}+\frac{1}{\eps} \sum_j \bigg( \sum_r
l_{jr} \mathbf n_{jr} p_{j,r}^{\eps}+\frac{\sigma}{\eps} \sum_r
\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg) 
\end{array}
\end{equation*}
Since $\sum_r l_{jr} \mathbf n_{jr}=0$,  we can write
\begin{equation*}
\begin{array}{ll}
D_3= &  \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}(\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps}),\textbf{n}_{j,r} \bigg) 
\frac{1}{|\Omega_j|}\int_{\Omega_j} p^{\eps} dx \\
& + \frac{1}{\eps} \sum_j \bigg( \sum_r 
l_{jr} \mathbf n_{jr} (p_{j,r}^{\eps}
-
p_j^{\eps})  \: , \: \frac{1}{|\Omega_j|}\int_{\Omega_j}
\uu^{\eps} d\x \bigg) \\
& +\frac{\sigma}{\eps^2} \bigg( \sum_r \sum_j\widehat{\beta}_{j,r}\textbf{u}_r^{\eps} 
\: , \: \frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg) .
\end{array}
\end{equation*}
One gets
\begin{equation*}
\begin{aligned}
D_3 &=\frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}(\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps}),\textbf{n}_{j,r} \bigg)
\delta_{j,r}(p^{\eps})+\frac{1}{\eps} \sum_j \sum_r \bigg( 
l_{jr} \mathbf n_{jr} (p_{j,r}^{\eps}
-
p_j^{\eps}) \: ,
\:\delta_{j,r}(\uu^{\eps}) \bigg) \\
&+ \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}(\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps}),\textbf{n}_{j,r} \bigg)
p^{\eps}(\x_r)+ \frac{1}{\eps} \sum_j \sum_r \bigg(  
l_{jr} \mathbf n_{jr} (p_{j,r}^{\eps}
-
p_j^{\eps})  \: , \:  \uu^{\eps}(\textbf{x}_{r}) \bigg) \\
&+\frac{\sigma}{\eps^2} \bigg( \sum_r \sum_j
\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg) .
\end{aligned}
\end{equation*}
We have the identities $\sum_{j,r}\ljr \njr 
=0$
and  $\sum_j  l_{jr} \mathbf n_{jr} p_{j,r}^{\eps}=0$ by definition
(\ref{eq:5}).
Therefore one can simplify the third and fourth term
in the previous expression and get 
\begin{equation*}
\begin{aligned}
D_3  & =  \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}(\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps}),\textbf{n}_{j,r} \bigg)
\delta_{j,r}(p^{\eps})+\frac{1}{\eps} \sum_j \sum_r \bigg( 
l_{jr} \mathbf n_{jr} (p_{j,r}^{\eps}
-
p_j^{\eps}) \: , \:
\delta_{j,r}(\uu^{\eps}) \bigg) \\
& - \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}\textbf{u}_j^{\eps},\textbf{n}_{j,r} \bigg) p^{\eps}(\x_r)- \frac{1}{\eps}
\sum_j \sum_r \bigg( l_{j,r}p_j^{\eps}\textbf{n}_{j,r} \: ,\:
\uu^{\eps}(\textbf{x}_{r}) \bigg)\\
 & + \frac{\sigma}{\eps^2} \bigg( \sum_r
\sum_j\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg) .
\end{aligned}
\end{equation*}
We now look at $D_4$. By definition, one has
\begin{equation*}
D_4= \frac{1}{\eps}\sum_j p_j^{\eps} \sum_r \int_{\Gamma_{j,r}}
(\uu^{\eps},\tnjr)d\sigma + \frac{1}{\eps}\sum_j
\bigg( \textbf{u}_j^{\eps} , \bigg( \sum_r \int_{\Gamma_{j,r}} p^{\eps}
\tnjr d\sigma
+\frac{\sigma}{\eps}\int_{\Omega_j}\uu^{\eps} d\x\bigg) \bigg)
\end{equation*}
where $\tnjr$ is the normal to the edge
$\Gamma_{j,r}=[\x_{r},\x_{r+1}]$ oriented toward the outside of the cell $j$. 
This expression needs an important manipulation which
is to approximate the integral on edges by corner values.
This necessary manipulation is one of the ideas that was introduced
in \cite{Mazeran} in order to proceed to the numerical
analysis of such corner based finite volume schemes.
This is why  interpolation terms  $\tilde{\delta}_{j,r}(h)= 
\frac1{|\Gamma_{jr}|}\int_{\Gamma_{j,r}} h
- \frac{h(\textbf{x}_{r})+h(\textbf{x}_{r+1})}{2}$ are introduced.
One gets after an algebraic manipulation 
\begin{equation*}
\begin{aligned}
D_4 &= \frac{1}{\eps}\sum_j \sum_r |\Gamma_{j,r}|   p_j^{\eps} \bigg( \tnjr
,\tilde{\delta}_{j,r}(\uu^{\eps}) \bigg) +\frac{1}{\eps}\sum_j \sum_r
|\Gamma_{j,r}| 
\bigg( \textbf{u}_j^{\eps} ,
\tnjr\tilde{\delta}_{j,r}(p^{\eps})\bigg) +
\frac{\sigma}{\eps^2}\sum_j\bigg(  \textbf{u}_j^{\eps} ,
\int_{\Omega_j}\uu^{\eps} \bigg)\\
&+ \frac{1}{\eps}\sum_j \sum_r  |\Gamma_{j,r}| p_j^{\eps} \bigg(
\tnjr ,\frac{\uu^{\eps}(\textbf{x}_{r})+\uu^{\eps}(\textbf{x}_{r+1})}{2}
\bigg) + \frac{1}{\eps}\sum_j \sum_r |\Gamma_{j,r}| \bigg( \textbf{u}_j^{\eps} ,
\tnjr
\frac{p^{\eps}(\textbf{x}_{r})+p^{\eps}(\textbf{x}_{r+1})}{2}\bigg) 
\end{aligned}
\end{equation*}
By definition (\ref{eq:not}),
$\njr\ljr=\frac{\tilde{n}_{j,r}|\Gamma_{j,r}|+\tilde{n}_{j,r-1}|\Gamma_{j,r-1}|}
{2}$, so one can see that 
$$
\sum_j \sum_r |\Gamma_{j,r}|  p_j^{\eps} \bigg(
\tnjr ,\frac{\uu^{\eps}(\textbf{x}_{r})+\uu^{\eps}(\textbf{x}_{r+1})}{2}
\bigg)= \sum_j \sum_r  l_{jr} p_j^{\eps} \big( \njr ,\uu^{\eps}(\textbf{x}_{r}) \big).
$$ 
It yields a slightly simpler expression
$$
\begin{aligned}
D_4 &= \frac{1}{\eps}\sum_j \sum_r |\Gamma_{j,r}|   p_j^{\eps} \bigg( \tnjr
,\tilde{\delta}_{j,r}(\uu^{\eps}) \bigg) +\frac{1}{\eps}\sum_j \sum_r
|\Gamma_{j,r}| 
\bigg( \textbf{u}_j^{\eps} ,
\tnjr\tilde{\delta}_{j,r}(p^{\eps})\bigg) +
\frac{\sigma}{\eps^2}\sum_j\bigg(  \textbf{u}_j^{\eps} ,
\int_{\Omega_j}\uu^{\eps} \bigg)\\
&+ \frac{1}{\eps}
\sum_j \sum_r  l_{jr} p_j^{\eps} \big( \njr ,\uu^{\eps}(\textbf{x}_{r}) \big)
 + \frac{1}{\eps}\sum_j \sum_r \ljr p^{\eps}(\textbf{x}_{r})\big(\njr ,\textbf{u}_j^{\eps}\big) 
\end{aligned}
$$
One can now compute the sum  $D_3+D_4$
\begin{equation*}
\begin{aligned}
D_3+D_4 &= \frac{1}{\eps} \sum_j \sum_r \bigg(
l_{j,r}(\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps}),\textbf{n}_{j,r} \bigg)
\delta_{j,r}(p^{\eps}) +\frac{1}{\eps} \sum_j \sum_r \bigg( 
 l_{j,r}\textbf{n}_{j,r}  (p_{j,r}^{\eps}- p_j^{\eps})  \: , \:
\delta_{j,r}(\uu^{\eps}) \bigg) \\
&+\frac{1}{\eps}\sum_j \sum_r  |\Gamma_{jr}|
p_j^{\eps}
\bigg(\textbf{n}_{j,r},\tilde{\delta}_{j,r}(\uu^{\eps}) \bigg) +
\frac{1}{\eps}\sum_j \sum_r  |\Gamma_{jr}| \bigg( \textbf{u}_j^{\eps} , \textbf{n}_{j,r}
\tilde{\delta}_{j,r}(p^{\eps})\bigg)\\
&+\frac{\sigma}{\eps^2} \bigg( \sum_r \sum_j \widehat{\beta}_{j,r}\textbf{u}_r^{\eps}
 \: , \: \frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg)
+\frac{\sigma}{\eps^2}\sum_j \bigg(  \textbf{u}_j^{\eps} ,
\int_{\Omega_j}\uu^{\eps} d\x\bigg).
\end{aligned}
\end{equation*}
One finally gets after rearrangement
the final result (\ref{eq:ee1}) for $\mathscr{E}'(t)
=D_1+D_2+D_3 +D_4$.
\end{proof}

The proof of the dissipative identity relies on a careful and technical evaluation of $E_1$, $E_2$ and $E_3$.
Combined with the Gronwall method and comparison with the first term in 
\ref{eq:ee1}) which 
damps, it is sufficient to obtained the desired result.
We refer the reader to the appendix for all details. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Study of $\| \mathbf D \mathbf A_h^\varepsilon
 - \mathbf P^0\|$}


This main result in this section is the following.

\begin{lemma} \label{lemme:tiptop}
One has the estimate
\begin{equation} \label{eq:enc1}
\| \mathbf W_h^\eps  -\W^\eps \|_{L^2([0,T]\times  \Omega)   )   }\leq  C_\downarrow (h+\eps)  \|  p_0  \|_{H^4(\Omega)}   . %: A Verifier.
\end{equation}
\end{lemma}
%\begin{proof}
This result, which is merely a consequence of  (\ref{eq:dconv1}) and (\ref{eq:dconv2})
in proposition  \ref{theor:main0},
will be obtained after studying in details the well-posedness,
stability and consistency of the new diffusion asymptotic scheme rewritten after a convenient rescaling.
The proof is provided just after the proof of the proposition.
Additional technical results will be derived at the end of the section.

\subsubsection{Rescaling of the equations}

We rescale  the semi-discrete diffuse asymptotic scheme
$ \mathbf D \mathbf A_h^\varepsilon$  (\ref{das}) wherein for  convenience we
made the following change of unknowns
\begin{equation} \label{eq:barn}
\overline \uu_r^{\eps}=\frac{\uu_r^{\eps}}{\eps}
\mbox{ and }
\overline \uu_j^{\eps}=\frac{\uu_j^{\eps}}{\eps}.
\end{equation} 
In order to keep  a simple
notation we dropped the superscript $\eps$ and the bars. Thus the scheme
(\ref{das}) is now written as:


\begin{equation}\label{das:1}
\left\{ 
\begin{array}{l}
\displaystyle \mid\Omega_{j}\mid\frac{d}{dt} p_{j}+
\sum_{r}(l_{jr}\ur,\njr)=0\\
 \sum_{r}l_{jr} (\njr,\uu_r -\uu_j)\njr=0\\
 \left( \eps\sum_{j}\alj+  \sigma B_r\right)\ur=
\sum_{j}\ljr p_j\njr+\eps\sum_j\alj\uj
\end{array}
\right.
\end{equation}
\begin{remark}
If wet set $\eps=0$ we naturally recover the limit diffusion scheme (\ref{diffglace}).
\end{remark}
This way of writing the system is much better to help the intuition, since it is can be naturally  interpreted
as a the discretization of a diffusion equation.

%\end{proof}
%The result 




\subsubsection{Well-posedness}

What we mean about well-posedness is the following:
if we are able to write the
last two relations of (\ref{das:1}) as a non singular linear system with the $\uu_r$'s and
$\uu_j$'s as unknowns, then we have a unique solution in terms of the $p_j$'s. 
This notion is the relevant one for numerical discretization.

Let us denote $Y=(\{\uu_j\}, \{\uu_r\})$  the vector of unknowns. We can write
 the last
two relations of (\ref{das:1}) as
$MY=b$ where $M$ is a $(J+R)^2$ square matrix, $J$ is the number of cells  and $R$.
One can observe that unless $\eps=0$, $M$ is not a blockwise
triangular matrix.
One has
$$(MY,Y)=\sum_r \left (\sigma B_r \uu_r, \uu_r\right ) 
+\eps\sum_j\sum_r \ljr \left( \uu_r- \uu_j,  \njr\right )^2$$
Assume  $(MY,Y)=0$: in this case   the geometrical assumption  (\ref{geom3}) implies that all
the $\uu_r$ are null and therefore it remains to study 
$
\sum_j\sum_r \ljr \left(  \uu_j,  \njr\right )^2=0
$ that is
$ \sum_j\left (\uu_j, C_j\uu_j\right )=0$ where $
C_j=\sum_r l_{jr} \mathbf n_{jr} \otimes \mathbf n_{jr}$.
 Since  the $C_j$ are all invertible
unless the mesh is degenerate, all the $\uu_j$ are null: we have proved the
invertibility of the matrix $M$ and thus the scheme (\ref{das:1}) exists and is
uniquely defined.



\subsubsection{Stability}
%The $L^2$ norm of the solution is 
We note $
E(t)=
\displaystyle \frac12  \sum_j \vert \Omega_j\vert p_j^2$.
The initial data is    $p_h(0)=\left( p_j(0)\right)_{j\in Cells}$.

\begin{pro}\label{das_stab} Under the geometrical assumption (\ref{geom3}), the
diffusion approximation scheme (\ref{das:1})  is stable in the $L^2$
norm, in the sense that $E'(t)\leq 0$.  One has 
\begin{equation}\label{das:gggg}
\vert\vert \uu_r \vert\vert_{L^2([0,T]\times \Omega)}\leq
\frac{1}{\alpha}  \left\|  p_h(0) \right\|_{L^2(\Omega)}
\end{equation}
and
\begin{equation}\label{das:gs}
\eps \int_{[0,T]}\sum_j \sum_r
l_{jr}(\njr,(\uj-\ur))^{2} \leq  
 \left\|   p_h(0) \right\|_{L^2(\Omega)}.
\end{equation}
\end{pro}

\begin{proof}
One has
$$
E^{'}(t)= \sum_j \vert \Omega_j\vert p_j \frac{d}{dt}p_j
= -\sum_j  p_j \sum_{r}(l_{jr}\ur,\njr) 
 =\sum_r \left (\uu_r, \sum_{j}l_{jr}\njr p_j\right ) .
$$
 With  last equation of (\ref{das:1}),  one finds
%\begin{equation*}\label{das:4}
$
E^{'}(t)= -\sum_r \left (\uu_r, \left( \eps\sum_{j}\alj+ 
\sigma B_r\right)\ur-
\eps\sum_j\alj\uj\right )
$. %\end{equation*}
We expand the right hand side  %of the previous equation 
%\begin{equation*}\label{das:5}
$E^{'}(t)= -\sum_r \left (\sigma B_r \uu_r, \uu_r\right ) 
-\eps \sum_r\left( \uu_r, \sum_j \alj(\uu_r -\uu_j\right )$.
%\end{equation*}
Permuting the sums in the second term of the right hand side , we show that
\begin{equation}\label{das:6}
E^{'}(t)= -\sum_r \left (\sigma B_r \uu_r, \uu_r\right ) 
-\eps\sum_j\sum_r \left( \uu_r,  \alj(\uu_r -\uu_j)\right ).
\end{equation}
Using the definition of the $\uu_j$, second line of (\ref{das:1}), one has
\begin{equation}\label{das:7}
\sum_j 
\left( \uu_j,  \sum_r \alj(\uu_r -\uu_j)\right)=0. 
\end{equation}
Combining (\ref{das:7})$\times \eps$  with (\ref{das:6}) and using the definition of
the matrices $\alj$ one has finally
\begin{equation*}\label{das:8}
E^{'}(t)= -\sum_r \left (\sigma B_r \uu_r, \uu_r\right ) 
-\eps\sum_j\sum_r \ljr \left( \uu_r- \uu_j,  \njr\right )^2.
\end{equation*}
By the geometrical assumption  (\ref{geom3}) we have $E^{'}(t)\leq 0$, that is
the $L^2$ stability.
By integrating this equality on $[0,T]$ we obtain
\begin{equation*}\label{eq:ds5bis}
\displaystyle
E(T) + \int_{[0,T]}\sum_r \left (\sigma B_r \uu_r, \uu_r\right ) 
 +\int_{[0,T]}\eps\sum_j\sum_r \ljr \left( \uu_r- \uu_j,  \njr\right )^2=E(0)
\end{equation*}
Using again the geometrical assumption  (\ref{geom3}) for the terms
$(B_r\ur,\ur)$ we have
\begin{equation*}\label{eq:ds6bis}
E(T) + \alpha \int_{[0,T]}\sum_r  \vert V_r\vert \  \| \uu_r \|^2 
+ \int_{[0,T]}\eps\sum_j\sum_r \ljr \left( \uu_r- \uu_j,  \njr\right )^2\leq
E(0)
\end{equation*}
which gives (\ref{das:gggg}) and (\ref{das:gs}).
\end{proof}


\subsubsection{Consistency}

For convenience   we set
\begin{equation} \label{eq:con1}
\pej=p(\x_j,t)\ \ \uej=-\frac{1}{\sigma}\nabla p(\x_j,t)\ \
\uer=-\frac{1}{\sigma}\nabla p(\x_r,t)
\end{equation}
where $p(x,t)$ is the solution of the diffusion equation. 
 We define three consistency errors 
by inserting these quantities into the three equations of
(\ref{das:1}) which are also rescaled by a factor $\frac1{\mid\Omega_{j}\mid}$, $\frac 1 h $ and $\frac{1}{\vert V_r\vert}$.
It yields 
\begin{equation*} \label{eq:d2.2}
\left\{ 
\begin{aligned}
&a_j=\displaystyle \frac{d}{dt} \pej+
\frac{1}{\mid\Omega_{j}\mid}\sum_{r}(l_{jr}\uer,\njr), \\
&\bj= \frac{1}{h}\sum_{r}l_{jr} (\njr,\uer -\uej)\njr , \\
& \ccr=\frac{1}{\vert V_r\vert}\left (   \sigma B_r \uer- \sum_{j}\ljr
\pej\njr+ \eps\sum_j\alj(\uer-\uej)\right ).
\end{aligned}
\right.
\end{equation*}





\begin{proposition}\label{consistency}
There exists a constant $C_c>0$ independent  on $h$ and $\varepsilon$  such that the following 
estimates hold
\begin{equation} \label{eq:d20}
\left\|a \right\|_{L^\infty([0,T]:L^2(\Omega))}
\leq C_c h  \| p_0 \|_{H^4(\Omega)},
\end{equation}
\begin{equation} \label{eq:d22}
\left\|\mathbf b \right\|_{L^\infty([0,T]:L^2(\Omega))}
 \leq C_c h \| p_0 \|_{H^3(\Omega)} ,
\end{equation}
and
\begin{equation} \label{eq:d21}
\left\|\mathbf c \right\|_{L^\infty([0,T]:L^2(\Omega))}
 \leq C_c(h+\eps) \| p_0 \|_{H^3(\Omega)}.
\end{equation}
\end{proposition}
%In this result the constant $C_c$  is not explicitly given since it only depends on the regularity of $p_0$.
\begin{proof} 
%Since $p_0\in H^{4}(\Omega)$, one has that $p(t)\in H^{4}(\Omega)$ 
%which turns into the fact that $\nabla p(t)\in L^\infty(\Omega)$ and $\nabla ^2p(t)\in L^\infty(\Omega)$ by means of 
%Sobolev embeddings. It is sufficient to justify 
%the Taylor expansions done hereafter.
The proof  uses  the inequalities of proposition \ref{pro:interp}.  
For example one has 
$$
a_j=\frac1\sigma  \left(\underbrace{
 \Delta p(\mathbf x_j, t)- \frac{\int_{\Omega_j} \Delta  p (\mathbf x, t) dx}{|\Omega_j |}}_{=d_j^1}
 \right)
 +\frac1{\sigma | \Omega_j |} \left(\underbrace{ \int_{\partial \Omega_j }   \partial_n p d\tau 
 -
  \sum_r l_{jr}\left(\njr, \nabla p (\xr,t) \right) }_{=d^2_j}\right).
$$
The first term  is $| d^1_j |\leq C \left\|  p \right\|_{H^4(\Omega_j)}$
by virtue of the first inequality of the proposition (\ref{pro:interp}) with $\mathbf x_r$ changed
into $\mathbf x_j$.
%$$
%\partial_t p(\xj,t) = \frac{\int_{\Omega_j} \partial_t
%p(x,t)dx}{\left| \Omega_j \right|}+O(h)=
%\frac{1}{\sigma}\frac{\int_{\Omega_j} \Delta p dx}{\left| \Omega_j
%\right|}+O(h)=
%\frac1{\sigma \left| \Omega_j \right|}\int_{\partial \Omega_j} \partial_n p d\sigma
%+O(h).
%$$
The second term $d_j^2$ can be rearranged. Indeed 
by definition of $l_{jr} \njr$ one has
$$
 \sum_r l_{jr}\left(\njr, \nabla p (\xr,t) \right)=
\sum_k \int_{\partial\Omega_{jk}} \left(
\frac{  \nabla p\left(x_{jk}^+ \right)+ \nabla p\left(x_{jk}^- \right)   }2,
\mathbf{n}_j \right) d\tau
$$
where $n_j=\tilde{n}_{j,r}$ defined in the previous part and the nodes $x_{jk}^+$ and $x_{jk}^-$ are the end of the edge
$\partial\Omega_{jk}=\Omega_j\bigcap\Omega_k$, with the relation $\partial \Omega_j =
\bigcup\partial\Omega_{jk} $.
Therefore
$$
%a_j=O(h) + \frac{1}{\sigma}\frac1{\left| \Omega_j \right|}
d_j^2=
\sum_k \int_{\partial\Omega_{jk}} \left(
\nabla p- \frac{  \nabla p\left(x_{jk}^+ \right)+ \nabla p\left(x_{jk}^- \right)
  }2, \mathbf{n}_j \right) d\tau .
$$
%Since the function under the integral is approximated by the trapezoidal rule, 
%the error of integration
% is $O(h^2)$
%$$
%\left| \int_{\partial\Omega_{jk}} \left(
%\nabla p- \frac{  \nabla p\left(x_{jk}^+ \right)+ \nabla p\left(x_{jk}^- \right)
%  }2, \mathbf{n}_j \right) d\sigma   \right| \leq C h^2 \left|
%\partial\Omega_{jk}\right|\leq Ch^3.
%$$
The second inequality of the proposition \ref{pro:interp} yields
$|d_j^2| \leq C_\mathcal A h^2 \|   p \|_{H^4(\Omega_j)}$
Therefore one can write  $a_j\leq \widetilde C \| p_0 \|_{H^4(\Omega_j)}$ where the constant is uniform
with respect to $j$. It yields
\begin{equation} \label{eq:bac1}
\| a\|_{L^2(\Omega)} = \sqrt{ \sum_j  |\Omega_j | a_j^2  }\leq \sqrt{ \sum_j  |\Omega_j| C^2 \| p \|_{H^4(\Omega_j)  }^2 } \leq C h \| p \|_{H^4(\Omega)  }\leq C h \| p_0 \|_{H^4(\Omega)  }.
\end{equation}
Since it is true at any time $t$, it yields the first bound (\ref{eq:d20}).

The second inequality can be  obtained with the same argument.
Consider  the decomposition
$$
\nabla p (\mathbf x_r)-\nabla p (\mathbf x_j)=
\left( \nabla p (\mathbf x_r) - -\frac{\int_{\Omega_j }\nabla p (\mathbf x) dv}{|\Omega_j |} \right)
-\left( \nabla p (\mathbf x_j) - -\frac{\int_{\Omega_j }\nabla p (\mathbf x) dv}{|\Omega_j |} \right).
$$
Each parenthesis can be estimated with the first inequality
of proposition \ref{pro:interp}. The rest of the proof of the second
 bound (\ref{eq:d20}) is immediate since $l_{jr}$ is neutralized by the $\frac1h$.
%$$
%\mathbf b_j=-\frac{1}{\sigma h}\sum_{r}l_{jr} 
%\left(\njr,\nabla p (\mathbf x_r) -\frac{\int_{\Omega_j }\nabla p (\mathbf x) dv}{|\Omega_j |}
%\right))\njr 
%+
%\frac{1}{\sigma h}\sum_{r}l_{jr} 
%\left(\njr,\nabla p (\mathbf x_j) -\frac{\int_{\Omega_j }\nabla p (\mathbf x) dv}{|\Omega_j |}
%\right))\njr 
%$$

The third term estimate is analyzed as follows. %One has 
We write  $\ccr=\ccr^a+\ccr^b$  with
$$
\ccr^a = 
\frac1{\vert V_r \vert}\left(  \left( \sigma \sum_j l_{jr}\njr\otimes
\left(\xr - \xj    \right)  \right)  
 \bigg(-\frac{\nabla p (\xr,t)}{\sigma}\bigg) -   \sum_j  l_{jr}\njr
p(\xj,t)\right)
$$
$$
= \frac1{\vert V_r \vert} \sum_j  
\bigg(
  \big( \xj - \xr  , \nabla p (\xr,t)   \big)  
  -    p(\xj,t)  
\bigg)
l_{jr}\njr
$$
$$
= \frac1{\vert V_r \vert} \sum_j  
\bigg(
  \big( \xj - \xr  , \nabla p (\xr,t)   \big)  
  -    p(\xj,t)  + p(\xr,t)
\bigg)
l_{jr}\njr
$$
and 
$
\ccr^b = 
\frac\eps{\sigma \vert V_r \vert} \left(  \sum_j l_{jr}\njr\otimes \njr \left( \nabla p
(\xj,t)  - \nabla p (\xr,t)\right )\right )
$. 
The first  interpolation of proposition \ref{pro:interp} can be used to evaluate the 
difference of point values in $ \ccr^b$. It yields
$
| \ccr^b | \leq C \frac \varepsilon h \| p \|_{H^3(\Omega_j)} $. 
Concerning $\ccr^a$ we notice that
$$
 \big( \xj - \xr  , \nabla p (\xr,t)   \big)  
  -    p(\xj,t)  + p(\xr,t)=
\left(   \frac1{|\x_r -\x_j  |} 
\int_{\xj}^{\xr}  \nabla p (\mathbf x)  d\tau-  \nabla p(\xr) 
, \xr -\xj
\right)
$$
where the integral in along the chord between $\xj$ and $\xj$.
The first term in the scalar product is the comparison between a mean value and a point value. So it can be estimated as in proposition \ref{pro:interp}. It yields similarly
\begin{equation} \label{eq:cch1}
\left| 
 \left(   \frac1{|\x_r -\x_j  |} 
\int_{\xj}^{\xr}  \nabla p (\mathbf x)  d\tau-  \nabla p(\xr) 
, \xr -\xj
\right)
\right| \leq C h \| p \|_{H^3(\Omega_j)}.
\end{equation}
Thus
$
| \ccr^a | \leq {C h }\| p \|_{H^3(\Omega_j)}$.
After summation of the $\ccr^a$s and $\ccr^b$s, one gets
the last inequality of the claim. The constant  $C_c$ is the maximum of the three
constants that show up in the three inequalities.
%
%So
%$
%\br ^a= 
%\frac1{\vert V_r \vert}  \sum_j O_{jr}(h^2)  l_{jr}\njr =O(h) \| p_0 \|_{H^4(\Omega)}
%$
%due to (\ref{eq:d7}), (\ref{eq:d5.2}). 
%Another Taylor expansion shows that 
%$ \nabla p (\xj,t)-\nabla p (\xr,t)=O_{jr}(h) \| p_0 \|_{H^4(\Omega)}$
%so that 
%$\vert \vert \br^b\vert \vert=O(\eps)$ and
%$| \br|=O(h+\eps) \| p_0 \|_{H^4(\Omega)}$.
% We finally obtain with similar arguments
%$ \vert \cj  \vert=O( h) \| p_0 \|_{H^4(\Omega)}$. 
%It ends the proof.
\end{proof}

%\subsection{Error at initialization}

\subsubsection{Convergence}

We study the numerical error between the solution of the %scheme 
diffusion asymptotic scheme written as (\ref{das:1}) and the point values
of the exact solution (\ref{eq:con1}).
Let us define  three  error variables 
$$
e_j= p_j -\pej
\mbox{ , }
\fr = \ur -\uer
\mbox{ and }
\mathbf g_j = \uj -\uej
$$ 

 
\begin{proposition} \label{theor:main0}
%Assume $p \in W^{3, \infty} (\Omega)$ and assume that geometrical conditions
%(\ref{geometrie}) are verified.
There  exists a  constant
$C>0$ independent of $h$ and $\varepsilon$ such that 
\begin{equation} \label{eq:dconv1}
 \| e \|_{L^\infty(  [0,T]: L^2(\Omega) )}
 \leq C(h+\eps) \| p_0  \|_{H^4(\Omega)} ,
\end{equation}
\begin{equation} \label{eq:dconv2}
 \| \ff \|_{L^2([0,T]\times \Omega)} \leq C (h+\eps)  
\| p_0  \|_{H^4(\Omega)}  
\end{equation}
and
     \begin{equation} \label{eq:dconv4}
\| \gf \|_{L^2([0,T]\times \Omega)} \leq C (h+\eps)\sqrt{(1+\frac
h\eps )} \| p_0  \|_{H^4(\Omega)} . 
\end{equation}
Moreover                                                           
\begin{equation} \label{eq:dconv3}
\sqrt{\eps \int_0^T \sum_j \sum_r    \ljr \left( \fr  -
\fj ,\njr \right)^2 } \leq C (h+\eps)\| p_0  \|_{H^4(\Omega)} , 
 \end{equation}
and 
%where
%\begin{equation*}
%\left\{
%\begin{aligned}
%&C_e(T) = \sqrt{2} e^{T/2} \max\bigg\{  \: C_0^2\|\mathbf{W}^{\eps}_0\|_{H^1(\Omega)}^2  \:  , \:  T\frac{2C_c^2|\Omega| }{\alpha \sigma} \max \bigg( 1 \: ,\: \frac{1}{C_\mathcal M^2\alpha}\max\big(\alpha \sigma ,  P \big) \bigg)  \bigg\}^{\frac12}    ,\\
%&C_f(T) = 2\sqrt{\frac{T+e^{-T}}{\alpha}}C_e , \\
%&C_h(T) = \sqrt{2( T+e^{-T})}C_e , \\
%&C_g(T) = \sqrt{\frac{2}{\alpha} } \max\bigg( C_h(T) , \frac{P}{C_\mathcal M}C_f^2(T) \bigg)^{\frac12} .
%\end{aligned}
%\right.
%\end{equation*}
\end{proposition}


\begin{proof}
By construction
\begin{equation*}\label{eq:d30}
\left\{ 
\begin{aligned}
& \mid\Omega_{j}\mid e'_{j}+
\sum_{r}(l_{jr}\fr,\njr)=-\mid\Omega_{j}\mid a_j\\
& \sum_{r}l_{jr} (\njr,\fr -\fj)\njr=-h\bj,  \\
& \left( \eps\sum_{j}\alj+  \sigma B_r\right)\fr-
\sum_{j}\ljr e_j\njr-\eps\sum_j\alj\fj=-\mid
V_r\mid\ccr .
\end{aligned}
\right.
\end{equation*}
The errors are measured with
$
E(t)=\frac12  \| e \|_{L^2(\Omega)}^2 $, 
$
F(t)=
\| \ff \|_{L^2([0,t]\times \Omega)}^2=
 \int_0^t \sum_r \left| V_r \right| |\fr|^2
$
and 
$
\| \gf \|_{L^2([0,t]\times \Omega)}^2=  \int_0^t
\sum_j
 \left| \Omega_j \right| \   \left| \fj  \right|^2 $.
By proceeding as for the results of stability one has the identity
\begin{equation*}
\begin{aligned}
E'(t)&= \sum_j |\Omega_j| e_j e_j'=
\sum_j  e_j\left(
- \left( \sum_r l_{jr}\left(\njr, \fr  
   \right)\right)
- \left| \Omega_j \right| a_j 
\right) \\
&
=- \sum_r \sum_j (  l_{jr}  \mathbf n_{jr} e_j , \mathbf f_r)
- \sum_j\left| \Omega_j \right| e_j a_j \\
&= -\sum_r \left (\fr, \left( \eps\sum_{j}\alj+  \sigma
B_r\right)\fr- \eps\sum_j\alj\fj\right )
- \sum_j \left| \Omega_j \right| e_ja_j
- \sum_r \left| V_r \right| (\ccr, \fr) \\
&= -\sum_r \left (\sigma B_r \fr, \fr\right ) 
-\eps \sum_r\left( \fr, \sum_j \alj(\fr -\fj ) \right )
- \sum_j \left| \Omega_j \right| e_j a_j
- \sum_r \left| V_r \right| (\ccr, \fr) \\
&= -\sum_r \left (\sigma B_r \fr, \fr\right ) 
-\eps\sum_j\sum_r \ljr \left( \fr- \fj,  \njr\right )^2
- \sum_j \left| \Omega_j \right| e_j a_j
- \sum_r \left| V_r \right| (\ccr ,\fr) +\eps\sum_j h (\bj,\fj).
\end{aligned}
\end{equation*}
Using  Young's inequality %, the Cauchy-Schwarz inequality
 and assumptions \eqref{eq:d5} and \eqref{geom3}, one gets
\begin{equation} \label{eq:yi}
\begin{aligned}
E'(t) \leq & E(t) % ||e||_{L^2(\Omega)} 
 + \frac12 ||a||_{L^2(\Omega)}^2 +
\left(
\frac{\mu}{2} ||\ff ||^2_{L^2(\Omega)} + \frac{1}{2\mu} ||\cc||^2_{L^2(\Omega)} 
\right)
-  \alpha \sigma \|  \ff \|^2_{L^2(\Omega)}\\
&
  -\eps\sum_j\sum_r \ljr \left( \fr- \fj,  \njr\right )^2 
+\frac{\eps}{2h  C_\mathcal M}\left (\eta||\gf ||^2_{L^2(\Omega)}
+\frac{1}{\eta}||\bb||^2_{L^2(\Omega)} \right )
\end{aligned}
\end{equation}
where $ \mu,\eta >0$ are two arbitrary coefficients that will be 
specified later. 
Now using  (\ref{geom4}) and \eqref{eq:d7},
we have
\begin{equation} \label{eq:fff1}
\vert \Omega_j\vert \ \vert \fj \vert^2\leq \frac{h}{\alpha} \sum_r\ljr(\njr,\fj)^2.
\end{equation} 
Therefore
$$
\vert \Omega_j\vert \ \vert\fj \vert^2\leq 
\frac{h}{\alpha} \left(2\sum_r\ljr(\njr,\fj-\fr)^2+2\sum_r \ljr \vert 
\fr \vert^2\right ),
$$
which yields, using \eqref{eq:d5.2} and \eqref{nbmr}
\begin{equation}\label{gl2}
\vert\vert\gf \vert\vert^2_{L^2(\Omega)} \leq 
\frac{2h}{\alpha} \sum_{jr}\ljr(\njr,\fj-\fr)^2 +  \frac{2P}{\alpha C_\mathcal M} \vert\vert\ff \vert\vert^2_{L^2(\Omega)}.
\end{equation}
So from (\ref{eq:yi}) we  obtain 
\begin{equation*}
\begin{aligned}
E'(t) &\leq  E(t) %||e||_{L^2(\Omega)} 
+\frac12  ||a||_{L^2(\Omega)}^2 +
 \frac{1}{2\mu} ||\cc||^2_{L^2(\Omega)} 
+\bigg( \frac{\mu}{2} + \frac{\eps P\eta }{ h C_\mathcal M^2 \alpha} - \sigma \alpha \bigg) \|  \ff \|^2_{L^2(\Omega)}
\\
&+ \bigg(\frac{ \eta}{C_\mathcal M \alpha} -1 \bigg)\eps\sum_j\sum_r \ljr \left( \fr- \fj,  \njr\right )^2 
+\frac{\eps}{2h C_\mathcal M \eta } ||\bb||^2, \qquad \forall \mu,\eta >0.
\end{aligned}
\end{equation*}
Let us choose the free coefficients $\mu$ and $\eta$
so that
$$
 \frac{\mu}{2} + \frac{\eps P\eta }{ h C_\mathcal M^2 \alpha} - \sigma \alpha 
 \leq -
 \frac{\sigma \alpha }{4}
 \mbox{ and }
 \frac{ \eta}{C_\mathcal M \alpha} -1 \leq -\frac12 .
$$
Let us choose first 
$\mu=\frac{\alpha\sigma}{2}$. The two inequalities reduce to
$$
%\frac{\mu}{2} +
 \frac{\eps P \eta}{ h C_\mathcal M^2 \alpha}  %- \frac{  \sigma \alpha  }2
 \leq 
 \frac{\sigma \alpha }{2}
 \mbox{ adn }
 \frac{ \eta}{C_\mathcal M \alpha} \leq \frac12 .
$$
 A natural solution is  $\eta =\frac{C_\mathcal M\alpha}{2} \min\big(1,\frac{\alpha \sigma hC_\mathcal M}{ \eps  P} \big)$. 
 So %These  values yield  %s, using the inequality $ab\leq \frac12(a^2+b^2)$
\begin{equation*} \label{eq:d40}
\begin{aligned}
E'(t) \leq & E(t)    - \frac{\alpha\sigma}{4}  F'(t)- \frac{\eps}{2}\sum_j\sum_r \ljr \left( \fr- \fj,  \njr\right )^2
\\
& 
+ \frac12  ||a||_{L^2(\Omega)}^2 
+  \frac{1}{ \alpha\sigma } ||\cc||^2_{L^2(\Omega)}
+  %  \frac{1}{C_\mathcal M^2\alpha} \max \bigg( \frac{\eps}{h} , \frac{\eps^2}{h^2} \frac{ P}{ \alpha \sigma C_\mathcal M } \bigg) 
\frac{\eps}{2h C_\mathcal M \eta } ||\bb||^2_{L^2(\Omega)} .
\end{aligned}
\end{equation*}
%where we have used  $\frac1{\min(a,b)}=\max(\frac1a, \frac1b)$.
By the consistency estimates (\ref{eq:d20}-\ref{eq:d22}-\ref{eq:d21}), one has 
$$
 \frac12  ||a||_{L^2(\Omega)}^2 
+  \frac{1}{ \alpha\sigma } ||\cc||^2_{L^2(\Omega)}
+  %  \frac{1}{C_\mathcal M^2\alpha} \max \bigg( \frac{\eps}{h} , \frac{\eps^2}{h^2} \frac{ P}{ \alpha \sigma C_\mathcal M } \bigg) 
\frac{\eps}{2h C_\mathcal M \eta } ||\bb||^2_{L^2(\Omega)}
$$
$$
\leq C_c^2 \left( \frac12  h^2 +\frac1{\alpha \sigma}  (h+\varepsilon)^2
+\frac{\eps}{2h C_\mathcal M \eta }h^2
\right) \|    p  \|_{L^\infty([0,T]: H^4(\Omega))}^2
$$
$$
C_c^2 \left( \frac12  h^2 +\frac1{\alpha \sigma}  (h+\varepsilon)^2
+\frac{\eps}{2h C_\mathcal M \eta }h^2
\right) \|    p _0 \|_{ H^4(\Omega)}^2.
$$
%The scaling with respect to $h$ and $\varepsilon$ of t
The last term in the parenthesis
is 
$$
\frac{\eps}{2h C_\mathcal M \eta }h^2 = \frac1{C_\mathcal M^2\alpha}\varepsilon h 
\max\left( 1, \varepsilon P/(\alpha \sigma h   C_\mathcal M )   \right)
$$
$$
\leq
\frac1{C_\mathcal M^2\alpha}\varepsilon h 
\left( 1+ \varepsilon P/(\alpha \sigma h   C_\mathcal M )   \right)=
\frac1{C_\mathcal M^2\alpha}\varepsilon h 
+ \frac P{C_\mathcal M^3\alpha^2\sigma}\varepsilon ^2.
$$
So there exists a constant $C_e$ independent of $h$ and $\varepsilon$ such that
\begin{equation} \label{eq:efg1}
E'(t) \leq  E(t)    - \frac{\alpha\sigma}{4}  F'(t)- \frac{\eps}{2}\sum_j\sum_r \ljr \left( \fr- \fj,  \njr\right )^2 + C_e (h+\varepsilon)^2 \|    p_0  \|_{H^4(\Omega)}^2.
\end{equation}
%Continuer revoir preuve.
% and since the domain $\Omega$ is bounded, one finds
%\begin{equation} \label{eq:d40bis}
%E'(t) \leq  E(t)  - \frac{\sigma \alpha}{4}  F'(t)
%- \frac{\eps}{2}\sum_j\sum_r \ljr \left( \fr- \fj,  \njr\right )^2
%+  C\left (h+\eps\right )^2,
%\end{equation}
%where
%\begin{equation*}
%C = \frac{2C_c^2|\Omega| }{\alpha \sigma} \max \bigg( 1 \: ,\: \frac{1}{C_\mathcal M^2\alpha}\max\big(\alpha \sigma , C_{\mathcal{M}} P \big) \bigg).
%\end{equation*}
%Thus 
%\begin{equation}\label{esti_E_DA}
%E'(t) \leq  E(t)  
%+  C\left (h+\eps\right )^2. 
%\end{equation}
By construction $E(0)\leq C_\mathcal A^2h^2\| p_0\|_{H^2(\Omega)}^2$, it 
%viewed as a consequence/corollary
 of inequality  \eqref{constant_init} which compares mean value and point
value.
 So one gets by the Gr\"onwall lemma
$
E(t)\leq C_f ^2(t) (h+\eps)^2 \|    p_0  \|_{H^4(\Omega)}^2
$, where the constant $C_f$ is depends on $T$ and $t\leq T$.
 %with $C_e(t) = \sqrt{2} \max ( C_\mathcal A^2\|\mathbf{W}^{\eps}_0\|_{H^1(\Omega)}^2 , T C )^{\frac12}e^{T/2}$,
 It gives (\ref{eq:dconv1}).

Integrating  (\ref{eq:efg1}),  we find that for any 
for $t\leq T$
$$
E(t)+  \frac{\alpha \sigma }{4}  F(t)+\int_0^t \frac{\eps}{2}\sum_j\sum_r \ljr \left(
\fr- \fj,  \njr\right )^2
$$
$$
\leq
 E(0)+ \int_0^t  E(s) ds 
 + t 
 C_e (h+\varepsilon)^2 \|    p_0  \|_{H^4(\Omega)}^2
\leq  C_g  (h+\varepsilon)^2 \|    p_0  \|_{H^4(\Omega)}^2
$$
where $C_g$ is uniform for $t\leq T$.
It shows   
the  estimates (\ref{eq:dconv2}) and (\ref{eq:dconv3}).

Integrating  (\ref{gl2}) and using the estimates \eqref{eq:dconv2} and \eqref{eq:dconv3}, one gets
\begin{equation*}\label{gl3}
\int_0^T \vert\vert\gf \vert\vert^2_{L^2(\Omega)} \leq 
\frac{2}{\alpha} h \int_0^T\sum_{jr}\ljr(\njr,\fj-\fr)^2 +  \frac{2P}{\alpha C_\mathcal M} \vert\vert\ff \vert\vert^2_{L^2([0,T]\times\Omega)}
\leq
C_h (h+\eps)^2(1+\frac h\eps) \|    p_0  \|_{H^4(\Omega)},
\end{equation*}
where the constant $C_g$ is uniform for $t\leq T$, and for $h$ and $\varepsilon$.
%\begin{equation*}
%C_g(T) = \sqrt{\frac{2}{\alpha} }  \max\bigg( C_h(T) , \frac{P}{C_\mathcal M}C_f^2(T) \bigg)^{\frac12} . 
%\end{equation*}
%from which (\ref{eq:dconv4}) follows.
 The proof is finished.
\end{proof}

\begin{proof}[Proof of lemma \ref{lemme:tiptop}]
The norm of the estimate in the lemma  \ref{lemme:tiptop} can be bounded 
from   the sum of (\ref{eq:dconv1}) and (\ref{eq:dconv4}).
However one must rescale  back (\ref{eq:dconv4}) since it corresponds to the scaled variable
(\ref{eq:barn}). This is why (\ref{eq:dconv4}) is must be multiplied by $\varepsilon$. It eliminates the 
$\varepsilon^{-\frac12}$
divergence in (\ref{eq:dconv4})  and ends the proof.
\end{proof}

\subsubsection{Technical estimates}

These technical estimates are  needed in the next section.
The first result compares two different velocities  at the initialization stage:   
on the one hand  the velocity   computed as the solution of  the linear system
made of the two last equations of (\ref{das:1}), on the other hand  the exact point wise
velocity.  
%The divergence of the second inequality for $\varepsilon=0^+$
%is the reason one cannot use the scheme $\mathbf P_h^0=\mathbf{DA}_h^{\varepsilon=0}$.

\begin{pro}\label{esti_initial_data}
There exists a constant $C$ independent of $h$ and $\varepsilon$
such that
\begin{equation} \label{eq:iner1}
\bigg\| \left(  \uu_r +\frac1\sigma \nabla p(\x _r)\right)(t=0)\bigg\|_
{ L^2(\Omega) }\leq C \sqrt {h\max( h, \eps )} \| p_0 \|_{H^3(\Omega)}
\end{equation}
and
\begin{equation} \label{eq:iner10}
\bigg\| \left(  \uu_j +\frac1\sigma \nabla p(\x _j)\right)(t=0)\bigg\|_
{ L^2(\Omega) }\leq C  
 \sqrt{ \frac{h}\eps} \max(  h, \eps)\| p_0 \|_{H^3(\Omega)}.  
\end{equation}
\end{pro}
\begin{proof}
Let us write $\mathbf q_r= \uu_r +\frac1\sigma \nabla p(\x _r)$
and $\mathbf s_j= \uu_j +\frac1\sigma \nabla p(\x _j)$.
These quantities are solution
of the system
$$
\left\{
\begin{array}{ccll}
 \left( \eps\sum_{j}l_{jr} \njr\otimes \njr  +  \sigma B_r\right) \mathbf q_r
& -
\eps \sum_{j}l_{jr} \njr\otimes \njr \mathbf s_j& =
\mathbf  d_r^1+\mathbf  d_r^2 , & \forall r,  \\
- \eps \sum_{r}l_{jr} (\njr, \mathbf q_r )\njr & +  
\eps  \sum_{r}l_{jr} (\njr, \mathbf s_j )\njr& =\mathbf  d_j,
&
 \forall j, 
\end{array}
\right.
$$
where the right hand sides are
$$
\mathbf  d_r^1=
\sum_{j}\ljr   p(\mathbf x_j)\njr+ \sum_j l_{jr}  (\x_r -\x_j, \nabla p(\x_r))\njr,
$$
$$
\mathbf  d_r^2=\eps\sum_j l_{jr}  \left( \njr, \nabla p(\x_r)
- \nabla p(\x_j) \right)
\njr    
$$
and
$$
\mathbf  d_j=-\eps\sum_r l_{jr}  \left( \njr, \nabla p(\x_r)
- \nabla p(\x_j) \right).
$$
The right hand side $ \mathbf d_r^1$   can be interpreted as a consistency error.
Indeed it  can be rewritten as
$$
\mathbf  d_r^1=
 \sum_{j} \left[     p(\mathbf x_j)- p(\mathbf x_r) + (\x_r -\x_j, \nabla p(\x_r) ] \right)\ljr \njr ,
$$
one obtains from (\ref{eq:cch1}) the bound 
$
| \mathbf  d_r^1 |\leq  \sum_{j \  {\rm neighboring } \ r} \left[ \widetilde C h \|  p \| _{H^3(\Omega_j)}   \right]h$. 
It yields after summation
\begin{equation} \label{eq:ddd1}
\| \mathbf d^1\|_{L^2(\Omega)} \leq C h^3 \|  p \| _{H^3(\Omega)} , \qquad C= \widetilde CP.
\end{equation}
Taking the scalar product of the first line by $\mathbf q_r$
and of the second line by   $\mathbf s_j$, one gets the identity
$$
\sigma \sum_r \left(B_r \mathbf q_r, \mathbf q_r    \right)
+ \eps \sum_{jr} l_{jr}
\left( \njr, \mathbf q_r - \mathbf s_j  \right)^2
$$
$$
=
\sum_r \left( \mathbf d_r^1, \mathbf q_r   \right)+  
\eps \sum_{jr} l_{jr}
\left( \njr, \mathbf q_r - \mathbf s_j  \right)
\left( \njr, 
 \nabla p(\x_r)
- \nabla p(\x_j)
 \right)
$$
where  $\mathbf d^1$ shows up explicitely. %Indeed $\mathbf d$ and $\mathbf d^2$ cancel each other.
A Young's inequality yields
\begin{equation} \label{eq:y1}
\sigma \sum_r \left(B_r \mathbf q_r, \mathbf q_r    \right)
+\frac\eps2
 \sum_{jr} l_{jr}
\left( \njr, \mathbf q_r - \mathbf s_j  \right)^2\leq 
\sum_r \left( \mathbf d_r^1, \mathbf q_r   \right)+
 \frac\eps2  \sum_{jr} l_{jr}
\left( \njr, 
 \nabla p(\x_r)
- \nabla p(\x_j)
 \right)^2.
\end{equation}
The first term in the right hand side is
$$
\sum_r \left( \mathbf d_r^1, \mathbf q_r   \right)= \sum_r h^2 \left( \frac1{h^2}\mathbf d_r^1, \mathbf q_r   \right)
\leq C \frac1{h^2} \| \mathbf d^1\|_{L^2(\Omega)} \| \mathbf q\|_{L^2(\Omega)}\leq
C h  \|  p \| _{H^3(\Omega)} \| \mathbf q\|_{L^2(\Omega)}.
$$
A similar reasoning as for (\ref{eq:ddd1}), which  one more time can be viewed as
a consequence   of the first technical inequality of proposition (\ref{pro:interp}), is  %eq:cch1
$$
  \sum_{jr} l_{jr}
\left( \njr, 
 \nabla p(\x_r)
- \nabla p(\x_j)
 \right)^2\leq C h \| p \|_{H^3(\Omega)}^2.
$$
So (\ref{eq:y1}) implies (after redefinition of the constants)
\begin{equation*} \label{eq:qq1}
\|\mathbf q\|_{L^2(\Omega)}^2\leq C\left( 
\|\mathbf q\|_{L^2(\Omega)}\| p \|_{H^3(\Omega)} +\varepsilon  \| p \|_{H^3(\Omega)}^2\right) h .
\end{equation*}  
It means that $z= \frac{ \|\mathbf q\|_{L^2(\Omega)}}{\| p \|_{H^3(\Omega)}}  $ is below the maximal root
of the polynomial $p(z)=z^2-Chz-C\eps h$, that is
for some constant $K >0$
$%\begin{equation} \label{eq:iner11}
z\leq x^+=\frac{Ch+\sqrt{C^2h^2+4C\eps h  }}{2}
\leq K \sqrt{ \max(h^2 , h \eps  )}$.
%\end{equation}
Noticing that $\| p \|_{H^3(\Omega)} \leq \| p_0 \|_{H^3(\Omega)} $, 
It finishes the proof of the first inequality (\ref{eq:iner1}).

Concerning the second inequality, we start from  (\ref{eq:fff1})  to show that 
$$
\|\mathbf s\|_{L^2(\Omega)}^2=
\sum_j | \Omega_j | |\mathbf s_j |^2\leq \frac h\alpha  \sum_{j}\sum_r  
 l_{jr}
\left( \njr,  \mathbf s_j  \right)^2
$$
$$
\leq 2 \frac h\alpha  \sum_{j}\sum_r  
 l_{jr}
\left( \njr,  \mathbf q_r   \right)^2
+
2 \frac h\alpha  \sum_{j}\sum_r  
 l_{jr}
\left( \njr,  \mathbf q_r  -\mathbf s_j  \right)^2
$$
$$
\leq 2 \frac 1 {C_\mathcal M  \alpha}  \sum_r  
 | V_r| 
\left|   \mathbf q_r   \right|^2 + 2 \frac h\alpha  \sum_{j}\sum_r  
 l_{jr}
\left( \njr,  \mathbf q_r  -\mathbf s_j  \right)^2
%$$
%$$
\leq 
2\frac 1 {C_\mathcal M  \alpha} \|\mathbf q\|_{L^2(\Omega)}^2+ 
2 \frac h\alpha  \sum_{jr} l_{jr}
\left( \njr, \mathbf q_r - \mathbf s_j  \right)^2.
$$
The first term is natural bounded   bounded using (\ref{eq:y1}).
The crux of the estimate is the second term which is naturally bounded by  (\ref{eq:iner1})
$$
 \sum_{jr} l_{jr}
\left( \njr, \mathbf q_r - \mathbf s_j  \right)^2 \leq \frac 2 \varepsilon
\left(K   \sqrt{\max(h^2, h\eps)  } h  + C  \varepsilon h 
\right)\| p _0\|_{H^3(\Omega)}  ^2 
\leq D (h+\varepsilon) \frac h \varepsilon \| p _0\|_{H^3(\Omega)}  ^2,
\quad D>0.
$$
 We obtain 
therefore
$$
\|\mathbf s\|_{L^2(\Omega)}^2
\leq C \left(   { \max(h^2 , h \eps  )}
+h  (h+\varepsilon) \frac{h}\eps
 % \sqrt{\max(h^2, h\eps)  }
 \right) \| p _0\|_{H^3(\Omega)}  ^2, \qquad C>0.
$$
The numbers  $h$ and $\eps$ can be considered less than 1.
There are two cases:
 Either $h<\eps$  so
$
\|\mathbf s\|_{L^2(\Omega)}^2
\leq \widetilde C  h\eps    \| p _0\|_{H^3(\Omega)}  ^2
$ for another constant $\widetilde C$;
or $\eps\leq h$, so 
$
\|\mathbf s\|_{L^2(\Omega)}^2
\leq \widetilde C \frac{h^3}\eps \| p _0\|_{H^3(\Omega)}  ^2$
 for another constant $\widetilde C$.
So we can writes
$
\|\mathbf s\|_{L^2(\Omega)}
\leq C \sqrt{ \frac{h}\eps} \max(  h, \eps)$ for a certain constant $ C>0$ independent of $h$ and $\varepsilon$.
The proof of (\ref{eq:iner10}) is ended.
\end{proof}




\begin{pro}
There exists a constant $C$ independent of $h$ and $\varepsilon$ such that
%\begin{equation*} \label{eq:der1}
%\bigg\|  \frac{d}{dt} p_h  \bigg\|_{ L^\infty([0,T];L^2(\Omega)   )  }
%\leq C \max\left(1,\sqrt{\frac{\eps} {h} }
%\right)\| p_0 \|_{H^3(\Omega)},  
%\end{equation*}
%and
\begin{equation}\label{e:qder2}
\bigg\|  \left( \frac{d}{dt} \mathbf u_r  \right) \bigg\|_{L^2([0,T] \times  \Omega)   }
\leq C \max\left(1,\sqrt{\frac{\eps} {h} }
\right)\| p_0 \|_{H^3(\Omega)}.
\end{equation}
\end{pro}
\begin{proof}
The proof is essentially a consequence of the previous proposition.
Let us denote the time derivative of any $f$
as $\tilde f=\partial_t f$.
By linearity of the system (\ref{das:1}), one has
\begin{equation*}\label{das:1:der}
\left\{ 
\begin{array}{l}
\displaystyle \mid\Omega_{j}\mid \frac{d}{dt} \tilde p_{j}+
\sum_{r}(l_{jr}  \tilde\uu_r,\njr)=0\\
 \sum_{r}l_{jr} (\njr, \tilde\uu_r - \tilde\uu_j)\njr=0\\
 \left( \eps\sum_{j}\alj+  \sigma B_r\right) \tilde\uu_r=
\sum_{j}\ljr  \tilde p_j\njr+\eps\sum_j\alj \tilde\uu_j
\end{array}
\right.
\end{equation*}
The $L^2$ stability property yields
\begin{equation} \label{eq:vvpp}
\|  \tilde p_h\|^2_{L^\infty([0,T];L^2(\Omega)  )   } 
+\int_0^T \sum_r ( B_r \tilde\uu_r, \tilde\uu_r )dt
\leq
\|  \tilde p_h(0)\|_{L^2(\Omega)}^2
\end{equation}
where this last quantity can be estimated
with the first equation of (\ref{das:1}):
the square of the norm in (\ref{e:qder2}) is also bounded by
the same quantity.
 It remains to bound $\|  \tilde p(0)\|_{L^2(\Omega)  )  } $.
 Using again the notation $\mathbf q_r = \mathbf u_r + \frac1\sigma \nabla p(\mathbf x_r)$, 
we consider at time  $t=0$  the relation %one has 
$$
\tilde p_j=
\frac{d}{dt} p_j=-
\frac1{|\Omega_j|}
\sum_r l_{jr}  ( \mathbf u_r,\njr)
=\frac1\sigma
 \underbrace{ \frac1{|\Omega_j|}
 \sum_r l_{jr}  ( \nabla p(\x_r) ,\njr)}_{=v_j^1}
-% \frac1\sigma
 \frac1{|\Omega_j|}
\underbrace{ \sum_r l_{jr}  ( \mathbf q_r ,\njr)}_{=v_j^2}.
$$
One has $v_j^1=  \frac1{|\Omega_j|} \sum_r l_{jr}  ( \nabla p(\x_r) - \nabla p(\x_j) ,\njr)$.
Using techniques which have been used many times in this paper, one has
$| v^1_j | \leq C \frac 1h \| p \|_{H^3(\Omega_j)} $, which turns into
$$
\|  v^1  \|_{L^2(\Omega)}= 
\sqrt{ \sum_j |\Omega_j | (v_j^1)^2    }
\leq C \| p \|_{H^3(\Omega)}
\leq C \| p_0 \|_{H^3(\Omega)}, \qquad C>0.
$$
The other term is naturally bounded by the norm of $\mathbf q$, that is
$
\|  v^2  \|_{L^2(\Omega)}\leq \frac P{C_\mathcal M h}\|\mathbf q\|_{L^2(\Omega)},
$
$P$ the maximal number of vertices per cell.
Going back to (\ref{eq:iner1}), one obtains 
\begin{equation} \label{eq:vv2}
\|  v^2  \|_{L^2(\Omega)}\leq C  \frac1h  \sqrt{h\max(h,\varepsilon)   }\|\mathbf p_0\|_{H^3(\Omega)}
\leq C \sqrt{\max( 1, \varepsilon/h  )  }\|\mathbf p_0\|_{H^3(\Omega)}.
\end{equation}
The sum $\|  v^1  \|_{L^2(\Omega)}+\|  v^2  \|_{L^2(\Omega)}$
yields the bound for $\tilde p_h(0)$ that was looked for. The estimate is dominated 
by the worst term which is the right hand side of (\ref{eq:vv2}).
%Using the same consistency arguments as before, the first
%term in the right hand side is bounded in $L^2$ uniformly
%with respect to $h$ and $\eps$. The second one is bounded in $L^2$ 
%by a term of order 
%$C\frac{h \sqrt {\max( h^2, \eps h)}}{h^2}=C
%\max\left(1,\sqrt{\frac{\eps} {h} }
%\right)$. 
Plugging in (\ref{eq:vvpp}), 
the proof is finished.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Study of $\| \mathbf P_h^\varepsilon - \mathbf {DA}_h^\eps  \|$}

In this section we estimate the difference between the hyperbolic
scheme $ \mathbf P_h^\varepsilon $ and the diffusion asymptotic scheme
$ \mathbf {DA}_h^\eps$. %Indeed, we will prove the following result.
Since the discrete form of the discrete equations are very similar, this proof is 
simple.
This is where we get the clear benefit of the introduction of the new diffusion asymptotic scheme.

\begin{lemma}\label{jlbtodas}
There exists a constant $C^\rightarrow $  such that the following estimate holds
\begin{equation} \label{eq:ww1}
\|\V_h^{\eps}-\mathbf W_h^\eps\|_{L^2([0,T] \times \Omega))} \leq
C^\rightarrow \left( h^2+ \eps
\max\left(1,\sqrt{\frac{\eps} {h} }
\right)\right)  \|  p_0  \|_{H^3(\Omega)}.
\end{equation}
\end{lemma}

\begin{proof}
We introduce $\mathbf{R}_j= \frac{d}{dt} \mathbf u_j$  such that the solution $\V_h$ of the
diffusion scheme \eqref{das} satisfies
\begin{equation}\label{schemP11diff}
\left\{\begin{array}{l}
\ds \mid \Omega_j \mid\frac{d}{dt} p_j+\frac{1}{\eps}\sum_r(\ljr\njr,\ur)=0,\\
\ds \mid \Omega_j \mid\frac{d}{dt}\uj+\frac{1}{\eps}\sum_r(\ljr p_j\njr
+\alj(\uj-\ur)) =|\Omega_j| \mathbf{R}_j,\\
\ds \left(A_r+\frac{\si}{\eps}B_r\right) \ur-\sum_j\ljr
p_j\njr-\sum_j\alj  \uj=0 .
\end{array}\right.
\end{equation}
By definition $\|\mathbf{R}\|_{L^2(\Omega)}= \|\frac{d}{dt}
\uj\|_{L^2(\Omega)}$. Using the second line of \eqref{das}, one has
$\uj=A_j^{-1}\sum_r \alj\ur$ and thus $\|\frac{d}{dt}
\uj\|_{L^2(\Omega)}\leq C \|\frac{d}{dt} \ur\|_{L^2(\Omega)}$. 
Using
(\ref{e:qder2}) (and taking care
that  rescaling (\ref{eq:barn}) by a factor $\eps$
was systematically used
in the previous section), one gets
for a smooth initial data
\begin{equation*}\label{dddd}
\|\mathbf{R}\|_{L^2([0,T]\times\Omega)}\leq C
\eps \max\left(1, \sqrt{\frac\eps h}   \right) \|  p_0  \|_{H^3(\Omega)}.
\end{equation*}
We denote by $e_j=p_j-p_j^{\eps}, \;
\fj =\uj-\uj^{\eps}$ and
$\fr =\ur-\ur^{\eps}$. One finds, making the
difference between the schemes \eqref{schemP11diff} and \eqref{eq:4}:
\begin{equation*}\label{schemP11diff2}
\left\{\begin{array}{l}
\ds \mid \Omega_j \mid\frac{d}{dt} e_j+\frac{1}{\eps}\sum_r(\ljr\njr,\fr )=0,\\
\ds \mid \Omega_j \mid\frac{d}{dt} \fj +\frac{1}{\eps}\sum_r(\ljr e_j\njr
+\alj(\fj -\fr )) =|\Omega_j| \mathbf{R}_j,\\
\ds \left(A_r+\frac{\si}{\eps}B_r\right) \fr -\sum_j\ljr
e_j\njr-\sum_j\alj  \fj =0.
\end{array}\right.
\end{equation*}
 We are going to write an inequality satisfied  by $E(t)=
\| e(t)\|_{L^2(\Omega)  } ^2+\|\mathbf f(t)\|_{L^2(\Omega)  }^2
$, knowing that  $e(0)=0  $. Using the
same kind of proof than for the $L^2$ stability of the JL-(b) scheme
(proposition \ref{propesti}), one can show that
 \begin{equation*}
\frac12 \frac{d}{dt}
E(t) \leq \sum_j
| \Omega_j | 
(\mathbf{R}_j,\fj ) \leq \|\mathbf f\|_{L^2(\Omega)}  \| \mathbf R\|
_{L^2(\Omega)} \leq \sqrt{E(t)}
\| \mathbf R\|
_{L^2(\Omega)} .
\end{equation*}
By integration,  one has for $t\leq T$ 
$$
\sqrt{  E(t)}\leq \sqrt{E(0)}+
\sqrt T \|\mathbf  R\|_{L^2([0,T]\times \Omega)}
=
\| \mathbf f (0)\|_{L^2(\Omega)  } +\sqrt T
\|\mathbf  R\|_{L^2([0,T]\times \Omega)}.
$$
One has 
 $\|\mathbf f(0)\|_{L^2(\Omega)  }\leq C \sqrt{h\eps}\max(h,\eps)   \|  p_0  \|_{H^3(\Omega)} $
by virtue of (\ref{eq:iner10}) (taking care that there is a rescaling (\ref{eq:barn})
by  $\eps$).
We simply a little 
$\|\mathbf f(0)\|_{L^2(\Omega)  }\leq C
\left(h^2+\eps^2   \right)\|  p_0  \|_{H^3(\Omega)}$, so
$$
\sqrt{  E(t)}\leq C\left(  h^2 + \eps \max\left(1, \sqrt{\frac\eps h}  \right)  \right) \|  p_0  \|_{H^3(\Omega)} , \qquad C>0.
$$
Since
$
\|\V_h^{\eps}-\mathbf W_h^\eps\|_{L^2([0,T] \times \Omega))} =\sqrt{\int_0^TE(t)dt}
$, 
the proof is ended after a last simplification by $\varepsilon^2$.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Space estimate for  uniform AP property in 2D}

We have the following
result of uniform convergence where the mesh satisfies the geometrical assumptions
(\ref{geometrie}).

\begin{theorem} [Space estimate]\label{theor:main}
  There exists a constant
C>0 independent of $h$ and $\eps$, such that the following estimate holds
\begin{equation*}\label{est2}
\|\V^{\eps}-\V^{\eps}_h\|_{L^2([0,T]\times\Omega)}\leq
C h^{\frac{1}{4}} \|    p_0  \| _{H^4(\Omega)}.
\end{equation*}
\end{theorem}

\begin{proof}
The proof is a slight adaptation of our initial proposition
\ref{prop:1}, where we use the norm
$\|\cdot\|=\|\cdot\|_{L^2([0,T]\times\Omega)   ) }$.
From the triangular inequality applied to the AP diagram,
one has 
$$
\|\V_h^\eps-\V^\eps\|\leq
\min \left(
\|\V_h^\eps-\V^\eps\|_{\rm naive},
\|\V_h^\eps-\mathbf{W}^\eps_h\|
+
\|\mathbf{W}^\eps_h-\W^\eps\|
+
\|\W^\eps-\V^\eps\|
\right).
$$
All these norms are estimated with 
(\ref{eq:veps2}), (\ref{eq:ww1}), (\ref{eq:enc1}) and (\ref{eq:enc22}).
Therefore one can write 
$$
\|\V_h^\eps-\V^\eps\|\leq
C\min \left(
\sqrt{\frac h \eps  } \; \; , \; \; 
\left( h^2+
\eps \max\left(1, \sqrt{\frac \eps h } \right) \right)+
 \left(h +\eps
\right) +\eps
\right) \|    p_0  \| _{H^4(\Omega)}, \quad C>0.
$$
The parenthesis is
$$
\mathcal Z=
\min \left(
\sqrt{\frac h \eps  } \; \; , \; \; 
\left( h^2+
\eps \max\left(1, \sqrt{\frac \eps h } \right) \right)+
 \left(h +\eps
\right) +\eps
\right)
$$
$$
\leq \min \left(
\sqrt{\frac h \eps  } \; \; , \; \; 
%\left( 
\eps \max\left(1, \sqrt{\frac \eps h } \right) %\right)
+
 2 h +2\eps
\right)
%$$
%$$
\leq \min \left(
\sqrt{\frac h \eps  } \; \; , \; \; 
%\left(
3 
\eps \max\left(1, \sqrt{\frac \eps h } \right) %\right)
+
 2 h %+2\eps
\right).
$$
%$$
%\leq  C \min \left(
%\sqrt{\frac h \eps  } \; \; , \; \;
%\eps 
%\right) \|    p_0  \| _{H^4(\Omega)} +
%C\min \left(
%\sqrt{\frac h \eps  } \; \; , \; \;
%\sqrt{\frac {\eps^3} h }
%\right) \|    p_0  \| _{H^4(\Omega)}
%$$
%$$
%\leq
%C \min \left(
%\eps^{-\frac12   } h^{\frac12  }\; \; , \; \;
%\eps 
%\right) \|    p_0  \| _{H^5(\Omega)}+Ch^{- \frac12  }
% \min \left(
%\eps^{-\frac12   } h \; \; , \; \;
%\eps^{\frac 32} 
%\right) \|    p_0  \| _{H^5(\Omega)}.
%$$
%The first contribution is estimated with
%$a=1$ and $b=c=\frac12$ as in 1D and as in the inequality (\ref{eq:toutdebut}) of 
As in proposition \ref{prop:1}, a threshold value is obtained by equating
the more singular terms, that is 
$
\sqrt{\frac {h }{\eps_{\rm thresh}}  }=\eps_{\rm thresh} \sqrt{
 \frac{\eps_{\rm thresh}}{h}}$, with solution
$\eps_{\rm thresh}= \sqrt h$. 
Two case occur.

First case is $\varepsilon \geq \eps_{\rm thresh}$. Then the  first term in $\mathcal Z$ shows that
$
\mathcal Z\leq \sqrt{\frac {h }{\eps_{\rm thresh}}  } = h^{\frac14}$.
Second case is $\varepsilon \leq \eps_{\rm thresh}$. Then the  second
 term in $\mathcal Z$ shows that
 $
 \mathcal Z\leq 3 \eps_{\rm thresh}\sqrt{\frac {\eps_{\rm thresh}}{h }  } + 2 h = 3 h^{\frac14}+ 2h \leq 
 5 h^\frac14$. 
 In both case $\mathcal Z\leq C h^{\frac14}$. The proof is ended.
%
%it yields a contribution $O(h^ \frac13 )$.
%The second contribution can also be estimated in the context of  proposition \ref{prop:1}.
%One gets  a convergence rate  
%$O \left( \frac{ac}{a+b}-\frac12 \right)$ where now $a=\frac32$, $b=\frac12$ and $c=1$:
%that is $O(h^ \frac14 )$.
%The worst case is 
%$O(h^ \frac14 )$
%which ends the proof.
\end{proof}







\section{Implicit discretization  and proof of theorem \ref{theor:fullconv}} \label{end:theor}


We explain hereafter how to compare the implicit scheme and the semi-discrete scheme, in  a way
that produces immediately abstract error bounds. This technique comes from
\cite{de04} where applications to the numerical analysis of 
 explicit schemes was the main goal. In what follows we concentrate on
implicit Euler discretization for two reasons. First reason is that the theory is a little  is simpler to explain than for
the explicit scheme, 
for which the interested reader may nevertheless refer to the cited work. The  very simple proof that is provided 
is 
new. Second reason is that
implicit discretization  is somehow necessary  to take into the account the intrinsic stiffness of the problem.
In particular the numerical tests have been performed with the implicit method. With the explicit method 
the CFL condition is so restrictive that it makes impossible the convergence study.
The proof is a consequence of the abstract estimate (\ref{eq:impimp6}) with the technical estimate
(\ref{eq:nhnhnh1})
for the initial data.

\subsection{An abstract estimate}

The idea is to compare the solution $U_{h}(t)$ of a semi-discrete scheme 
 \begin{equation} \label{eq:impimp1}
 U_{h}(t)=A_h U_{h}(t), \quad U_{h}(0)=U_h^{\rm ini}
 \end{equation}
 with the solution of the corresponding  implicit Euler scheme
 with   time step  $\Delta t$
  \begin{equation} \label{eq:impimp2}
\frac{  U_{h}^{n+1} -U_{h}^n}{\Delta t}=A_h U_{h}^{n+1}, \quad U_{h}^0=U_h^{\rm ini}
 \end{equation}
 The operator depends an abstract  parameter $h$:  with symbolic notation, this abstract 
 parameter is  $h\leftarrow (h,\eps)$
 in the case
 of  %The initial condition depends only on $h$. 
 %This situation corresponds to 
 our problem $\mathbf P_h^\eps$. The question is to bound the difference of these
 two uniformly with respect to $\Delta t$ uniformly with respect to the abstract parameter $h$.

 We  assume a natural $L^2$ norm denoted as $\|  \cdot \|$ with the associated scalar product.
 For simplicity we also assume that $A_h$ is dissipative in the sense that 
 $$
 (U_h, A_h U_h)\leq 0 \quad \mbox{ for all }
 U_h \mbox { in the appropriate discrete space.}
 $$ 
 Taking the scalar product of (\ref{eq:impimp2}) with $U_h^{n+1}$,  one deduces that 
 $\|   U_{h}^{n+1}\| \leq \| U_{h}^n\|$ for all $U_h^n$.
Assuming the discrete space in finite (this is always true for discrete methods in a compact domain),
one gets the unconditional stability estimate
\begin{equation} \label{eq:impimp5}
\|  (I_h-\Delta t A_h)^{-1}\| \leq 1 \qquad \forall \Delta t>0
\end{equation}
where $I_h$ is the discrete  identity operator and the norm is the induced one for operators.
Note that (\ref{eq:impimp5})  ultimately shows that the matrix $I_h-\Delta t A_h$ is non singular.
So the matrix of the problem can be assemble and invert on a computer.

Let us define for convenience  $V_{h}^n=U_{h}(n\Delta t)$ so that the semi-discrete scheme can  be rewritten as
$$
\frac{  V_{h}^{n+1} -V_{h}^n}{\Delta t}-
A_h V_{h}^{n+1}=
\frac1{\Delta t}\int_{n\Delta t}^{(n+1)\Delta t}  U_{h}'(s) ds - A_h U_{h}((n+1)\Delta t)
$$
$$
= \frac1{\Delta t}\int_{n\Delta t}^{(n+1) \Delta t}A_h U_{h}(s) ds - A_h U_{h}((n+1)\Delta t)
=\Delta t A_h  s_{h}^{n+1}
$$
where the residual %$s_h^{n+1}$
 is 
$
 s_{h}^{n+1}=
\frac1{\Delta t}\int_{n\Delta t}^{(n+1)\Delta t}  \frac{ U_{h}(s)  -  U_{h}((n+1)\Delta t)}{\Delta t} ds
$. 
We notice that
\begin{equation} \label{eq:impimp11}
\| s_{h}^n \|\leq 
\sup_{0\leq s \leq T}\| U_{h}'(s) \| \leq 
\| A_h U_h^{\rm ini}\|, \quad n\Delta t \leq T.
\end{equation}
Therefore this special residual is uniformly bounded provided $\| A_h U_h^{\rm ini}\|$.
This is actually true: it comes the fact that $W_{h}(t)=U_{h}'(t)$ is solution of 
$
W_{h}'(t)=A_h W_h(t)$ and  $ W_{h}(0)=A_h U_h^{\rm ini}$. 
So the strong $L^2$ stability of the semi-discrete scheme due to (\ref{eq:impimp3}) yields the  bound
(\ref{eq:impimp11}).




\begin{proposition} [Time estimate]
%Assume $\| A_h U_h^{\rm ini} \|$ is bounded uniformly  with respect to $h$.
%Assume $(U_h, A_h U_h)\leq 0$ for all $U_h$.
 Let $T>0$ be a final time.
Then there exists a constant $C$ independent of $h$, $\eps$  and $\Delta t$ such that 
\begin{equation} \label{eq:impimp6}
\| U_{h}^n -U_{h}(n\Delta t)  \| \leq C \sqrt {\Delta t} \| A_h U_h^{\rm ini}  \|  , \quad n\Delta t \leq T.
\end{equation}
\end{proposition}

\begin{proof}
 The initial data is the same $V_{h}^0=U_h^{\rm ini}$. Let us define the error $E_{h}^n=V_{h}^n-
 U_{h}^n$ which is solution
of
\begin{equation} \label{eq:impimp3}
\frac{  E_h^{n+1} -E_h^n}{\Delta t}=A_h E_h^{n+1}+ \Delta t A_h  s_h^{n+1}, \quad E_h^0=0.
\end{equation}
It yields $(I_h -\Delta tA_h ) E_h^{n+1}= E_h^n + \Delta t^2 A_h  s_h^{n+1}$, that is 
$
E_h^{n+1}= (I_h -\Delta tA_h  )^{-1} E_h^n +\Delta t (I_h -\Delta tA_h  )^{-1} \Delta t A_h  s_h^{n+1}
$.
We obtain the  representation  formula (discrete Duhamel's formula)
\begin{equation} \label{eq:impimp23}
E_h^n= \Delta t \sum_{p=0}^{n-1} \left[  (I_h -\Delta tA_h  )^{-1}   \right]^{n-1-p}
 (I_h -\Delta tA_h  )^{-1} \Delta t A_h  s_h^{p+1}.
\end{equation}
Let us define
the operator $T_h= (I_h -\Delta tA_h  )^{-1}$ which is  bounded $\| T_h \|\leq 1$.
One has the formula
$T_h-I_h=  (I_h -\Delta tA_h  )^{-1} \Delta t A_h $
     and the formula
%\begin{equation} \label{eq:impimp21}
$\left(I_h -\frac{\Delta t}2 A_h  \right)^{-1} \frac{I_h+T_h}{2}= \left(I_h -{\Delta t} A_h  \right)^{-1}$.
%\end{equation}
Plug\-ging in the discrete Dumael's formula, one obtains
another representation
\begin{equation} \label{eq:impimp24}
E_h^n= \Delta t \sum_{p=0}^{n-1} \left[  (I_h -\frac{\Delta t}2A_h  )^{-(n-1-p)}   \right]
 \left[ \left(\frac{I_h+T_h}{2}    \right)^{n-1-p} (T_h -I_h)  \right] s_h^{p+1}.
\end{equation}
The first operator in brackets is bounded by 1 due to the stability (\ref{eq:impimp5}).
On the other hand it is an easy exercise in  number theory to show
that for $q\geq 0$
$$
%\left\|
 \left(\frac{I_h+T_h}{2}    \right)^{q} (T_h -I_h) 
% \right\| 
=
\frac1{2^q} 
%\left\|
 \sum_r    \left(   
\left(
\begin{array}{c}
q \\
r-1
\end{array}
 \right)
 -
 \left(
\begin{array}{c}
q \\
r
\end{array}
 \right)
 \right) T_h^r 
 %\right\| 
$$
where the binomial coefficients are $ \left(
\begin{array}{c}
q \\
r
\end{array}
 \right)= \frac{q!}{r!(q-r)!}$ for $0\leq r \leq q$, otherwise zero.
Therefore 
$$
\left\|
 \left(\frac{I_h+T_h}{2}    \right)^{q} (T_h -I_h) 
 \right\| \leq
 \frac1{2^q} 
%\left\|
 \sum_r    \left|   
\left(
\begin{array}{c}
q \\
r-1
\end{array}
 \right)
 -
 \left(
\begin{array}{c}
q \\
r
\end{array}
 \right)
 \right| \leq \frac1{2^q} 2  \left(
\begin{array}{c}
q \\
r_*
\end{array}
 \right)
$$
where the last inequality is from a telescoping reasoning and $r_*$ is one of the  closest entire number
to $q/2$, that is 
$| \frac q 2-r_*| \leq 1$. But there exists a universal constant, let it call it $K$, such that 
$ \frac1{2{q-1}}   \left(
\begin{array}{c}
q \\
r_*
\end{array}
 \right) \leq \frac K{\sqrt {q+1}}$. Therefore 
 $
 \left\|
 \left(\frac{I_h+T_h}{2}    \right)^{q} (T_h -I_h) 
 \right\| \leq
K{\sqrt {q+1}}$. 
Using this universal estimate in (\ref{eq:impimp24}) and the estimate on $s_h^n$, we obtain
$
\| E_h^n\| \leq \Delta t \sum_{p=0}^{n-1}  \frac K{\sqrt {n-1-p+1}} \| A_h U_h^{\rm ini}  \|
=
 \Delta t \sum_{p=1}^{n}  \frac K{\sqrt {p}} \| A_h U_h^{\rm ini}  \| $. 
A basic bound shows that $\sum_{p=1}^{n}  \frac 1{\sqrt {p}} \leq \widetilde K \sqrt{n}$.
Therefore
$$
\| E_h^n\| \leq  \Delta t  K \widetilde K \sqrt n \| A_h U_h^{\rm ini}  \|\leq 
( K \widetilde K \sqrt T) \sqrt{\Delta t } \| A_h U_h^{\rm ini}  \|, \quad n\Delta t\leq T.
$$
The proof is ended.
\end{proof}

To finish the proof of the theorem \ref{theor:fullconv}, it is now necessary and sufficient
to show that $\| \frac d{dt} U_h(0) \|= \| A_h U_h^{\rm ini}  \|$
is bounded independently of $h$ for the initial data of $\mathbf P_h^\eps$.
This is the purpose of the next section.



\subsection{Technical estimates}

To prove the uniform on the initial data, we will use in a slightly different manner
the estimates for the initial data that have been obtained for the diffusion approximation
scheme $\mathbf{DA}_h^\eps$.
However we will need an additional assumption of the mesh 
 \begin{equation}\label{geom4.r}
(A_r\uu, \uu)\geq \alpha h (\uu,\uu), \quad A_r=\sum_j\alj.
\end{equation}
This assumption is not restrictive so we do not comment on it.
The following  technical estimates show two things.
First in explains in what sense the corner velocity  is a good approximation
of the gradient at the corner at initial stage.
Second it provides in (\ref{eq:nhnhnh1}) a control of the time derivative at time $t=0$
 uniformly 
with respect to $h$ and $\eps$, 
it 
immediately shows the boudedness of the abstract quantity $A_h U_h^{\rm ini}$ in (\ref{eq:impimp11}).
So it is possible to apply the above proposition and the main theorem is proved.
We now turn to the proof the technical estimates.

\begin{proposition}
There exists a constant $C$ independent of $h$ and $\varepsilon$ such that the initial data
of $\mathbf P_h^\eps$ satisfies
\begin{equation} \label{eq:urur}
\left\|  \mathbf u_r^\eps(0) + \frac\varepsilon \sigma \nabla p_0(\mathbf x_r) \right\|_{L^2(\Omega)}
\leq Ch \varepsilon \|  p_0 \|_{H^3(\Omega)}.
\end{equation}
\end{proposition}

\begin{proof}
The corner problem (\ref{eq:5bis})
that defines $\mathbf u_r=\mathbf u_r^\eps(0)$ at initial time is rewritten as
%\begin{equation} \label{eq:5bis}
$$
\displaystyle \left( A_r+ 
\frac{\sigma}{\varepsilon} B_r\right)\ur= \sum_{j}\ljr
p_0(\mathbf x_j)\njr-\frac\eps\sigma \sum_j\alj \nabla p_0(\mathbf x_j).
$$%\end{equation}
Let us defined 
$
\mathbf d_{r}^1= \sum_{j}\ljr  \left(
 p_0(\mathbf x_j)-p_0(\mathbf x_r) -  \left( \mathbf x_j - \mathbf x_r, \nabla p_0(\mathbf x_r)   \right) \right)
 \njr
$,
already defined and bounded  in (\ref{eq:ddd1}).
So elimination of $p(\mathbf x_j)$ and simplification with $\sum_j \ljr \njr p(\mathbf x_r)=0$ yield
%  one can write
$$
\displaystyle \left( A_r+ 
\frac{\sigma}{\varepsilon}B_r\right)\ur= \sum_{j}\ljr
\left( \mathbf x_r - \mathbf x_j, \nabla p_0(\mathbf x_r) \right)\njr+ \mathbf d_r^1
$$
$$
-\frac\eps\sigma \sum_j\alj \nabla p_0(\mathbf x_r)  +  \frac\eps\sigma \sum_j\alj \left( \nabla p_0(\mathbf x_r) 
-
 \nabla p_0(\mathbf x_j) \right) .
$$
that is with the definition of the matrices
$$
\displaystyle \left( A_r+ 
\frac{\sigma}{\varepsilon}B_r\right)\left(   \ur  +\frac\eps \sigma \nabla p_0(\mathbf x_r) \right)
=\mathbf d_r^1+  \frac\eps\sigma \sum_j\alj \left( \nabla p_0(\mathbf x_r) 
-
 \nabla p_0(\mathbf x_j) \right) .
$$
The coercivity (\ref{geom4.r}) of the matrices $A_r$ and $B_r$ yields  
$$
\alpha \left(h +\frac{\sigma h^2}\eps \right)\left|    \ur  +\frac\eps \sigma \nabla p_0(\mathbf x_r)   \right| 
\leq | \mathbf d_r^1 |+
\frac\eps\sigma \sum_j\alj \left| \nabla p_0(\mathbf x_r) 
-
 \nabla p_0(\mathbf x_j) \right|.
$$
 With estimate of $\mathbf d_r^1$  (\ref{eq:ddd1}), estimate of the difference $\nabla p_0(\mathbf x_r) 
-
 \nabla p_0(\mathbf x_j) $,   it yields
 $$
 \alpha \left(h +\frac{\sigma h^2}\eps \right)\left|    \ur  +\frac\eps \sigma \nabla p_0(\mathbf x_r)   \right| 
\leq
C ( h^2  + \frac\eps \sigma h)  \|  p  \|_{\Omega_j}, 
 $$
 with a constant uniform with respect to $h$, $\eps$ and the index of the cell $j$.
 That is
 $$
 \left|    \ur  +\frac\eps \sigma \nabla p_0(\mathbf x_r)   \right|  \leq \frac C\alpha \eps  \|  p  \|_{\Omega_j}.
 $$
 After squaring and summation with respect to $j$, it yields the result.
\end{proof}


\begin{proposition}
There exists a constant $C>0$ which do not depend on $h$ and $\eps$ such that
the initial data of $\mathbf P_h^\eps$ satisfies
\begin{equation} \label{eq:nhnhnh1}
\left\|  \frac d{dt} \mathbf V_h^\eps \right\|_{L^2(\Omega))} 
%=
%\|  A_h  \|_{L^2(\Omega)}
 \leq C \|   p_0   \|_{H^3(\Omega)}.
\end{equation}
\end{proposition}
\begin{proof}
The $\mathbf P_h^\eps$ scheme (\ref{eq:4}) or (\ref{eq:4bis}) can be rewritten 
\begin{equation} \label{eq:nh1}
\mathbf P_h^\eps:
\qquad
\left\{ 
\begin{array}{ll}
\displaystyle
\frac{d}{dt} p_{j}^{\eps}&\displaystyle
 =  -\frac{1}{ \varepsilon |\Omega_{j} | }
\sum_{r}l_{jr}(\ur^{\eps}- \uj^\eps,\njr) \\
%& \displaystyle
%=
%-\frac{1}{ \varepsilon |\Omega_{j} | }
%\sum_{r}l_{jr}(\ur^{\eps} + \frac \eps \sigma \nabla p_0(\mathbf x_r)  ,\njr)
%+
%\frac{1}{ \sigma |\Omega_{j} | }
%\sum_{r}l_{jr}(   \nabla p_0(\mathbf x_r)- \nabla p_0(\mathbf x_j)   ,\njr)
%\\
\displaystyle 
\frac{d}{dt}\uj^{\eps}& = - \frac{1}{\varepsilon  |
\Omega_{j}| }
\sum_{r}l_{jr} (\njr,\uu_r^{\eps} -\uu_j^{\eps})\njr.
\end{array}
\right.
\end{equation}
At time $t=0$ one has 
$
\ur^{\eps}- \uj^\eps= \left( \ur^{\eps} + \frac \eps \sigma \nabla p_0(\mathbf x_r) \right)
+ \frac\eps \sigma \left(   \nabla p_0(\mathbf x_j)- \nabla p_0(\mathbf x_r) \right)$:
the first term can be estimated by (\ref{eq:urur}) and the second one as usual.
Therefore there exists  constants such that
$$
\left\| \frac{1}{ \varepsilon |\Omega_{j} | }
\sum_{r}l_{jr}(\ur^{\eps} + \frac \eps \sigma \nabla p_0(\mathbf x_r)  ,\njr)  \right\|_{L^2(\Omega)}
\leq C \frac{h}{C_\mathcal M h^2}P h\eps \| p_0 \|_{H^3(\Omega)}\leq
\widehat C\| p_0 \|_{H^3(\Omega)}.
$$
In a similar way
$$
\frac{1}{ \sigma |\Omega_{j} | }
\left| 
\sum_{r}l_{jr}(   \nabla p_0(\mathbf x_r)- \nabla p_0(\mathbf x_j)   ,\njr)  \right|
\leq \frac{h}{\sigma C_\mathcal M h^2} \widetilde C \| p_0 \|_{H^3(\Omega_j)}\leq
\frac{ \overline C} h  \| p_0 \|_{H^3(\Omega_j)}.
$$
Therefore 
$$
\left\| \frac{1}{ \sigma |\Omega_{j} | }
\sum_{r}l_{jr}(   \nabla p_0(\mathbf x_r)- \nabla p_0(\mathbf x_j)   ,\njr)  \right\|_{L^2(\Omega)}
\leq { \overline C}  \| p_0 \|_{H^3(\Omega)}.
$$
It shows that $\left\|  \frac d{dt}p^\eps(0)  \right\|_{L^2(\Omega)} \leq C \| p_0 \|_{H^3(\Omega_j)}$.
Considering (\ref{eq:nh1}) , a similar result for $ \frac d{dt}\mathbf u^\eps(0)$.
It shows
$
\left\|  \frac d{dt} \mathbf V_h^\eps(0) \right\|_{L^2(\Omega))}  \leq C \|   p_0   \|_{H^3(\Omega)}$.
%Since the function $\frac d{dt} \mathbf V_h^\eps$ is also solution of the system
%(\ref{eq:nh1}) and the scheme is $L^2$ stable, this $L^2$ bound is propagated.
The proof is ended.  %proves the result.
\end{proof}



\section{Numerical illustration}\label{sec::num}

To illustrate the theory and have a more quantitative version of the error estimates
studied in this work, we consider the academic square $\Omega=[0,1]^2$ and discretize the hyperbolic heat equation
of a mesh made with random quads. 
A random quad mesh is made of quads where the vertices are moved
randomly around their initial position, by a factor between 10\% and 30\%.
We use the fully implicit time discretization version of the 2D scheme
detailed in this work. The solution of the linear systems is computed via
an iterative GMRES algorithm, which converges smoothly in our numerical experiments.
The reference analytical solution used in our tests is designed by separation of variables.
A solution of (\ref{eq:mod1}) is
$$
p= f+\frac{\eps^2}\sigma \partial_t f
\mbox{ and }
\mathbf u= -\frac\eps \sigma \nabla f,
$$
with $f$ solution of 
\begin{equation}\label{telegraph}
\partial_t f +\frac{\eps^2}\sigma \partial_t^2 f -\frac1\sigma \Delta f=0.
\end{equation}
 We propose to construct a solution for a subset of small $\eps$ to validate the uniform convergence. Firstly we consider that the solution is a periodic solution on the square $\left[0,2\right]\times\left[0,\frac{2}{L}\right]$. For this we use the separation of the variables. We consider the following function
$$
f(t,x,y)=\alpha(t)\cos (L \pi x ))\cos( L\pi y ).
$$
and we propose to find the function $\alpha(t)$ such that $f(t,x,y)=\alpha(t)\cos (L\pi x )\cos( L\pi y )$ is a periodic solution of (\ref{telegraph}).
The function $\alpha$ is determined as the solution of
$$
\alpha'(t)+ \frac{\eps^2}\sigma\alpha''(t)+\frac{2L^2\pi^2}\sigma \alpha(t)=0
$$
with $\alpha'(0)=0$ and $\alpha(0)=1$.
For small $\eps$, which is the case we are interested in, the solution is computed as follows.
First determine 
$$
\lambda_1=-
\frac{
\sigma \left( \sqrt{1-\frac{\eps^2}{\sigma^2}  8L^2 \pi^2  } +1\right)
}{2 \eps^2  }
\mbox{ and }
\lambda_2=
\frac{
\sigma \left( \sqrt{1-\frac{\eps^2}{\sigma^2}  8L^2 \pi^2  } -1\right)
}{2 \eps^2  }.
$$
Then 
$$
\alpha(t)=\frac{\lambda_2}{\lambda_2-\lambda_1}e^{\lambda_1 t  }
-
\frac{\lambda_1}{\lambda_2-\lambda_1}e^{\lambda_2 t  }
$$
from which $p(t)$ and $\mathbf u(t)$ are easily recovered.

We decide that an exact relation is enforced between
$\eps$ and $h=\frac1N$, so that the error can be expressed as a function of $h$ solely.
The relation between $\eps$ and $h$ writes
$\eps=0.01 (40h)^\tau$ for $\tau \in\{0,\frac14, \frac12, 1, 2  \}$.
The error between the exact solution and the numerical solution is computed numerically
in function of $h=\frac1N$, for different values of $\tau$, and the results of some of these numerical experiments is
displayed in figure \ref{fig1}. The results correspond  to the  time $T = 0.02$ using the time step 
$\Delta t = 0.2 h^2$.




\begin{figure}
\begin{center}
\includegraphics[width=9cm,angle=-90]{sortie_enfin.eps}
\end{center}
\caption{The error is plotted in log scale versus the number of cells per direction for the test problem described
in section \ref{sec::num}.
Each curve corresponds to a value of $\tau\in\{0,\frac14, \frac12, 1, 2  \}$, plus 
a reference line for order one. One sees that the order of convergence is an increasing function
of $\tau$.}
\label{fig1}
\end{figure}

As predicted by  the theory, the scheme is uniformly AP and the error behavior is a continuous function of $\gamma$
between the  hyperbolic and parabolic limits. However the results are much better, in the sense
the order is greater than the theoretical prediction since
the order is approximatively 1 for $\gamma=0$ (hyperbolic limit)
and 2 for $\gamma=2$ (parabolic regime).  We can find a closed result on the second order convergence for the parabolic regime in the paper \cite{TAHO} (1D linear problem).
The reason is probably that the theory is based on worst case estimates, as it is often the case for the numerical analysis
of finite volume schemes \cite{FV}. Numerically we observe a time-linear rate for the numerical which shows that the time-exponential rate obtained using the Gronwall lemma is clearly not optimal.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Conclusion}


The proof that was given of the uniform AP property is quite technical.
It relies on specific hyperbolic and parabolic estimates for linear
nodal finite volume schemes on general meshes.
We observe that the multidimensional case yields an additional contribution in the error
that ultimately slightly degrades the convergence rate.
It is an open problem to determine if these inequalities are optimal.
The numerical results indicate that it is probably not the case.


\appendix

\section{Detailed proof of the naive estimate (\ref{eq:veps2})}

Our aim is to now examine each term in the right hand side of the dissipative identity
(\ref{eq:ee1}). Its  first term is already non positive.
%We  look at the second line of (\ref{eq:ee1})  which we call
%$E_1$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{pro}
Let $\gamma>0$ be a number which precise value will be determined further.
There exists a constant $C_1(\gamma)$ which depends on $\gamma$ such that 
one has
the bound for the second term of the dissipative identity (\ref{eq:ee1})
\begin{equation} \label{eq:bound1}
\int_0^T E_1(t)dt \leq 
\frac{\gamma}{\eps}\int_0^T\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 +
C_1(\gamma) \frac h { \varepsilon \sqrt{C_\mathcal M} }  \| \mathbf{V}^{\eps}(0) \|_{H^1(\Omega)}^2.
\end{equation}
%$$+ T \frac {P C_{\mathcal A}^2  h }{2\gamma \eps} \vert \vert \mathbf{V}^{\eps}(0) \vert\vert^2_{H^1(\Omega)}  +  \bigg( \frac1{\alpha} + PC_{\mathcal A}^2 \bigg) \frac{ h }{2}\sqrt{\frac{P}{C_\mathcal M}}  \| \mathbf{V}^{\eps}(0) \|_{H^1(\Omega)}^2.
%$$ %\end{equation}
%where the constants $P$ and $C_{\delta}$ are defined respectively in \eqref{nbmr} and  (\ref{const_delta1}-\ref{const_delta2}).
\end{pro}
\begin{proof}
We use a Young's inequality 
$ab\leq \frac\gamma2 a^2+\frac1{2\gamma} b^2$,with some positive constant$\gamma$ which will be defined later,   for the second term
and the definition of the fluxes (\ref{eq:5}) for the third term: we get
\begin{equation*}
\begin{aligned}
E_1 &\leq  \frac{\gamma }{2 \eps}\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}
-\textbf{u}_r^{\eps})^2 + \frac{h}{2\gamma\eps} \sum_j \sum_r  %l_{j,r}
\delta_{j,r}(p^{\eps})^2 \\
&+
%\frac{1}{\eps} \sum_j \sum_r \bigg(
%\widehat{\alpha}_{j,r}(\textbf{u}_j^{\eps}-\textbf{u}_r^{\eps}) \: , \:
%\delta_{j,r}(\uu^{\eps}) \bigg)
\sum_j \sum_r 
\ljr(\njr , \textbf{u}_j^{\eps}-\textbf{u}_r^{\eps} )(\njr ,
  \delta_{j,r}(\uu^{\eps}))
 -\frac{1}{\eps} \sum_j \sum_r \bigg(
\frac{\sigma}{\eps} \widehat{\beta}_{j,r}\textbf{u}_r^{\eps}\: , \:
\delta_{j,r}(\uu^{\eps} )\bigg) \\
\end{aligned}
\end{equation*}
% By definition of $\widehat{\alpha}_{j,r}$,
%one rewrites
% $$
%\sum_j \sum_r \bigg(
%\widehat{\alpha}_{j,r}(\textbf{u}_j^{\eps}-\textbf{u}_r^{\eps}) \: , \:
%\delta_{j,r}(\uu^{\eps}) \bigg) = \sum_j \sum_r 
%\ljr(\njr , \textbf{u}_j^{\eps}-\textbf{u}_r^{\eps} )(\njr ,
%  \delta_{j,r}(\uu^{\eps})).
%$$ 
Another use of Young's inequality with the same coefficient $\gamma$ for the third term
yields
\begin{equation*}
\begin{aligned}
E_1 &\leq  \frac{\gamma }{ \eps}\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}
-\textbf{u}_r^{\eps})^2 + \frac{h}{2\gamma\eps} \sum_j \sum_r %l_{j,r}
\delta_{j,r}(p^{\eps})^2 
\\
&
+\frac{1}{2\gamma\eps} \sum_j \sum_r l_{jr}
|
\delta_{j,r}(\uu^{\eps})|^2 -\frac{1}{\eps} \sum_j \sum_r \bigg(
\frac{\sigma}{\eps} \widehat{\beta}_{j,r}\textbf{u}_r^{\eps}\: , \:
\delta_{j,r}(\uu^{\eps} )\bigg) \\
&\leq  \frac{\gamma }{ \eps}\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}
-\textbf{u}_r^{\eps})^2 + \frac{h}{2\gamma\eps} \sum_j \sum_r %l_{j,r}
\delta_{j,r}(p^{\eps})^2 
\\
&
+\frac{h}{2\gamma\eps} \sum_j \sum_r 
|
\delta_{j,r}(\uu^{\eps})|^2 -\frac{1}{\eps} \sum_j \sum_r \bigg(
\frac{\sigma}{\eps} \widehat{\beta}_{j,r}\textbf{u}_r^{\eps}\: , \:
\delta_{j,r}(\uu^{\eps} )\bigg) .
\end{aligned}
\end{equation*}
We now look at the last term of this inequality $W=-\frac{1}{\eps} \sum_j \sum_r \bigg( \frac{\sigma}{\eps}
\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}\: , \: \delta_{j,r}(\uu^{\eps} )\bigg)$.
By definition (\ref{eq:6}) of $\widehat{\beta}_{j,r}$, one has $\left|   \widehat{\beta}_{j,r} \right|\leq h^2$. Therefore
%\begin{equation*}
%W=-\frac{1}{\eps} \sum_j \sum_r \bigg( \frac{\sigma}{\eps}
%\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}\: , \: \delta_{j,r}(\uu^{\eps} )\bigg)
%= -\frac{\sigma}{\eps^2} \sum_j \sum_r
%\bigg((\ljr)^{\frac12 }\njr\otimes(\xr -\xj)
%\textbf{u}_r^{\eps}\: , \: (\ljr)^{\frac12 }\delta_{j,r}(\uu^{\eps})
%\bigg)
%\end{equation*}
% Using the Cauchy-Schwarz inequality, we get
$$
\vert W \vert \leq \frac{\sigma h^2}{\eps^2} \bigg(\sum_j \sum_r
%\ljr 
 \bigg| 
%\njr\otimes(\xr -\xj)
\textbf{u}_r^{\eps}
 \bigg|^2 \bigg)^{\frac12 } \bigg( \sum_j   \sum_r
 %\ljr
   | \delta_{j,r}(\uu^{\eps})| ^2 \bigg)^{\frac12 } 
   $$
   $$
\leq  \frac{\sigma  h^2}{\eps^2} \sqrt P \bigg( \sum_r
%\ljr 
 \bigg| 
%\njr\otimes(\xr -\xj)
\textbf{u}_r^{\eps}
 \bigg|^2 \bigg)^{\frac12 } \bigg(\sum_j \sum_r
 %\ljr
   | \delta_{j,r}(\uu^{\eps})| ^2 \bigg)^{\frac12 } 
\leq  \frac{\sigma  h}{\eps^2} \sqrt \frac P{C_\mathcal M} \bigg( \sum_r
%\ljr 
|V_r |
 \bigg| 
%\njr\otimes(\xr -\xj)
\textbf{u}_r^{\eps}
 \bigg|^2 \bigg)^{\frac12 } \bigg(\sum_j \sum_r
 %\ljr
   | \delta_{j,r}(\uu^{\eps})| ^2 \bigg)^{\frac12 } 
   $$
   $$
   \leq
    \frac{\sigma h  }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}}
\bigg( \sum_r |V_r| \big( \textbf{u}_r^{\eps},\textbf{u}_r^{\eps}\big)  + \sum_j \sum_r    | \delta_{j,r}(\uu^{\eps})|^2 \bigg).
$$
%Since $| \njr\otimes(\xr -\xj) \textbf{u}_r^{\eps} |^2 = | (\xr -\xj) ,  \textbf{u}_r^{\eps} |^2$, one obtains by using %successively assumptions 
% %\eqref{nbmr} and 
%% \eqref{eq:d5.2} 
%%\begin{equation*}\label{esti_beta}
%%\textcolor{red}{\vert W \vert
%%\leq  \frac{\sigma h  }{\eps^2}\sqrt{\frac{P}{C_\mathcal M}}
%%\bigg( \sum_r |V_r| \big( \textbf{u}_r^{\eps},\textbf{u}_r^{\eps}\big)\bigg)^{\frac12} \bigg(\sum_j \sum_r    | \delta_{j,r}(\uu^{\eps})|^2 \bigg)^{\frac12 },}
%%\end{equation*}
%%and therefore
%\begin{equation}\label{eq:bound2}
%\vert W \vert
%\leq \frac{\sigma h  }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}}
%\bigg( \sum_r |V_r| \big( \textbf{u}_r^{\eps},\textbf{u}_r^{\eps}\big)  + \sum_j \sum_r    | \delta_{j,r}(\uu^{\eps})|^2 \bigg).
%\end{equation}
It yields
\begin{equation} \label{eq:bound1bis}
\begin{aligned}
E_1 &\leq \frac{\gamma }{\eps}
\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 + \frac{h}{2\gamma\eps} \sum_j \sum_r %l_{j,r}
\delta_{j,r}(p^{\eps})^2  \\
& 
+\left( \frac{h}{2 \gamma \eps} + \frac{\sigma h }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}}  \right) \sum_j \sum_r
 %\ljr
   | \delta_{j,r}(\uu^{\eps})| ^2 % \\
%& 
+ \frac{\sigma h }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}} 
%\bigg(
 \sum_r |V_r|  \big( \textbf{u}_r^{\eps},\textbf{u}_r^{\eps}\big)  .
%+\sum_j \sum_r    | \delta_{j,r}(\uu^{\eps})|^2 \bigg)\\
\end{aligned}
\end{equation}
Using the first   interpolation result of proposition \ref{pro:interp} and  the assumption  \eqref{nbmr}, one has 
\[
\sum_j \sum_r  \delta_{j,r} ( p^{\eps})^2 \leq P C_{\mathcal A}^2   \vert \vert p^\eps \vert\vert^2_{H^2(\Omega)}
\mbox{ and }
\sum_j \sum_r | \delta_{j,r}\uu^{\eps}|^2 \leq P C_{\mathcal A}^2 \vert \vert \uu^\eps \vert\vert^2_{H^2(\Omega)}
.
\]
So we obtain
\begin{equation*}
\begin{aligned}
\int_0^T E_1 dt & \leq  \frac{\gamma}{\eps}
\int_0^T
\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 dt  +  \frac {P C_{\mathcal A}^2  h }{2\gamma \eps}%\bigg(
\vert \vert p^\eps \vert\vert^2_{L^2([0,T]; H^2(\Omega) )} 
%+
%\vert \vert \uu^\eps \vert\vert^2_{L^2([0,T]; H^2(\Omega) )} 
% \bigg)
  \\
&
+ %\frac{\sigma h  C_{\mathcal A}^2 P^{3/2} }{2\sqrt{C_\mathcal M} \eps^2}  
P C_{\mathcal A}^2 \left( \frac{h}{2 \gamma \eps} + \frac{\sigma h }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}}  \right) 
\vert \vert \uu^\eps \vert\vert^2_{L^2([0,T]; H^2(\Omega) )} 
+\frac{\sigma h  }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}}  \| \textbf{u}_r^{\eps} \|_{L^2([0,T]\times\Omega)}^2 .
\end{aligned}
\end{equation*}
Using  energy estimate (\ref{bee1})  for the the second  term of the rhs of the above inequality,  
 (\ref{bee2}) for the third term   and    (\ref{gggg}) for the last term,
one gets finally %after simplifications
\begin{equation} \label{eq:si1}
\int_0^T E_1(t)dt \leq 
\frac{\gamma}{\eps}\int_0^T\sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 
\end{equation}
$$
+
% T \frac {P C_{\delta}^2 h }{2\gamma \eps} \vert \vert \mathbf{V}^{\eps}(0) \vert\vert^2_{H^2(\Omega)}  +  \bigg( \frac1{\alpha} + PC_{\delta}^2 \bigg) \frac{ h }{2}\sqrt{\frac{P}{C_\mathcal M}}
  \left(T \frac {P C_{\mathcal A}^2 h }{2\gamma \eps}+
  P C_{\mathcal A}^2 \left( \frac{h}{2 \gamma \eps} + \frac{\sigma h }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}}  \right)
  \frac{\varepsilon^2}{\sigma}
 +
 \frac{\sigma h  }{2\eps^2}\sqrt{\frac{P}{C_\mathcal M}} 
   \frac{\varepsilon^2}{\sigma \alpha}
     \right)
  \| \mathbf{V}^{\eps}(0) \|_{H^2(\Omega)}^2 .
$$
After a convenient definition os the  constant $C_1(\gamma)$, 
it ends the proof. 
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Now we consider the third term in (\ref{eq:ee1}). 
%of  $\mathscr{E}'(t)$, which we call $E_2$.

\begin{pro} \label{pro:e2}
There exists a constant $C_2$ such that 
the  third term in the dissipative identity (\ref{eq:ee1}) can be bounded as
\begin{equation} \label{eq:si2}
\int_0^T E_2(t) dt \leq % \frac{hT}{\eps} C_{\mathcal A} P \bigg( \frac{1+C_\mathcal A^2h^2}{C_\mathcal M} + h \bigg)  
C_2 \frac{h}{\varepsilon C_\mathcal M}
 \|  \mathbf V ^\eps(0) \|^2_  { H^3(\Omega)  } .
\end{equation} 
%where the constants $P$ and $C_{\delta}$ are defined respectively in \eqref{nbmr} and  (\ref{const_delta1}-\ref{const_delta2}).
\end{pro}
\begin{proof}
We decompose 
$E_2$ in (\ref{eq:ee1})  in  two terms.
Making use of the second set of inequalities
of the proposition \ref{pro:interp} and the assumptions \eqref{eq:d7} and \eqref{nbmr}, the first one can be bounded as
$$
| A|= \left |
\frac{1}{\eps}\sum_j \sum_r  |\Gamma_{j,r} | p_j^{\eps} (\mathbf{n}_{j,r} ,
\tilde{\delta}_{j,r}(\uu^{\eps}) ) 
\right |
\leq 
\frac{  C_{\mathcal A} P }{\eps}  h^{2}  \sum_j | p_j  ^\eps |
 \|  \mathbf V ^\eps (t) \|_  { H^3(\Omega_j)  }.
$$
Using the inequality $ab\leq \frac{1}{2}(a^2+b^2)$,
it yields  
$
| A|\leq 
\frac{ C_{\mathcal A} P }{ 2\eps} h^{3}  \sum_j  | p_j  ^\eps |^2 + \frac{ C_{\mathcal A} P}{2 \eps} h \sum_j 
 \|  \mathbf V ^\eps (t) \|^2_  { H^3(\Omega_j)  }$.
The assumption \eqref{eq:d5} yields
$$
| A| \leq 
\frac{ C_{\mathcal A} P}{2 \eps}  \frac {h} {C_\mathcal M}  \|  \mathbf V_h ^\eps (t) \|^2_  { L^2(\Omega)  } + \frac{ C_{\mathcal A} P }{2 \eps}  h
 \|  \mathbf V ^\eps (t) \|^2_  { H^3(\Omega)  }
% \leq 
% \left(
% \frac{ C_{\mathcal A} P}{2 \eps}  \frac h {C_\mathcal M}
% +
% \frac{ C_{\mathcal A} P }{2 \eps}  h^2
% \right) \|  \mathbf V ^\eps (t) \|^2_  { H^3(\Omega)  }
.
$$
The $L^2$ stability  (\ref{gggg})  of the scheme $\mathbf P_h ^\eps$ shows
that 
$$
\|  \mathbf V_h ^\eps (t) \|_  { L^2(\Omega)  }\leq
\|  \mathbf V_h ^\eps (0) \|_  { L^2(\Omega)  }\leq\|  \mathbf V ^\eps(0) \|_  { L^2(\Omega)  }
+ \|  \mathbf V_h ^\eps (0)-  \mathbf V ^\eps(0)  \|_  { L^2(\Omega)  }
\leq (1+C_\mathcal A h) \|  \mathbf V ^\eps(0) \|_  { H^2(\Omega)  } 
$$
 where the last inequality comes from the initialization stage \eqref{constant_init}. With  the basic energy estimate (\ref{bee1}), and since $h$ is bounded, we obtain
$$
\int_0^T\vert A \vert dt \leq T
\left(  
\frac{ C_{\mathcal A} P}{2 \eps}  \frac h {C_\mathcal M} (1+C_\mathcal A h)
+\frac{ C_{\mathcal A} P }{2 \eps}  h
\right)
  \|  \mathbf V ^\eps(0) \|^2_  { H^3(\Omega)  } .
$$
The  second contribution in $E_2$ is 
$
B=\frac{1}{\eps}\sum_j \sum_r |\Gamma_{j,r} | \bigg(
\mathbf{u}_j^{\eps} , \mathbf{n}_{j,r} \tilde{\delta}_{j,r}(p^{\eps}) \bigg)$.
Almost the same  calculations show the  bound
$$
\int_0^T\vert B \vert dt \leq 
T
\left(  
\frac{ C_{\mathcal A} P}{2 \eps}  \frac h {C_\mathcal M} (1+C_\mathcal A h)
+\frac{ C_{\mathcal A} P }{2 \eps}  h
\right)
  \|  \mathbf V ^\eps(0) \|^2_  { H^3(\Omega)  } .
%\frac{hT}{\eps} \frac{ C_{\mathcal A} P }{2 }\bigg( \frac{1+C_\mathcal A^2h^2}{C_\mathcal M} + h \bigg)   \|  \mathbf V ^\eps(0) \|^2_  { H^3(\Omega)  },
$$
Summing the two contributions, it   concludes the proof after a convenient definition of $C_2$.
\end{proof}

%We now study the two last lines of $\mathscr{E}'(t)$, which we call S. 
\begin{pro}  \label{pro:conv2}
Let $\widehat \gamma>0$ be a number which precise value will be determined further.
There exists a constant $C_3(\widehat \gamma)$ which depends on $\widehat \gamma$ such that 
one has
the bound for the last term of the dissipative identity (\ref{eq:ee1})
%Let  $\widehat{\gamma}>0$ be a number which precise value will be determined further.
%The  last term in the dissipative identity (\ref{eq:ee1}) can be bounded as
\begin{equation} \label{eq:bound3}
\begin{aligned}
\int _0^TE_3(t) dt \leq 
\frac{\widehat{\gamma} \sigma}{2\eps}\int_0^T\sum_r \sum_j \ljr \bigg( 
\njr  \: , \: \textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg)^2 dt 
+
%  P \bigg\{ C_{\mathcal A}\bigg( 1 + \frac{2P}{C_\mathcal M\alpha} \bigg) h^2 
%+
%\frac{  h}{2 C_\mathcal M \widehat{\gamma}\eps} \bigg\}
C_3(\widehat \gamma) \frac{h}{ \varepsilon C_\mathcal M}
 \|  \mathbf{V}^{\eps}(0) \|_{H^1(\Omega)}^2.
\end{aligned}
\end{equation}
%where the constants $P$, $C_{\delta}$ and $C_\mathcal A$ are defined respectively in \eqref{nbmr}, (\ref{const_delta1}-\ref{const_delta2}) and \eqref{eq:d5.2}.
\end{pro}

\begin{proof}
The definition of $E_3$ in (\ref{eq:ee1}) is
\begin{equation*}
\begin{aligned}
E_3 &= \frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg(
\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x \bigg)
+\frac{\sigma}{\eps^2}\sum_j\bigg(  \textbf{u}_j^{\eps} ,
\int_{\Omega_j}\textbf{u}^{\eps} \bigg) \\
&-\frac{\sigma}{\eps^2}\sum_j\int_{\Omega_j}(\textbf{u}^{\eps},\textbf{u}^{\eps}
)d\x -\frac{\sigma}{\eps^2}\sum_{j}\sum_{r}
(\widehat{\beta}_{j}
\textbf{u}_r^{\eps},\textbf{u}_r^{\eps}).
\end{aligned}
\end{equation*}
Using the Cauchy-Schwarz inequality on the third term
 $\int (\textbf{u}^{\eps},\textbf{u}^{\eps})$, one gets
\begin{equation*}
\begin{aligned}
E_3 &\leq \frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg(
\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps}d\x \bigg) +\frac{\sigma}{\eps^2}\sum_j\bigg(  \textbf{u}_j^{\eps} ,
\int_{\Omega_j}\textbf{u}^{\eps} \bigg) \\
&- \frac{\sigma}{\eps^2}\sum_j \frac{1}{|\Omega_j|}\bigg(
\int_{\Omega_j}\textbf{u}^{\eps}d\x \bigg)^2 -\frac{\sigma}{\eps^2}\sum_{j}
\sum_{r}
(\widehat{\beta}_{j}    
\textbf{u}_r^{\eps},\textbf{u}_r^{\eps}),
\end{aligned}
\end{equation*}
which can be written
\begin{equation*}
E_3 \leq -\frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg(
\widehat{\beta}_{j,r}\textbf{u}_r^{\eps}  \: , \:
\textbf{u}_r^{\eps}-\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x
\bigg) - \frac{\sigma}{\eps^2}\sum_j\bigg( \int_{\Omega_j}\textbf{u}^{\eps}d\x ,
 \frac{1}{|\Omega_j|}  \int_{\Omega_j}\textbf{u}^{\eps}d\x -\uj^{\eps}
\bigg) 
\end{equation*}
that is 
\begin{equation*}
\begin{aligned}
E_3 \leq &-\frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}\bigg(
\textbf{u}_r^{\eps}- \frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x\bigg) 
\: , \: \textbf{u}_r^{\eps}-\frac{1}{|\Omega_j|}\int_{\Omega_j}
\textbf{u}^{\eps} d\x \bigg) \\
&-\frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}
\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x  \: , \:
\textbf{u}_r^{\eps}-\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x
\bigg)\\
&-\frac{\sigma}{\eps^2}\sum_j\bigg( \int_{\Omega_j}\textbf{u}^{\eps}d\x , 
\frac{1}{|\Omega_j|}  \int_{\Omega_j}\textbf{u}^{\eps}d\x -\textbf{u}^{\eps}_j
\bigg).
\end{aligned}
\end{equation*}
One has, using the geometric identity 
$\sum_r \be =|\Omega_j| I_d$ which can be found in \cite{glaceap,de10},
$$
  \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}
\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x  \: , \:
\textbf{u}_r^{\eps}-\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x
\bigg) +
\sum_j  \bigg( \int_{\Omega_j} \textbf{u}^{\eps} d\x  \: , \:
\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x-\textbf{u}_j^{\eps}
\bigg)
$$
$$
= \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}
\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x  \: , \:
\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg) 
.
 $$
We thus get after simplification 
\begin{equation}\label{eq:s1}
\left.
\begin{aligned}
E_3\leq &-\frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}\bigg(
\textbf{u}_r^{\eps}- \frac{1}{|\Omega_j|}\int_{\Omega_j} 
\textbf{u}^{\eps} d\x\bigg ) 
\: , \: \textbf{u}_r^{\eps}-\frac{1}{|\Omega_j|}\int_{\Omega_j}
\textbf{u}^{\eps} d\x \bigg)
 \\
&-\frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}
\frac{1}{|\Omega_j|}\int_{\Omega_j} \textbf{u}^{\eps} d\x  \: , \:
\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg)
\end{aligned}
\hspace{0.3cm}
\right|
\begin{aligned}
:=S_1 \\
\\
:=S_2
\end{aligned}
\end{equation}
We add and subtract at each average on the cell the nodal value. 
We recall the notation  
$\delta_{j,r}(\textbf{u}^{\eps})=\frac{1}{|\Omega_j|}\int_{\Omega_j}
\uu^{\eps} d\x-\textbf{u}^{\eps}(\textbf{x}_r)$.
We get for the term under the first sum in (\ref{eq:s1})
$$
\bigg( \widehat{\beta}_{j,r}\bigg(
\textbf{u}_r^{\eps}- \frac{1}{|\Omega_j|}\int_{\Omega_j} 
\textbf{u}^{\eps} d\x\bigg ) 
\: , \: \textbf{u}_r^{\eps}-\frac{1}{|\Omega_j|}\int_{\Omega_j}
\textbf{u}^{\eps} d\x \bigg)
$$
$$
=\bigg( \widehat{\beta}_{j,r}\bigg(
\textbf{u}_r^{\eps}- u^\eps(\x_r)
\bigg ) 
\: , \: \textbf{u}_r^{\eps}-u^\eps(\x_r)
 \bigg)
-
\bigg( \widehat{\beta}_{j,r}\bigg(
\textbf{u}_r^{\eps}- u^\eps(\x_r)
\bigg ) 
\: , \: \delta_{j,r}(\textbf{u}^{\eps})
 \bigg)
$$
\begin{equation} \label{eq:bb1}
-
\bigg( 
\widehat{\beta}_{j,r} \delta_{j,r}(\textbf{u}^{\eps})
\: , \:
\textbf{u}_r^{\eps}- u^\eps(\x_r)
 \bigg)
+
\bigg( 
\widehat{\beta}_{j,r} \delta_{j,r}(\textbf{u}^{\eps})
\: , \:
 \delta_{j,r}(\textbf{u}^{\eps})
 \bigg).
\end{equation}
The first of these quantities is purely nodal: one has after summation 
$$
\sum_j \sum_r \bigg( \widehat{\beta}_{j,r}\bigg(
\textbf{u}_r^{\eps}- u^\eps(\x_r)
\bigg ) 
\: , \: \textbf{u}_r^{\eps}-u^\eps(\x_r)
 \bigg)
$$
\begin{equation} \label{eq:ss2}
=
 \sum_r \bigg( B_{r}\bigg(
\textbf{u}_r^{\eps}- u^\eps(\x_r)
\bigg ) 
\: , \: \textbf{u}_r^{\eps}-u^\eps(\x_r)
 \bigg)\geq \alpha \sum_r |V_r| | 
 \textbf{u}_r^{\eps}-
 u^\eps(\x_r) |^2
\end{equation}
with the help of (\ref{geom3}). The second and third term in the identity (\ref{eq:bb1}) can be bounded
by a Young's inequality with a convenient constant $C=\frac{C_\mathcal M \alpha}{2P}$
so that all terms containing $ \textbf{u}_r^{\eps}-u^\eps(\x_r)$
are controlled by (\ref{eq:ss2}).
So we obtain concerning $S_1$ defined in (\ref{eq:s1})
\begin{equation*} \label{eq:ss4}
S_1
\leq  \bigg( 1 + \frac{2 P}{C_\mathcal M\alpha} \bigg) \frac{h^2\sigma }{\eps^2}\sum_r \sum_j  \left|  \delta_{j,r}(\textbf{u}^{\eps}) \right|^2.
\end{equation*}
Using the first  interpolation result stressed in proposition \ref{pro:interp},
one has in dimension two
$| \delta_{j,r}(\textbf{u}^{\eps}) |
 \leq C_{\mathcal A}  \|\uu^\eps(t)\|_{H^2(\Omega_j)} $. 
So, taking into account energy estimate (\ref{bee2})
we have for the first term
\begin{equation*} \label{eq:ss6}
\int_0^T S_1dt \leq  C_{\mathcal A} ^2P  \bigg( 1 + \frac{2P}{C_\mathcal M\alpha} \bigg) h^2   \| \mathbf{V}^{\eps}(0) \|_{H^2(\Omega)}^2 .
\end{equation*}
We now consider the second term called  $S_2$ in (\ref{eq:s1})
\begin{equation*} \label{eq:ss5}
S_2=-\frac{\sigma}{\eps^2} \sum_r \sum_j  \bigg( \widehat{\beta}_{j,r}
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x  \: , \:
\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg) .
\end{equation*}
Using $(\vec{a}\otimes\vec{b}\:\vec{c},\vec{d}) 
= (\vec{b},\vec{c})(\vec{a},\vec{d})$, one has 
\begin{equation*}
S_2 = -\frac{\sigma}{\eps^2} \sum_r
\sum_j \ljr \bigg( (\x_r-\x_j), \frac{1}{|\Omega_j|}\int_{\Omega_j}
\uu^{\eps} d\x \bigg) \bigg(  \njr  \: , \:
\textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg)
\end{equation*}
Using the  Young's inequality  $ab \leq \frac{\widehat \gamma \eps }2 a^2+\frac1{2\widehat \gamma \eps } b^2 $, we get
$$\int_0^T
S_2 dt \leq \frac{\widehat{\gamma} \sigma}{2\eps}\int_0^T\sum_r \sum_j \ljr \bigg( 
\njr  \: , \: \textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg)^2 dt\\
+
\int_0^T\frac{\sigma}{2\widehat{\gamma}  \eps^3}\sum_r \sum_j \ljr \bigg( (\x_r-\x_j),
\frac{1}{|\Omega_j|}\int_{\Omega_j} \uu^{\eps} d\x \bigg) ^2dt
$$
Using one more time the energy estimate (\ref{bee2}) the second term in the right hand side of the above inequality  is bounded by
 $\frac{ P h}{2 C_\mathcal M \widehat \gamma \eps} \| \V^\eps(0)\|^2_{ L^2(\Omega)}$. Thus 
$$
\int _0^TE_3 (t)dt \leq 
\frac{\widehat{\gamma} \sigma}{2\eps}\int_0^T\sum_r \sum_j \ljr \bigg( 
\njr  \: , \: \textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg)^2 dt 
+
 P \bigg\{ C_{\mathcal A}^2\bigg( 1 + \frac{2P}{C_\mathcal M\alpha} \bigg) h^2 
+
\frac{  h}{2 C_\mathcal M \widehat{\gamma}\eps} \bigg\} \|  \mathbf{V}^{\eps}(0) \|_{H^2(\Omega)}^2 ,
$$
which is the expected result after convenient redefinition of the constant in front of the last term.
\end{proof}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[End of the proof of the naive estimate of  proposition (\ref{ein_prosit})]
One gets
$$
\mathscr{E}(T) \leq \mathscr{E}(0)
-\frac1\eps \int_0^T \sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 +
\int_0^T E_1(t)dt + \int_0^T E_2(t)dt
+\int_0^T E_3(t)dt
$$
where integrals are estimated in 
(\ref{eq:bound1}), (\ref{eq:si2}) and   
(\ref{eq:bound3}).  Using equation \eqref{constant_init}, one finds
\begin{equation*}
\begin{aligned}
\mathscr{E}(T)& \leq C_0^2h^2 \| \mathbf{V}^{\eps}(0) \|_{H^2(\Omega)}^2 \\
& 
-\frac1\eps \int_0^T \sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2\\
&  +  \frac{\gamma}{\eps}
     \int_0^T \sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 
%T \frac {P C_{\mathcal A}^2  h }{2\gamma \eps} \vert \vert \mathbf{V}^{\eps}(0) \vert\vert^2_{H^1(\Omega)}  +  \bigg( \frac1{\alpha} + PC_{\delta}^2 \bigg) \frac{ h }{2}\sqrt{\frac{P}{C_\mathcal M}}  \| \mathbf{V}^\eps(0) \|_{H^1(\Omega)}^2   
+
C_1(\gamma) \frac h { \varepsilon \sqrt{C_\mathcal M} }   \| \mathbf{V}^{\eps}(0) \|_{H^1(\Omega)}^2
\\
& + %\frac{hT}{\eps} C_{\mathcal A} P \bigg( \frac{1+C_\mathcal A^2h^2}{C_\mathcal M} + h \bigg)   \|  \mathbf V ^\eps(0) \|^2_  { H^3(\Omega)  }  
C_2 \frac{h}{\varepsilon C_\mathcal M}
 \|  \mathbf V ^\eps(0) \|^2_  { H^3(\Omega)  } \\
&
+ \frac{\widehat{\gamma} \sigma}{2\eps}\int_0^T\sum_r \sum_j \ljr \bigg( 
\njr  \: , \: \textbf{u}_r^{\eps}-\textbf{u}_j^{\eps} \bigg)^2 dt 
+ C_3(\widehat \gamma) \frac{h}{ \varepsilon C_\mathcal M}
 \|  \mathbf{V}^{\eps}(0) \|_{H^2(\Omega)}^2.
%  P \bigg\{ C_{\mathcal A}\bigg( 1 + \frac{2P}{C_\mathcal M\alpha} \bigg) h^2 
%+
%\frac{ h}{2 C_\mathcal M \widehat{\gamma}\eps} \bigg\} \|  \mathbf{V}^{\eps}(0) \|_{H^1(\Omega)}^2 
 \end{aligned}.
\end{equation*}
%Therefore
%$$
%\mathscr{E}(T)\leq
%Ch\left( 1 +\frac T \eps +h +\frac{1}{\eps \widehat{\gamma}}
%\right)-
%\frac{1}{\eps}\bigg( 1- \gamma - \frac{\sigma \widehat{\gamma}}{2}
%\bigg)\int_0^T \sum_{j,r}l_{j,r}\bigg(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}_r^{\eps} \bigg)^2 dt,
%$$
%where
%\begin{equation*}
%\begin{aligned}
%C = \|\mathbf{V}^\eps(0) \|^2_{H^3(\Omega)}  \: \hbox{max}\bigg\{ & \:  \frac{ 1 }{2}\sqrt{\frac{P}{C_\mathcal M}} \bigg( \frac1{\alpha} + PC_{\mathcal A}^2 \bigg) \: , \: \frac{P}{2C_\mathcal M}  , \\
%&  C_{\mathcal A} P \bigg( \frac{1+C_\mathcal A^2h^2}{C_\mathcal M} +\frac{ C_{\mathcal A} }{2 \gamma } + h \bigg) \: , \: C_\mathcal A^2 +   P C_{\mathcal A}\bigg( 1 + \frac{2P}{C_\mathcal M\alpha} \bigg) \: \bigg\} .
%\end{aligned}
%\end{equation*}
 This estimate is fundamental, since it 
shows the competition between different kind of error terms
and the dissipation of the fluxes.
Choosing by example $\widehat{\gamma}<\frac{1}{\sigma}$ and $\gamma<\frac12$, 
all terms like $ \int_0^T \sum_{j,r}l_{j,r}(\textbf{n}_{j,r},\textbf{u}_j^{\eps}-\textbf{u}
_r^{\eps})^2 $ vanish.
All other terms can put together as
%
%the last
%term is non positive, which means that
%the dissipation of the fluxes is dominant in some sense. We choose $\widehat{\gamma}=\frac{1}{\sigma}$ and $\gamma=\frac12$, the last term  vanishes thus
$
\mathscr{E}(T)\leq
\frac{~_\downarrow C}2 \frac{h}{\varepsilon } 
\| p_0\|^2_{H^4(\Omega)}$. 
It ends the proof of the naive estimate.
\end{proof}


{\footnotesize 
\begin{thebibliography}{abcde}   

\bibitem{TAHO} D. Aregba, M. Briani, R. Natalini, \emph{Time Asymptotic High Order Schemes for Dissipative BGK Hyperbolic Systems}, arXiv:1207.6279.

\bibitem{ber1} C. Berthon, P. Charrier, R. Turpault \emph{An HLLC scheme
to solve the $M^1$ model of radiative transfer in two dimensions} J. Scie.
Comput., J. Sci. Comput., 31, no. 3, pp 347-389, 2007.

\bibitem{ber3} C. Berthon, P. LeFloch, R. Turpault  \emph{Late-time relaxation limits of nonlinear hyperbolic systems. A general framework}, Math. of Comput., 82 , pp. 831-860, 2013. 

\bibitem{ber4} C. Berthon, R. Turpault \emph{Asymptotic preserving HLL
schemes}, Numerical Methods for Partial Differential Equations, 27 (6)  pp
1396-1422, 2011.

\bibitem{buet} C. Buet, S. Cordier,   B. Lucquin-Desreux, S. Mancini, 
\emph{Diffusion limit of the lorentz model: asymptotic preserving schemes.}
ESAIM: M2AN vol. 36, 4, pp 631-655, 2002.

\bibitem{glaceap} C. Buet, B. Despr\'es, E. Franck \emph{Design of
asymptotic preserving schemes for the hyperbolic heat equation on unstructured
meshes} Numerish Mathematik, Volume 122, Issue 2, pp 227-278,  2012.

\bibitem{cras} C. Buet, B. Despr\'es, E. Franck, \emph{An asymptotic
preserving scheme with the maximum principle for the $M_1$ model on distorted
meshes}, C.R. Acad. Sci., Paris, S\'er. I, Math., Vol 350,  11-12
pp 633-638,  2012.

\bibitem{FVCA6} C. Buet, B. Despr\'es, E. Franck, \emph{Asymptotic
preserving finite volumes discretization for non-linear moment model on
unstructured meshes}, Finite Volumes for Complex Applications VI Problems,
Perspectives, Springer Proceedings in Mathematics Volume 4, pp 467-474, 2011.



\bibitem{MMvlasov} N. Crouseilles, M. Lemou \emph{An asymptotic preserving
scheme based on a micro-macro decomposition for collisional Vlasov equations:
diffusion and high-field scaling limits}, Kinetic and related models 4, 2 
pp 441-477, 2011.

\bibitem{couplingRad} N. Crouseilles, M. Roger \emph{A dynamic multi-scale
model for transient radiative transfer calculations} Journal of Quantitative
Spectroscopy and Radiative Transfer , pp 110-121, 2013.

\bibitem{de04} B. Despr\'es,  \emph{Lax theorem and finite volume
schemes},
Math. Comput 0, 73:1203-1234, 2004.

\bibitem{de10} B. Despr\'es,  \emph{Weak consistency of the cell-centered
Lagrangian GLACE scheme on general
meshes in any dimension}, Comput. Methods Appl. Mech. Engrg. 199, pp 
2669-2679, 2010.

\bibitem{FV} R. Eymard, T. Gallouet, R. Herbin, \emph{The finite volume
method}, Handbook for Numerical Analysis, Ph. Ciarlet J.L. Lions eds, North
Holland, 715-1022, 2000.


\bibitem{F2}  F. Filbet, A. Rambaud, Analysis of an asymptotic preserving scheme for relaxationsystems, ESAIM M2AN, Vol. 47 pp. 609-633 (2013)

\bibitem{cveps} F. Golse, S. Jin, C. D. Levermore \emph{The convergence
of numerical transfer schemes in diffusive regimes I: discrete-ordinate method},
SIAM J. Numer. Anal. 36, pp 1333-1369, 1999.

\bibitem{2Driemann}
L. Gosse, 
A two-dimensional version of the Godunov scheme for scalar balance laws,
 SIAM J. Numer. Anal. 52 (2014) 626-652.

\bibitem{Gosse} L. Gosse, G. Toscani \emph{An asymptotic-preserving
well-balanced scheme for the hyperbolic heat equations} C. R. Acad. Sci
Paris,Ser. I 334, pp  337-342, , 2002.

\bibitem{GosseSn} L. Gosse \emph{Transient radiative transfer in the grey
case: Well-balanced and asymptotic-preserving schemes built on Cases's
elementary solutions} Journal of Quantitative Spectroscopy and Radiative
Transfer 112, pp 1995-2012, 2011.

\bibitem{2DRiemann}  L. Gosse, \emph{A two-dimensional version of the Godunov scheme for scalar balance laws}. SIAM J. Numer. Anal. 52 (2014) 626?652.

\bibitem{jinbase} S. Jin, \emph{Efficient Asymptotic-Preserving (AP)
Schemes for Some Multiscale Kinetic Equations},  SIAM J. Sci. Comp. 21, 441-454,
1999.

\bibitem{jinreview} S. Jin, \emph{Asymptotic preserving (AP) schemes for
multiscale kinetic and hyperbolic equations: a review}, Lecture Notes for Summer
School on \og Methods and Models of Kinetic Theory\fg\, (M and MKT), Porto
Ercole (Grosseto, Italy),  2010.

\bibitem{jinlev} S. Jin, D. Levermore \emph{Numerical schemes for
hyperbolic conservation laws with stiff relaxation terms.} JCP 126, pp 449-467,
1996.

\bibitem{SnJin} S. Jin, D. Levermore \emph{The discrete-ordinate method in
diffusive regime.} Transport Theory and Statistical Physics, vol. 20, issue 5,
pp 413-439, 1991.




\bibitem{F3} S. Jin; L. Pareschi and G. Toscani, Uniformly accurate diffusive relaxation schemesfor multiscale transport equations. SIAM J. Numer. Anal. 38 (2000), 913?936

\bibitem{F4}S. Jin, L. Pareschi and G. Toscani, Diffusive Relaxation Schemes for Discrete-Velocity Kinetic Equations, SIAM J. Num. Anal. 35 (1998) 2405-2439

\bibitem{lemou} M. Lemou, L.Mieussens \emph{A new asymptotic preserving
scheme based on micro-macro formulation for linear kinetic equations in the
diffusion limit.} SIAM J. Sci. COMPUT. Vol. 31, 1, pp 334-368, 2008.

\bibitem{mmcv} J. G. Liu, L. Mieussens \emph{Analysis of an Asymptotic
Preserving Scheme for Linear Kinetic Equations in the Diffusion Limit} SIAM J.
Numer. Anal,  48(4), pp 1474-1491, 2010.

\bibitem{Mazeran} C. Mazeran \emph{Sur la structure mathematique et
l'approximation numerique de l'hydrodynamique lagrangienne bidimentionnelle}
 PhD, University Bordeaux 1, 2007.
 
 
\bibitem{F1}
G. Naldi and L. Pareschi, Numerical schemes for hyperbolic systems of conservationlaws with stiff diffusive relaxation, SIAM J. Numer. Anal., 37 (2000), pp.1246-1270.
 
\end{thebibliography}

}

\end{document}





